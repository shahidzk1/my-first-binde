{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Cuts Optimization using Extra Gradient Boosting\n",
    "<br></p><br>\n",
    "\n",
    "Over the last years, **Machine Learning** tools have been successfully applied to problems in high-energy physics. For example, for the classification of physics objects. Supervised machine learning algorithms allow for significant improvements in classification problems by taking into account observable correlations and by learning the optimal selection from examples, e.g. from Monte Carlo simulations.\n",
    "\n",
    "\n",
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook.\n",
    "\n",
    "**Matplotlib** comes handy in plotting data while the machine learning is performed by **XGBOOST**. We will import data splitter from **Scikit-learn** as *train_test_split*. **Evaluation metrics** such as *confusion matrix*, *Receiver operating characteristic (ROC)*, and *Area Under the Receiver Operating Characteristic Curve (ROC AUC)*  will be used to asses our models.\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import weakref \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "#from root_pandas import read_root\n",
    "\n",
    "\n",
    "from data_cleaning import clean_df\n",
    "from KFPF_lambda_cuts import KFPF_lambda_cuts\n",
    "from plot_tools import AMS, preds_prob, plot_confusion_matrix, plt_sig_back\n",
    "import tree_importer \n",
    "import uproot\n",
    "\n",
    "\n",
    "#To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") \n",
    "        \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(7)\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The quality_cuts_plus_other_cuts function applies quality selection criteria with other selection criteria to reduce data size\n",
    "\"\"\"\n",
    "def quality_cuts_plus_other_cuts():\n",
    "    #The following quality selection criteria is applied\n",
    "    mass_cut = \"(LambdaCandidates_mass > 1.077) &\"\n",
    "    \n",
    "    coordinate_cut = \"(LambdaCandidates_x>-50) & (LambdaCandidates_x<50) & (LambdaCandidates_y>-50) & (LambdaCandidates_y<50) & (LambdaCandidates_z>-1) & (LambdaCandidates_z<80) &\"\n",
    "    \n",
    "    chi_2_positive_cut =\"(LambdaCandidates_chi2geo>0) & (LambdaCandidates_chi2topo>0) & (LambdaCandidates_chi2primpos>0) & (LambdaCandidates_chi2primneg > 0) &\"\n",
    "    \n",
    "    distance_cut = \"(LambdaCandidates_ldl>0) & (LambdaCandidates_l<80) & (LambdaCandidates_distance>0) & (LambdaCandidates_distance<100) &\"\n",
    "    \n",
    "    pz_cut = \"(LambdaCandidates_pz>0) & \"\n",
    "    #Other cuts\n",
    "    pseudo_rapidity_cut_based_on_acceptance = \"(LambdaCandidates_eta>1) & (LambdaCandidates_eta<6.5) &\"\n",
    "    \n",
    "    angular_cut = \"(LambdaCandidates_cosineneg>0.1) & (LambdaCandidates_cosinepos>0.1) &\"\n",
    "    \n",
    "    data_reducing_cut = \"(LambdaCandidates_mass < 1.3) &  (LambdaCandidates_p<20)  &   (LambdaCandidates_chi2geo<1000) &  (LambdaCandidates_chi2primpos<1e6) & (LambdaCandidates_chi2primneg < 3e7) &  (LambdaCandidates_ldl<5000) & (LambdaCandidates_chi2topo < 100000)\"\n",
    "    \n",
    "    cuts= mass_cut+coordinate_cut+chi_2_positive_cut+distance_cut+pz_cut+pseudo_rapidity_cut_based_on_acceptance+angular_cut+data_reducing_cut\n",
    "    return cuts\n",
    "\n",
    "\"\"\"\n",
    "This tree_importer_with_cuts imports tree and also applies quality selection criteria on the data along with some further data reducing cuts.\n",
    "\"\"\"\n",
    "\n",
    "def tree_importer_with_cuts(path,treename, n):\n",
    "    \n",
    "    #This part changes the labels of the root tree's branches \n",
    "    labels=[\"LambdaCandidates_chi2geo\", \"LambdaCandidates_chi2primneg\", \"LambdaCandidates_chi2primpos\",\n",
    "         \"LambdaCandidates_distance\", \"LambdaCandidates_ldl\",\"LambdaCandidates_mass\", \"LambdaCandidates_pT\",\n",
    "            \"LambdaCandidates_rapidity\", \"LambdaCandidates_is_signal\"]\n",
    "    \n",
    "    new_labels=['chi2geo', 'chi2primneg','chi2primpos', 'distance', 'ldl','mass', 'pT', 'rapidity','issignal']\n",
    "    \n",
    "    cuts = quality_cuts_plus_other_cuts()\n",
    "    \n",
    "    #The number of parallel processors\n",
    "    executor = ThreadPoolExecutor(n)\n",
    "    \n",
    "    #To open the \n",
    "    file = uproot.open(path+':'+treename, library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(labels,cuts, library='np',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "    df= pd.DataFrame(data=file)\n",
    "    df.columns = new_labels\n",
    "    #df['issignal']=((df['issignal']>0)*1)\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_importer(path,treename, n):    \n",
    "    #The number of parallel processors\n",
    "    executor = ThreadPoolExecutor(n)\n",
    "    \n",
    "    #To open the root file and convert it to a pandas dataframe\n",
    "    df = uproot.open(path+':'+treename, library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The quality_cuts_plus_other_cuts function applies quality selection criteria with other selection criteria to reduce data size\n",
    "\"\"\"\n",
    "def quality_cuts_plus_other_cuts():\n",
    "    #The following quality selection criteria is applied\n",
    "    mass_cut = \"(LambdaCandidates_mass > 1.07) &\"\n",
    "    \n",
    "    coordinate_cut = \"(LambdaCandidates_x>-50) & (LambdaCandidates_x<50) & (LambdaCandidates_y>-50) & (LambdaCandidates_y<50) & (LambdaCandidates_z>-1) & (LambdaCandidates_z<80) &\"\n",
    "    \n",
    "    chi_2_positive_cut =\"(LambdaCandidates_chi2geo>0) & (LambdaCandidates_chi2topo>0) & (LambdaCandidates_chi2primpos>0) & (LambdaCandidates_chi2primneg > 0) &\"\n",
    "    \n",
    "    distance_cut = \"(LambdaCandidates_ldl>0) & (LambdaCandidates_l<80) & (LambdaCandidates_distance>0) & (LambdaCandidates_distance<100) &\"\n",
    "    \n",
    "    pz_cut = \"(LambdaCandidates_pz>0) & \"\n",
    "    #Other cuts\n",
    "    pseudo_rapidity_cut_based_on_acceptance = \"(LambdaCandidates_eta>1) & (LambdaCandidates_eta<6.5) &\"\n",
    "    \n",
    "    #angular_cut = \"(LambdaCandidates_cosineneg>0.1) & (LambdaCandidates_cosinepos>0.1) &\"\n",
    "    \n",
    "    data_reducing_cut = \"(LambdaCandidates_mass < 1.2) &  (LambdaCandidates_p<20)  &   (LambdaCandidates_chi2geo<1000) &  (LambdaCandidates_chi2primpos<1e6) & (LambdaCandidates_chi2primneg < 3e7) &  (LambdaCandidates_ldl<5000) & (LambdaCandidates_chi2topo < 100000)\"\n",
    "    \n",
    "    cuts= mass_cut+coordinate_cut+chi_2_positive_cut+distance_cut+pz_cut+pseudo_rapidity_cut_based_on_acceptance+data_reducing_cut\n",
    "    return cuts\n",
    "\n",
    "\"\"\"\n",
    "This tree_importer_with_cuts imports tree and also applies quality selection criteria on the data along with some further data reducing cuts.\n",
    "\"\"\"\n",
    "\n",
    "def tree_importer_with_cuts(path,treename, n):\n",
    "    \n",
    "    #This part changes the labels of the root tree's branches \n",
    "    labels=['LambdaCandidates_chi2geo', 'LambdaCandidates_chi2primneg',\n",
    " 'LambdaCandidates_chi2primpos', 'LambdaCandidates_chi2topo', 'LambdaCandidates_cosineneg', 'LambdaCandidates_cosinepos',\n",
    " 'LambdaCandidates_cosinetopo', 'LambdaCandidates_distance',\n",
    " 'LambdaCandidates_eta', 'LambdaCandidates_l', 'LambdaCandidates_ldl', 'LambdaCandidates_mass', 'LambdaCandidates_p',\n",
    " 'LambdaCandidates_pT', 'LambdaCandidates_phi',  \n",
    " 'LambdaCandidates_rapidity',   'LambdaCandidates_is_signal']\n",
    "    \n",
    "    new_labels=['chi2geo', 'chi2primneg','chi2primpos', 'distance', 'ldl','mass', 'pT', 'rapidity','issignal']\n",
    "    \n",
    "    cuts = quality_cuts_plus_other_cuts()\n",
    "    \n",
    "    #The number of parallel processors\n",
    "    executor = ThreadPoolExecutor(n)\n",
    "    \n",
    "    #To open the \n",
    "    file = uproot.open(path+':'+treename, library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(labels,cuts,library='np',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "    df= pd.DataFrame(data=file)\n",
    "    df[\"LambdaCandidates_is_signal\"]=df[\"LambdaCandidates_is_signal\"].astype(\"int8\")\n",
    "    \n",
    "    #df.columns = new_labels\n",
    "    #df['issignal']=((df['issignal']>0)*1)\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = uproot.open('/home/shahid/Mount/gsi/u/dcm_5m_signal.root:plain_tree',library='pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/c6_pt_y_0_1.59_yield_bdt_cut_0.8.root:t2',library='pd').arrays(library='pd')\n",
    "df2 = uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/c6_pt_y_1.59_y_yield_bdt_cut_0.8.root:t2',library='pd').arrays(library='pd')\n",
    "dfs = [df1,df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urqmd = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dcm = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dff[dff['Candidates_plain_generation']==0].shape)\n",
    "print(dff[dff['Candidates_plain_generation']>0].shape)\n",
    "print(dff[dff['Candidates_plain_generation']==1].shape)\n",
    "print(dff[dff['Candidates_plain_generation']==2].shape)\n",
    "print(dff[dff['Candidates_plain_generation']==3].shape)\n",
    "print(dff[dff['Candidates_plain_generation']==4].shape)\n",
    "print(dff[dff['Candidates_plain_generation']==5].shape)\n",
    "(0, 33)\n",
    "(2397995, 33)\n",
    "(1670525, 33)\n",
    "(724350, 33)\n",
    "(2858, 33)\n",
    "(261, 33)\n",
    "(1, 33)\n",
    "\n",
    "(0, 33)\n",
    "(2578578, 33)\n",
    "(1801764, 33)\n",
    "(774482, 33)\n",
    "(2121, 33)\n",
    "(210, 33)\n",
    "(0, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dff[dff['Candidates_generation']==0].shape)\n",
    "print(dff[dff['Candidates_generation']>0].shape)\n",
    "print(dff[dff['Candidates_generation']==1].shape)\n",
    "print(dff[dff['Candidates_generation']==2].shape)\n",
    "print(dff[dff['Candidates_generation']==3].shape)\n",
    "print(dff[dff['Candidates_generation']==4].shape)\n",
    "print(dff[dff['Candidates_generation']==5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "185149/ 643537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[dff['Candidates_mass']<0.7]['Candidates_mass'].hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = uproot.open('/home/shahid/Mount/gsi/u/analysis_plain_ttree.root:plain_tree').arrays(library='pd')\n",
    "#print(df[df['Candidates_plain_generation']>0].shape)\n",
    "#print(df[df['Candidates_plain_generation']==0].shape)\n",
    "df['Candidates_plain_mass'].hist(bins=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file, for example a URQMD 12 AGeV, is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **tree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "In this example, we download three Analysis Trees. The first one contains mostly background candidates for lambda i.e. protons and pions which do not come from a lambda. The second file contains mostly signal candidates of lamba i.e. it contains protons and pions which come from a lambda decay. The third one contains 10k events generated using URQMD generator with 12 AGeV energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['index', 'LambdaCandidates_chi2geo', 'LambdaCandidates_chi2primneg',\n",
    "       'LambdaCandidates_chi2primpos', 'LambdaCandidates_chi2topo',\n",
    "       'LambdaCandidates_cosineneg', 'LambdaCandidates_cosinepos',\n",
    "       'LambdaCandidates_cosinetopo', 'LambdaCandidates_distance',\n",
    "       'LambdaCandidates_eta', 'LambdaCandidates_l', 'LambdaCandidates_ldl',\n",
    "       'LambdaCandidates_mass', 'LambdaCandidates_p', 'LambdaCandidates_pT',\n",
    "       'LambdaCandidates_phi', 'LambdaCandidates_rapidity','LambdaCandidates_issignal']\n",
    "new_labels = []\n",
    "\n",
    "for i in a:\n",
    "    if 'LambdaCandidates_' in i:\n",
    "        new_labels.append(i.replace('LambdaCandidates_',''))\n",
    "    else:\n",
    "        new_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal = tree_importer.tree_importer_with_cuts('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm_5m_signal.root','plain_tree',3,0,9,0,9,7)\n",
    "gc.collect()\n",
    "\n",
    "signal = df_clean_signal[ (df_clean_signal['mass']>df_clean_signal['mass'].mean()-3*df_clean_signal['mass'].std())\n",
    "               & (df_clean_signal['mass']<df_clean_signal['mass'].mean()+3*df_clean_signal['mass'].std()) ]\n",
    "\n",
    "#signal['issignal']=((signal['issignal']<2)*0 )\n",
    "signal['issignal']=((signal['issignal']>0)*1)\n",
    "signal[\"issignal\"]=signal[\"issignal\"].astype(\"int8\")\n",
    "#signal[\"issignal\"].replace({1: 0, 2: 1}, inplace=True)\n",
    "\n",
    "del df_clean_signal,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_urqmd3 = uproot.open('/media/shahid/KINGSTON/urqmd/c_3_pt_0_9_y_0_9.root:plain_tree',library='np', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(['pT','rapidity','b','issignal'],library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df_clean_urqmd6 = uproot.open('/media/shahid/KINGSTON/urqmd/c_6_pt_0_9_y_0_1.59.root:plain_tree',library='np', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(['pT','rapidity','b','issignal'],library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df_clean_urqmd62 = uproot.open('/media/shahid/KINGSTON/urqmd/c_6_pt_0_9_y_1.59_9.root:plain_tree',library='np', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(['pT','rapidity','b','issignal'],library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df_clean_urqmd9 = uproot.open('/media/shahid/KINGSTON/urqmd/c_9_pt_0_9_y_0_9.root:plain_tree',library='np', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(['pT','rapidity','b','issignal'],library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df_clean_urqmd20 = uproot.open('/media/shahid/KINGSTON/urqmd/c_20_pt_0_9_y_0_9.root:plain_tree',library='np', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(['pT','rapidity','b','issignal'],library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "#df_clean_urqmd['issignal']=((df_clean_urqmd['issignal']>0)*1)\n",
    "#df_clean_urqmd[\"issignal\"]=df_clean_urqmd[\"issignal\"].astype(\"int8\")\n",
    "#df_clean_urqmd = df_clean_urqmd[df_clean_urqmd['issignal']>0]\n",
    "#df_clean_urqmd[\"issignal\"].replace({1: 0, 2: 1}, inplace=True)\n",
    "#df_clean_urqmd=df_clean_urqmd[df_clean_urqmd['b']<1.59]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_clean_urqmd3, df_clean_urqmd6, df_clean_urqmd62,df_clean_urqmd9,df_clean_urqmd20]\n",
    "df_scaled = pd.concat(dfs)\n",
    "del df_clean_urqmd3, df_clean_urqmd6, df_clean_urqmd62,df_clean_urqmd9,df_clean_urqmd20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "with PdfPages('multipage_pdf.pdf') as pdf:\n",
    "    for j, k in [(3, 0), (6, 3),(9, 6),(20, 9)]:\n",
    "        for i in np.unique(df_scaled['issignal']):\n",
    "            df = df_scaled[(df_scaled['issignal']==i)&(df_scaled['b']<j)&(df_scaled['b']>k)]\n",
    "            h = plt.hist2d(df['rapidity'],df['pT'], bins=100, norm=mpl.colors.LogNorm())\n",
    "            v1 = np.linspace(0, h[0].max(), 4, endpoint=True)\n",
    "            cbar = plt.colorbar()\n",
    "            plt.title(\"URQMD %0.1f<\"%k+ \"b < %0.1f\"%j+\"; MC pid = %0.1f\"%i)\n",
    "            plt.xlabel('rapidity', fontsize=18)\n",
    "            plt.ylabel('pT', fontsize=18)\n",
    "            #plt.legend(\"%f\"%i, fontsize=18)\n",
    "            plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "            del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(df_clean_urqmd.isnull().any( axis = 1 ))\n",
    "np.unique(df_clean_urqmd.isin( [np.inf, -np.inf]).any(axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean_urqmd =  tree_importer.tree_importer('/media/shahid/KINGSTON/urqmd/c_3_pt_0_9_y_0_9.root','plain_tree',7)\n",
    "df_clean_urqmd =  tree_importer.tree_importer('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/urqmd/c_0_pt_0_9_y_0_9_urqmd_5.root','plain_tree',7)\n",
    "df_clean_urqmd['issignal']=((df_clean_urqmd['issignal']>0)*1)\n",
    "df_clean_urqmd[\"issignal\"]=df_clean_urqmd[\"issignal\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_urqmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean =  tree_importer.tree_importer('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/c_0_pt_0_9_y_0_9_dcm_5.root','plain_tree',7)\n",
    "df_clean['issignal']=((df_clean['issignal']>0)*1)\n",
    "df_clean[\"issignal\"]=df_clean[\"issignal\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "df_clean = df_clean[df_clean['issignal']>0]\n",
    "df_clean_urqmd = df_clean_urqmd[(df_clean_urqmd['issignal']>0)]\n",
    "\n",
    "h2d_dcm = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3)\n",
    "for i in range(0,len( df_clean['rapidity'])):\n",
    "    h2d_dcm.Fill( df_clean['rapidity'].iloc[i],df_clean['pT'].iloc[i])\n",
    "h2d_dcm1=h2d_dcm.Clone()\n",
    "type(h2d_dcm)\n",
    "\n",
    "h2d_urqmd = ROOT.TH2F(\"urqmd\", \"urqmd\", 15,0,3,15,0,3)\n",
    "for i in range(0,len(df_clean_urqmd['rapidity'])):\n",
    "    h2d_urqmd.Fill(df_clean_urqmd['rapidity'].iloc[i],df_clean_urqmd['pT'].iloc[i])\n",
    "type(h2d_urqmd)\n",
    "h2d_urqmd1=h2d_urqmd.Clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h2d_dcm, h2d_urqmd, h2d_dcm1, h2d_urqmd1, MC_DCM, MC_URQMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = uproot.recreate(\"new_c0_pt_y_y_yield_bdt_cut_0.8.root\")\n",
    "file[\"t1\"] = df_clean_urqmd\n",
    "file[\"t2\"] = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/02\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(7)\n",
    "df_clean = uproot.open(\"new_c0_pt_y_y_yield_bdt_cut_0.8.root:t2\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df_clean_urqmd = uproot.open(\"new_c0_pt_y_y_yield_bdt_cut_0.8.root:t1\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2d_dcm = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3)\n",
    "for i in range(0,len( df_clean['rapidity'])):\n",
    "    h2d_dcm.Fill( df_clean['rapidity'].iloc[i],df_clean['pT'].iloc[i])\n",
    "h2d_dcm1=h2d_dcm.Clone()\n",
    "type(h2d_dcm)\n",
    "\n",
    "h2d_urqmd = ROOT.TH2F(\"urqmd\", \"urqmd\", 15,0,3,15,0,3)\n",
    "for i in range(0,len(df_clean_urqmd['rapidity'])):\n",
    "    h2d_urqmd.Fill(df_clean_urqmd['rapidity'].iloc[i],df_clean_urqmd['pT'].iloc[i])\n",
    "type(h2d_urqmd)\n",
    "h2d_urqmd1=h2d_urqmd.Clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TH2F::Divide>: Cannot divide histograms with different number of bins\n",
      "Error in <TH2F::Divide>: Cannot divide histograms with different number of bins\n",
      "Info in <TCanvas::Print>: png file hists.png has been created\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/dcm_5.root\")\n",
    "MC_DCM=file.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "file1 = ROOT.TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/urqmd/urqmd_5.root\")\n",
    "MC_URQMD=file1.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "\n",
    "c1=ROOT.TCanvas(\"\")\n",
    "c1.Draw()\n",
    "ratio = h2d_dcm.Divide(MC_DCM)\n",
    "h2d_urqmd.Divide(h2d_dcm)\n",
    "h2d_urqmd.Divide(MC_URQMD)\n",
    "ROOT.gStyle.SetPaintTextFormat(\"4.2f\")\n",
    "h2d_urqmd.Draw(\"colz\")\n",
    "h2d_urqmd.SetStats (0)\n",
    "h2d_urqmd.Draw(\"TEXT SAME\")\n",
    "c1.Print (\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean1 =  tree_importer.tree_importer('/media/shahid/KINGSTON/dcm/c_20_pt_0.0_1.0_y_0.0_1.59.root','plain_tree',7)\n",
    "df_clean2 = tree_importer.tree_importer('/media/shahid/KINGSTON/dcm/c_20_pt_0.0_1.0_y_1.59_9.0.root','plain_tree',7)\n",
    "df_clean3 =  tree_importer.tree_importer('/media/shahid/KINGSTON/dcm/c_20_pt_1.0_9.0_y_0.0_1.59.root','plain_tree',7)\n",
    "df_clean4 = tree_importer.tree_importer('/media/shahid/KINGSTON/dcm/c_20_pt_1.0_9.0_y_1.59_9.0.root','plain_tree',7)\n",
    "dfs = [df_clean1, df_clean2, df_clean3,df_clean4]\n",
    "df_clean = pd.concat(dfs)\n",
    "del df_clean2, df_clean1, df_clean3,df_clean4, dfs\n",
    "\n",
    "#df_clean.columns = new_labels\n",
    "df_clean['issignal']=((df_clean['issignal']>0)*1)\n",
    "df_clean[\"issignal\"]=df_clean[\"issignal\"].astype(\"int8\")\n",
    "#df_clean = df_clean[df_clean['issignal']>0]\n",
    "#df_clean[\"issignal\"].replace({1: 0, 2: 1}, inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_clean.isnull().any( axis = 1 ))\n",
    "np.unique(df_clean.isin( [np.inf, -np.inf]).any(axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels=['$\\chi^{2}_{geometrical}$', '$\\chi^{2}_{primary\\ \\pi^-}$','$\\chi^{2}_{primary\\ proton}$', 'DCA (cm)', 'L/$\\Delta$L','mass', '$p_{T}$', '$y_{LAB}$','issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_bac_dist(df1):\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    with PdfPages('multipage_pdf.pdf') as pdf:\n",
    "        df = df1.copy()\n",
    "        for i in df1.columns:\n",
    "            list =['chi2geo', 'chi2primneg', 'chi2primpos', 'chi2topo', 'cosinepos', 'cosinetopo', 'distance', 'l', 'ldl', 'b', 'rapidity','mass', 'pT']\n",
    "            if i in df1.columns:\n",
    "                df[i]=np.log(df[i])\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            with pd.option_context('mode.use_inf_as_na', True):\n",
    "                df = df.dropna()\n",
    "            bin1 = 300 \n",
    "            plt.hist(df[df['issignal']==0][i],bins = bin1, color = 'blue',alpha = 0.3,label='Background')\n",
    "            plt.hist(df[df['issignal']==1][i],bins = bin1, color = 'red',label='Primaries', alpha =0.3)\n",
    "            plt.hist(df[df['issignal']==2][i],bins = bin1, color = 'green',label='Secondaries', alpha =0.3)\n",
    "            plt.hist(df[df['issignal']>2][i],bins = bin1, color = 'yellow',label='>2', alpha =0.3)\n",
    "            plt.yscale('log')\n",
    "            #plt.grid()\n",
    "            plt.ylabel('counts (log scale)', fontsize = 18)\n",
    "            #plt.xlabel('$\\chi^{2}_{geometrical}$', fontsize = 18)\n",
    "            plt.legend(loc='upper right',fontsize=15)\n",
    "            plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "            #ax.text(0, 1500, r'CBM Performance', fontsize=15)\n",
    "            #ax.text(0, 500, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "            plt.title('URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=18)\n",
    "            if i in list:\n",
    "                plt.xlabel(\"log \"+i, fontsize = 18)\n",
    "            else:\n",
    "                plt.xlabel(i, fontsize = 18)\n",
    "            #plt.xlim([1.07,1.2])\n",
    "            #plt.show()\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        del df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,4):\n",
    "    signal[signal['issignal']==i]['l'].hist(bins=1000, label=i, figsize=(12,8))\n",
    "plt.legend(loc='upper right',fontsize=15)\n",
    "plt.xlim(-10,100)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.ylabel('counts (log scale)', fontsize = 18)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Distance between PV and SV',fontsize = 18 )\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hists.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "bin1 = 300 \n",
    "range1=[0,4000]\n",
    "plt.hist(df_urqmd_5k[df_urqmd_5k['issignal']==0]['ldl'],bins = bin1,range=range1, color = 'red',alpha = 0.3,label='Background')\n",
    "plt.hist(df_urqmd_5k[df_urqmd_5k['issignal']==1]['ldl'],bins = bin1, range=range1, color = 'blue',label='Signal', alpha =0.3)\n",
    "#plt.vlines(x=4,ymin=-1,ymax=10000, color='r', linestyle='-')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.ylabel('counts (log scale)', fontsize = 18)\n",
    "plt.xlabel(plot_labels[4], fontsize = 18)\n",
    "plt.legend(loc='upper right',fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.text(0.3, 15000, r'CBM Performance', fontsize=15)\n",
    "ax.text(0.3, 5000, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "#ax.text(4, 10000, r'$PFSimple$', fontsize=20, color ='r')\n",
    "#plt.xlim([0,20])\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig('hists.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data frame object has some columns/features and for them at the very last column the true Monte Carlos information is available. This MC information tells us whether this reconstructed particle was originally produced as a decaying particle or not. So a value of 1 means that it is a true candidate and 0 means that it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which do not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. Ofcourse, we lose some data points but these outliers sometimes cause problems when we perform analysis. \n",
    "\n",
    "Since our experiment is a fixed target experiment so there are certain constraints which have to be applied on the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_bac(MC,data):\n",
    "    signal_selected= MC[MC['issignal']]\n",
    "    background_selected = data[(data['issignal'] == 0)\n",
    "                    & ((data['mass'] > 1.07)\n",
    "                    & (data['mass'] < 1.108) | (data['mass']>1.1227) \n",
    "                       & (data['mass'] < 1.3))].sample(n=3*(signal_selected.shape[0]))\n",
    "    dfs = [signal_selected, background_selected]\n",
    "    df_scaled = pd.concat(dfs)\n",
    "    df_scaled = df_scaled.sample(frac=1)\n",
    "    del dfs, signal_selected, background_selected\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac(signal,df_clean_urqmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Background and Signal\n",
    "Our sample contains a lot of background (2178718) and somewhat signal candidates (36203). For analysis we will use a signal set of 4000 candidates and a background set of 12000 candidates. The background and signal candidates will be selected by using MC information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_selected= signal[(signal['mass']>1.1) & (signal['mass']<1.135)]\n",
    "background_selected = df_clean_urqmd[(df_clean_urqmd['issignal'] == 0)\n",
    "                & ((df_clean_urqmd['mass'] > 1.077)\n",
    "                & (df_clean_urqmd['mass'] < 1.1) | (df_clean_urqmd['mass']>1.135) \n",
    "                   & (df_clean_urqmd['mass'] < 1.3))].sample(n=3*(signal_selected.shape[0]))\n",
    "gc.collect()\n",
    "\n",
    "#Let's combine signal and background\n",
    "dfs = [signal_selected, background_selected]\n",
    "df_scaled = pd.concat(dfs)\n",
    "\n",
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "#del dfs, signal_selected, background_selected, signal\n",
    "# Let's take a look at the top 10 entries of the df\n",
    "df_scaled.iloc[0:10,:]\n",
    "del signal, signal_selected, background_selected, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaled.shape)\n",
    "print(df_scaled[df_scaled['issignal']==1].shape)\n",
    "print(df_scaled[df_scaled['issignal']==2].shape)\n",
    "(5805268, 18)\n",
    "(1056684, 18)\n",
    "(394633, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(df_clean_urqmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 0.5 * (np.log(E+P/E-P))\n",
    "\n",
    "\n",
    "https://cbm-wiki.gsi.de/foswiki/bin/view/PWG/CbmCollisionEnergies\n",
    "\n",
    "\n",
    "y = 0.5 * (np.log((12+10)/(12-10)))\n",
    "\n",
    "\n",
    "using this the rapidity is 3.1992 for Ebeam =12.04 and pbeam =12 and mid rapidity is y/2=  1.5996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt_sig_back(df_scaled)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(8)\n",
    "axs.text(1.13, 6000, r'DCM-QGSM-SMM', color = 'magenta',  fontsize=15)\n",
    "axs.text(1.13, 4000, r'Au+Au @ 12 $A$GeV/$c$', color = 'magenta',  fontsize=15)\n",
    "axs.text(1.13, 2000, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "fig.savefig(\"hists.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1 = signal_selected[signal_selected['issignal']==0]\n",
    "new2 = signal_selected[signal_selected['issignal']==1]\n",
    "new3 = new1.sample(n=1*(new2.shape[0]))\n",
    "dfs = [new2, new3]\n",
    "df_scaled = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_selected.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2geo', 'chi2primneg', 'chi2primpos', 'distance', 'ldl']\n",
    "#cuts = [ 'chi2geo', 'chi2primneg', 'chi2primpos', 'chi2topo', 'cosinepos', 'cosinetopo', 'distance', 'l', 'ldl', 'b']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "x_whole = df_clean[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFPF_lambda_cuts(df):\n",
    "    KFPF_lambda= df.copy()\n",
    "    KFPF_lambda['new_signal']=0\n",
    "    mask1 = (KFPF_lambda['chi2primpos'] > 18.4) & (KFPF_lambda['chi2primneg'] > 18.4)\n",
    "\n",
    "    mask2 = (KFPF_lambda['ldl'] > 5) & (KFPF_lambda['distance'] < 1)\n",
    "\n",
    "    mask3 = (KFPF_lambda['chi2geo'] < 3) \n",
    "\n",
    "    KFPF_lambda = KFPF_lambda[(mask1) & (mask2) & (mask3)] \n",
    "\n",
    "    #After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "    # column by 1\n",
    "    KFPF_lambda['new_signal'] = 1\n",
    "    return KFPF_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a new df \n",
    "\n",
    "#new_check_set=KFPF_lambda_cuts(df_original)\n",
    "new_check_set=KFPF_lambda_cuts(df_clean_urqmd)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>XGB Boost \n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient. Stratify so that both train and test get the same ratio of signal to background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=324, stratify=y)\n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_1 = df_clean_urqmd[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole_1 = pd.DataFrame(df_clean_urqmd['issignal'], dtype='int8')\n",
    "dtest2 = xgb.DMatrix(x_whole_1, label = y_whole_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_1 = df_clean_urqmd_bac[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole_1 = pd.DataFrame(df_clean_urqmd_bac['issignal'], dtype='int')\n",
    "dtest2 = xgb.DMatrix(x_whole_1, label = y_whole_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate,scale_pos_weight):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,'scale_pos_weight':scale_pos_weight,\n",
    "              'subsample': 0.8,\n",
    "              'eval_metric': 'auc','tree_method':'hist','objective':'binary:logistic', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0.01,1),\n",
    "                                             'n_estimators':(100,500),'scale_pos_weight':(1,10)\n",
    "                                            })\n",
    "\n",
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")\n",
    "#0.9983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,∞]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "\n",
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8, \n",
    "              'num_class':np.unique(dtrain.get_label()).shape[0], \n",
    "              'eval_metric': 'auc','tree_method':'hist', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0.01,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")\n",
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')\n",
    "#0.9951\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import sqrt, log, argmax\n",
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its\n",
    "discrimination threshold is varied. This function requires the true binary value and the target scores, which can either be probability estimates of\n",
    "the positive class, confidence values, or binary decisions.\n",
    "The function roc_auc_score computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "To find the best threshold which results more signal to background ratio for lambda candidates we use the parameter S0 called the approximate median significance\n",
    "by the higgs boson  ML challenge (http://higgsml.lal.in2p3.fr/documentation,9.)\n",
    "\"\"\"\n",
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linewidth=3 ,linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random guess')\n",
    "    #plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n AMS = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', s=80, color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n AMS = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 18)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('hists.png')\n",
    "    plt.show()\n",
    "    return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict, average='macro', multi_class='raise')\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1, average='macro', multi_class='raise')\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linewidth=3 ,linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random guess')\n",
    "    #plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n AMS = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', s=80, color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n AMS = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 18)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('hists.png')\n",
    "    plt.show()\n",
    "    return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_urqmd_bac['xgb_preds'] = bst.predict(dtest2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), \n",
    "         'objective':'binary:logistic','tree_method':'hist','nthread' : 7}\n",
    "\n",
    "#Fit/train on training data\n",
    "bst = xgb.train(param, dtrain, num_boost_round=20)\n",
    "\n",
    "#predicitions on training set\n",
    "bst_train= pd.DataFrame(data=bst.predict(dtrain),  columns=[\"xgb_preds\"])\n",
    "y_train=y_train.set_index(np.arange(0,bst_train.shape[0]))\n",
    "bst_train['issignal']=y_train['issignal']\n",
    "\n",
    "#predictions on test set\n",
    "bst_test = pd.DataFrame(data=bst.predict(dtest1),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']\n",
    "\n",
    "#ROC cures for the predictions on train and test sets\n",
    "train_best, test_best = AMS(y_train, bst_train['xgb_preds'],y_test, bst_test['xgb_preds'])\n",
    "\n",
    "#The first argument should be a data frame, the second a column in it, in the form 'preds'\n",
    "preds_prob(bst_train,'xgb_preds', 'issignal',bst_test,'xgb_preds', 'issignal')\n",
    "\n",
    "#Applying XGB on the 100k events data-set\n",
    "df_clean['xgb_preds'] = bst.predict(dtest)\n",
    "#preds_prob(df_clean,'xgb_preds', 'issignal','test')\n",
    "\n",
    "df_clean_urqmd['xgb_preds'] = bst.predict(dtest2)\n",
    "#del x_test, y_test, x_train, y_train, dtrain, dtest, x_whole, y_whole, x_whole_1, y_whole_1, dtest1, df_scaled\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_test, y_test, x_train, y_train, dtrain, dtest, x_whole, y_whole, x_whole_1, y_whole_1, dtest1, df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), \n",
    "         'objective':'binary:logistic','tree_method':'hist','nthread' : 7}\n",
    "\n",
    "#Fit/train on training data\n",
    "bst = xgb.XGBClassifier(**param).fit(x_train, y_train)\n",
    "\n",
    "#predicitions on training set\n",
    "bst_train= pd.DataFrame(data=bst.predict_proba(x_train))\n",
    "y_train=y_train.set_index(np.arange(0,bst_train.shape[0]))\n",
    "bst_train['issignal']=y_train['issignal']\n",
    "\n",
    "bst_test = pd.DataFrame(data=bst.predict_proba(x_test))\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']\n",
    "\n",
    "#Applying XGB on the 100k events data-set\n",
    "bst_test1 = pd.DataFrame(data=bst.predict_proba(x_whole))\n",
    "df_clean['xgb_preds0'], df_clean['xgb_preds1'], df_clean['xgb_preds2']= bst_test1[0], bst_test1[1], bst_test1[2]\n",
    "#preds_prob(df_clean,'xgb_preds', 'issignal','test')\n",
    "\n",
    "bst_test2 = pd.DataFrame(data=bst.predict_proba(x_whole_1))\n",
    "df_clean_urqmd['xgb_preds0'], df_clean_urqmd['xgb_preds1'], df_clean_urqmd['xgb_preds2']= bst_test2[0], bst_test2[1], bst_test2[2]\n",
    "\n",
    "\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "784/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following graph will show us that which features are important for the model\n",
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "#ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df3_base['xgb_preds1'], bins=300, label=0)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#plt.hist(bst_test['xgb_preds'], bins=300)\n",
    "#plt.yscale('log')\n",
    "bst_test['xgb_preds'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_clean['xgb_preds0'], bins=300, label=0)\n",
    "plt.hist(df_clean['xgb_preds1'], bins=300, label=1)\n",
    "plt.hist(df_clean['xgb_preds2'], bins=300, label=2)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_clean_urqmd_bac['xgb_preds'], bins=300, label=0)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = preds_prob(bst_test,'xgb_preds0', 'issignal','test')\n",
    "fig.savefig('hists.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_test = pd.DataFrame(data=bst.predict(x_test),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_prob(df,preds,true,df1,preds1, true1):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bins1=100\n",
    "    TP = df[(df[true]==1)]\n",
    "    TN = df[(df[true]==0)]\n",
    "    \n",
    "    plt.hist(TN[preds], bins=bins1,facecolor='blue',alpha = 0.3, label='background in train')\n",
    "    plt.hist(TP[preds], bins=bins1,facecolor='red',alpha = 0.3, label='signal in train')\n",
    "    \n",
    "    \n",
    "    TP1 = df1[(df1[true1]==1)]\n",
    "    TN1 = df1[(df1[true1]==0)]\n",
    "    \n",
    "    hist1, bins1 = np.histogram(TN1[preds1], bins=bins1)\n",
    "    err1 = np.sqrt(hist1)\n",
    "    center1 = (bins1[:-1] + bins1[1:]) / 2\n",
    "    plt.errorbar(center1, hist1, yerr=err1, fmt='o',\n",
    "                 c='blue', label='background in test')\n",
    "    \n",
    "    hist, bins = np.histogram(TP1[preds1], bins=bins1)\n",
    "    err = np.sqrt(hist)\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o',\n",
    "                 c='red', label='signal in test')\n",
    "    \n",
    "   # ax.annotate('cut on probability', xy=(0, 90),  xycoords='data',xytext=(0.13,0.5), textcoords='axes fraction',\n",
    "    #            fontsize=15,arrowprops=dict(facecolor='black', shrink=0.05),horizontalalignment='right', verticalalignment='top')\n",
    "    \n",
    "\n",
    "    \n",
    "    if df[true].unique().shape[0]>2:\n",
    "        TP2= df[df[true]>1]\n",
    "        plt.hist(TP2[preds], bins=bins1,facecolor='green',alpha = 0.3, label='secondaries in train')\n",
    "        TP2= df1[df1[true1]>1]\n",
    "        hist2, bins2 = np.histogram(TP2[preds1], bins=bins1)\n",
    "        center2 = (bins2[:-1] + bins2[1:]) / 2\n",
    "        err2 = np.sqrt(hist2)\n",
    "        plt.errorbar(center2, hist2,yerr=err2, fmt='o',c='green',label='secondaries in test')\n",
    "\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Probability',fontsize=18)\n",
    "    plt.ylabel('Counts', fontsize=18)\n",
    "    ax.set_xticks(np.arange(0,1.1,0.1))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    plt.legend(fontsize=18)\n",
    "    fig.show()\n",
    "    fig.tight_layout()\n",
    "    return fig, ax\n",
    "    #fig.savefig('Lambda_XGB_prediction_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "h=plt.hist2d(df_clean['rapidity'],df_clean['pT'], bins=15*25, norm=mpl.colors.LogNorm())\n",
    "plt.xlabel('$y_{Lab}$',fontsize=15)\n",
    "plt.ylabel(\"pT\",fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('hists.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_train1 = bst_train[(bst_train[0]<0.35) ]\n",
    "bst_test1 = bst_test[(bst_test[0]<0.35) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base = df_clean[(df_clean['rapidity']<0.5) & (df_clean['pT']<0.5) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3_base[df3_base['issignal']==0].shape[0]/529256.0)\n",
    "print(df3_base[df3_base['issignal']==1].shape[0]/101)\n",
    "print(df3_base[df3_base['issignal']>1].shape[0]/35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_prob(bst_train,0, 'issignal',bst_test,0, 'issignal')\n",
    "#for i in ['xgb_preds0','xgb_preds1','xgb_preds2']:\n",
    "for i in ['xgb_preds0']:\n",
    "    fig, ax = preds_prob(df_clean_urqmd,i, 'issignal',df_clean_urqmd,i, 'issignal')\n",
    "    plt.legend([\"back\",\"prim\",\"second\"],fontsize=18)\n",
    "    fig.savefig(\"hists\"+str(i)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import sqrt, log, argmax\n",
    "import itertools\n",
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    #roc_auc=roc_auc_score(y_true, y_predict, multi_class='ovo', average='weighted')\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    #S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    #S0 = S0[~np.isnan(S0)]\n",
    "    #xi = argmax(S0)\n",
    "    #S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    #roc_auc1=roc_auc_score(y_true1, y_predict1, multi_class='ovo', average='weighted')\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    #S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    #S01 = S01[~np.isnan(S01)]\n",
    "    #xi1 = argmax(S01)\n",
    "    #S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linewidth=3 ,linestyle=':',color='darkorange')\n",
    "    plt.plot(fpr1, tpr1, color='green')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random guess')\n",
    "    #plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n AMS = '+ \"%.2f\" % S0[xi])\n",
    "    #plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', s=80, color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n AMS = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 18)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('hists.png')\n",
    "    plt.show()\n",
    "    #return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best, test_best = AMS(y_train, bst_train[1],y_test, bst_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will display the inavriant mass histogram of the original 10k event set along with the mass histoigram after we apply a cut\n",
    "# on the probability prediction of xgb\n",
    "def cut_visualization(df, variable,cut, range1=(1.09, 1.15), bins1= 300 ):\n",
    "    mask1 = df[variable]>cut\n",
    "    df3=df[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(8, 6), dpi = 300)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(df['mass'],bins = bins1, range=range1, facecolor='blue' ,alpha = 0.35, label='no selection')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc=(0.55, 0.55))\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax2.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "    \n",
    "    \n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label=\"XGB  > %.2f\"%cut+'')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax1.legend( fontsize = 15,loc=(0.55, 0.47) )\n",
    "\n",
    "    #plt.title(\"The original sample's Invariant Mass along with mass after selection of XGB\", fontsize = 15)\n",
    "    plt.text(1.09, 6000, 'CBM Performance', fontsize=15)\n",
    "    plt.text(1.09, 5500, 'URQMD, Au+Au$', fontsize=15)\n",
    "    plt.text(1.09, 5000, '@ 12A GeV/$c$', fontsize=15)\n",
    "    plt.text(1.13, 6000, '$\\Lambda$ hyperons', fontsize=15)\n",
    "    #plt.text(0.02, 0.1, r'cut > %.4f'%cut, fontsize=15)\n",
    "    plt.show()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"Fig2_Lambda_XGB_selection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_visualization(df_clean_urqmd,'xgb_preds',test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range1= (1.11, 1.122)\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.1) ]['mass'].plot.hist(bins = bins1, range=range1, histtype='step', Fill=False,sharey=True, label='BDT > 0.1 XGB selected $\\Lambda$s')\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.5) ]['mass'].plot.hist(bins = bins1, range=range1, histtype='step', Fill=False,sharey=True, label='BDT > 0.5 XGB selected $\\Lambda$s')\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.8) ]['mass'].plot.hist(bins = bins1, range=range1, histtype='step', Fill=False,sharey=True, label='BDT > 0.8 XGB selected $\\Lambda$s')\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.9) ]['mass'].plot.hist(bins = bins1, range=range1, color = 'red', histtype='step', Fill=False,sharey=True, label='BDT > 0.9 XGB selected $\\Lambda$s')\n",
    "\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.93\n",
    "df3_base=df_clean_urqmd[(df_clean_urqmd['xgb_preds']>cut3) ]\n",
    "#df3_base=df3_base[df3_base['xgb_preds1']>0.45]\n",
    "#df3_base=df3_base[(df3_base['xgb_preds2']<0.45)&(df3_base['xgb_preds2']>0.15)]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "#xgb\n",
    "\n",
    "#issignal has 0,1,2 . So we convert all signals above zero to 1\n",
    "\n",
    "\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True, '\\n True positives = \\n (MC =1)\\n signal in \\n the distribution')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = bins1, range=range1,facecolor='magenta',alpha = 0.3,grid=True,sharey=True )\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"XGB selected $\\Lambda$ candidates with a cut of %.3f \"%cut3 +\"on the XGB back probability distribution\", fontsize = 18)\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "#axs.text(1.123, 4000, 'CBM Performance', fontsize=18)\n",
    "#axs.text(1.123, 3500, 'URQMD, Au+Au @ 12A GeV/$c$', fontsize=18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(df3_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base[(df3_base['issignal']==0)&(df3_base['chi2topo']<50)]['chi2topo'].hist(bins =100)\n",
    "df3_base[(df3_base['issignal']==1) &(df3_base['chi2topo']<50)]['chi2topo'].hist(bins =100)\n",
    "plt.xlim(0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3_base[(df3_base['issignal']==0)&(df3_base['chi2topo']<5)].shape[0])\n",
    "print(df3_base[(df3_base['issignal']==1) &(df3_base['chi2topo']<5)].shape[0])\n",
    "print(df3_base[df3_base['issignal']>1].shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base['cosinetopo'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.9\n",
    "#mask1 = df_clean_urqmd['xgb_preds']>cut3\n",
    "#df3_base0=df_clean_urqmd[mask1]\n",
    "#df3_base = pd.concat([df_clean_urqmd,df3_base0]).drop_duplicates(keep=False)\n",
    "mask1 = df_clean_urqmd['xgb_preds']>cut3\n",
    "df3_base=df_clean_urqmd[mask1]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True, label ='\\n True positives = \\n (MC =1)\\n signal in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "plt.title(\"XGB selected $\\Lambda$ candidates with a cut of %.3f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.text(1.123, 5000, 'CBM Performance', fontsize=18)\n",
    "axs.text(1.123, 4000, 'URQMD, Au+Au @ 12A GeV/$c$', fontsize=18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.9\n",
    "mask1 = df_clean_urqmd['xgb_preds']>cut3\n",
    "df3_base=df_clean_urqmd[mask1]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "plt.title(\"XGB selected $K_{s}$ candidates with a cut of %.3f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficiency_plot_mass(df,signal_column, predictions_column, cut_value, range_min, range_max, bin1):\n",
    "    from matplotlib import gridspec\n",
    "    x_min, x_max = range_min , range_max\n",
    "    range1= (x_min, x_max)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1,figsize=(10,10), sharex=True, constrained_layout=True,  gridspec_kw={'width_ratios': [10],\n",
    "                               'height_ratios': [8,4]})\n",
    "    \n",
    "    ns, bins, patches=axs[0].hist((df[(df[predictions_column]>cut_value) & (df[signal_column]==1)]['mass']),bins = bin1,histtype='step', range=range1,Fill=False, color='red', facecolor='red', linewidth=2)\n",
    "    ns1, bins1, patches1=axs[0].hist((df[df[signal_column]==1]['mass']),bins = bin1,histtype='step', Fill=False, range=range1,facecolor='blue',linewidth=2)\n",
    "\n",
    "    #plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "    axs[0].set_ylabel(\"log(counts)\", fontsize = 18)\n",
    "    axs[0].legend(('XGBoost TP','MC TP'), fontsize = 18, loc='upper right')\n",
    "    axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "    axs[0].set_yscale('log')\n",
    "\n",
    "    err = np.std(ns)\n",
    "    err1 = np.std(ns1)\n",
    "    corr_ns_ns1 = np.corrcoef(ns,ns1)[[0],[1]][0]\n",
    "    err_dif = (ns / ns1) * (np.sqrt( ((err/ns)**2) + ((err1/ns1)**2)\n",
    "                                      -2* ((corr_ns_ns1*err*err1)/(ns*ns1))))\n",
    "\n",
    "\n",
    "    axs[1].hlines(y=1, xmin=x_min, xmax=x_max, colors='black', linestyles='dashed', label='')\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    axs[1].errorbar(center,  ns / ns1, yerr=err_dif,  fmt='o',\n",
    "                     c='Blue', label='Background in predictions')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "    axs[1].set_ylabel(\"XGB / MC\", fontsize = 18)\n",
    "    axs[1].grid()\n",
    "    axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "    fig.show()\n",
    "    fig.tight_layout()\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_plot_mass(df_clean_urqmd,'issignal', 'xgb_preds', 0.96, 1.112, 1.12, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pT_vs_rapidity(df, var_xaxis , var_yaxis , range_var_xaxis, range_var_yaxis,signal_column, predictions_column, cut_value):\n",
    "    import matplotlib as mpl\n",
    "    fig, axs = plt.subplots(figsize=(6, 4),dpi = 300)\n",
    "    h=plt.hist2d(df[(df[predictions_column]>cut_value) & (df[signal_column]==1)][var_xaxis],\n",
    "                                 df[(df[predictions_column]>cut_value) & (df[signal_column]==1)][var_yaxis],\n",
    "                                 range=[range_var_xaxis,range_var_yaxis], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())\n",
    "    cbar = plt.colorbar()\n",
    "    h=plt.hist2d(df[df[signal_column]==1][var_xaxis],df[df[signal_column]==1][var_yaxis],range=[range_var_xaxis,range_var_yaxis], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())\n",
    "    axs.hist2d(h[0],h[1])\n",
    "    #v1 = np.linspace(0, h[0].max(), 4, endpoint=True)\n",
    "    #cbar = fig.colorbar(h[3], ticks = v1 )\n",
    "    #cbar.set_ticks([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    #cbar.set_ticklabels([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    \n",
    "    #v1 = np.linspace(Z.min(), Z.max(), 8, endpoint=True)\n",
    "    #cbar=plt.colorbar(ticks=v1)              # the mystery step ???????????\n",
    "    #cbar.ax.set_yticklabels([ '0', '1784', '3568', '5353']) # add the labels\n",
    "    \n",
    "\n",
    "    \n",
    "    #plt.vlines(x=1.59,ymin=-1,ymax=2.4, color='r', linestyle='-')\n",
    "    #plt.hlines(y=bins4[1], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=bins4[2], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "\n",
    "    #plt.hlines(y=0.4, xmin=-0.1, xmax=df[var_xaxis].max(), colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.2, xmin=-0.1, xmax=1.5996, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.9, xmin=-0.1, xmax=3.5, colors='b', linestyles='solid', label='')\n",
    "    plt.xlabel('$y_{Lab}$', fontsize=20)\n",
    "    plt.ylabel('$p_{T}$ (GeV/$c$)', fontsize=18)\n",
    "    axs.text(0.02, 3, r'CBM Performance', fontsize=15)\n",
    "    axs.text(0.02, 2.8, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15, color ='r')\n",
    "    axs.text(1.2, 0.6, r'$y_{CM}$', fontsize=20, color ='r')\n",
    "    axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "    axs.grid(b=True, animated=True )\n",
    "    axs.set_xticks(np.arange(0,17)*0.2+0)\n",
    "    axs.set_xticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    axs.set_yticks(np.arange(0,16)*0.2+0)\n",
    "    axs.set_yticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    #plt.title(\"  y-$p_{T}$ plot for signal candidates (MC=1) with a cut = %.2f\"%0.95,  fontsize=18)\n",
    "    #plt.grid(which='both', ydata =yy)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity.png\")\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "h=plt.hist2d(df_clean_urqmd['rapidity'],df_clean_urqmd['pT'],range=[3,3], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pT_vs_rapidity(df_clean_urqmd,'rapidity','pT',3,3, 'issignal','xgb_preds', 0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base[df3_base['issignal']==2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ 97718      0]\n",
    " [186553      0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(32947+76208)/97718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "Thus in binary classification, the count of true positives is $C_{0,0}$, false positives is $C_{1,0}$, true negatives is $C_{1,1}$ and false negatives is $C_{0,1}$.\n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "cut3 = test_best\n",
    "df_clean['xgb_preds'] = ((df_clean['xgb_preds']>cut3)*1)\n",
    "cnf_matrix = confusion_matrix(df_clean['issignal'], df_clean['xgb_preds'], labels=[2,1,0])\n",
    "#cnf_matrix = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['secondaries','primaries','background'], title='Confusion Matrix for XGB for cut > '+str(cut3))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "df_cm = pd.DataFrame(cnf_matrix, ['secondaries','primaries','background'], ['secondaries','primaries','background'])\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion matrix, without normalization\n",
    "[[       0    37483     3232]\n",
    " [       0   101579     8955]\n",
    " [       0   437569 45809781]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cnf_matrix[[0]][0][1])/ (cnf_matrix[[0]][0][0]+cnf_matrix[[0]][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(bst,num_trees=2)\n",
    "plt.rcParams['figure.figsize'] = [80, 160]\n",
    "plt.rcParams['figure.dpi']=300\n",
    "#plt.show()\n",
    "plt.savefig(\"hists.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.to_graphviz(xg_reg, fmap='', num_trees=0, rankdir=None, yes_color=None, no_color=None, condition_node_params=None, leaf_node_params=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.08, 1.15)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(10,10), sharex=True, constrained_layout=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base['mass']),bins = 50,histtype='step', range=range1,Fill=False, color='red', facecolor='red', linewidth=2)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 50,histtype='step', Fill=False, range=range1,facecolor='blue',linewidth=2)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 18)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost','KFPF '), fontsize = 18, loc='upper right')\n",
    "\n",
    "\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "axs[1].hlines(y=1, xmin=1.112, xmax=1.12, colors='black', linestyles='dashed', label='')\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "err = np.std(ns)\n",
    "err1 = np.std(ns1)\n",
    "corr_ns_ns1 = np.corrcoef(ns,ns1)[[0],[1]][0]\n",
    "err_dif = (ns / ns1) * (np.sqrt( ((err/ns)**2) + ((err1/ns1)**2)\n",
    "                                      -2* ((corr_ns_ns1*err*err1)/(ns*ns1))))\n",
    "axs[1].errorbar(center,  ns / ns1,   fmt='o',\n",
    "                 c='Blue', label='Background in predictions')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 18)\n",
    "#axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "fig.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base[df3_base['issignal']>0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = plt.figure(constrained_layout=True)\n",
    "widths = [2, 3, 1.5]\n",
    "heights = [1, 3, 2]\n",
    "spec5 = fig5.add_gridspec(ncols=3, nrows=3, width_ratios=widths,\n",
    "                          height_ratios=heights)\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtest, dtrain, dtest1, df_scaled, x, y, x_whole, y_whole, x_train, x_test, y_train, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_100k = df_clean.copy()\n",
    "del df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bdt cut 0.7\n",
    "df0 = df3_base\n",
    "df0 = df0[(df0['mass']>1.07)&(df0['mass']<1.3)]\n",
    "df0 = df0[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bdt cut 0.8\n",
    "df1 = df3_base\n",
    "df1 = df1[(df1['mass']>1.07)&(df1['mass']<1.3)]\n",
    "df1 = df1[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bdt cut 0.9\n",
    "df2 = df3_base\n",
    "df2 = df2[(df2['mass']>1.07)&(df2['mass']<1.3)]\n",
    "df2 = df2[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.92\n",
    "df3 = df3_base\n",
    "df3 = df3[(df3['mass']>1.07)&(df3['mass']<1.3)]\n",
    "df3 = df3[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best\n",
    "df4 = df3_base\n",
    "df4 = df4[(df4['mass']>1.07)&(df4['mass']<1.3)]\n",
    "df4 = df4[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best\n",
    "df4_urqmd = df3_base\n",
    "df4_urqmd = df4_urqmd[(df4_urqmd['mass']>1.07)&(df4_urqmd['mass']<1.3)]\n",
    "df4_urqmd = df4_urqmd[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df4[df4['issignal']==1]\n",
    "#df4 = df4[['rapidity', 'mass', 'pT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyRoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, ROOT\n",
    "from ROOT import TF1, TCanvas,TMath, TColor\n",
    "\n",
    "class Linear:\n",
    "    def __call__( self, x, par ):\n",
    "        return par[0] + x[0]*par[1]\n",
    "\n",
    "class lorenztian:\n",
    "    def _call_(self, x, p):\n",
    "        return 0.5*p[0]*p[1] /( ((x[0]-p[2])**2) + ((0.5 * (p[1])**2))) \n",
    "\n",
    "class gaus:\n",
    "    def _call_(self, x ,p):\n",
    "        return p[0]*np.exp(-0.5*((x[0]-p[2])/p[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def truncate(number, decimals=2):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "\n",
    "def background_selector(df):\n",
    "    df1 = df[(df['mass']<1.108)]\n",
    "    df2 = df[df['mass']>1.13]\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    return df3['mass'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate(0.39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentile binning\n",
    "#df0 = df3_base\n",
    "#df = df0[(df0['mass']>1.07)&(df0['mass']<1.3)]\n",
    "df = df4\n",
    "out0, bins0 =pd.qcut(df['rapidity'], q=4, retbins=1)\n",
    "lowest_rapidity = df[df['rapidity']<bins0[1]]\n",
    "low_rapidity = df[(df['rapidity']>bins0[1])&(df['rapidity']<bins0[2])]\n",
    "mid_rapidity = df[(df['rapidity']>bins0[2])&(df['rapidity']<bins0[3])]\n",
    "high_rapidity = df[(df['rapidity']>bins0[3])]\n",
    "    \n",
    "out1, bins1 =pd.qcut(lowest_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_lowest_rapidity = lowest_rapidity[lowest_rapidity['pT']<bins1[1]]\n",
    "mid_pT_lowest_rapidity = lowest_rapidity[(lowest_rapidity['pT']>bins1[1]) & (lowest_rapidity['pT']<bins1[2])]\n",
    "high_pT_lowest_rapidity =lowest_rapidity[(lowest_rapidity['pT']>bins1[2])]\n",
    "\n",
    "out2, bins2 =pd.qcut(low_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_low_rapidity = low_rapidity[low_rapidity['pT']<bins2[1]]\n",
    "mid_pT_low_rapidity = low_rapidity[(low_rapidity['pT']>bins2[1]) & (low_rapidity['pT']<bins2[2])]\n",
    "high_pT_low_rapidity= low_rapidity[(low_rapidity['pT']>bins2[2])]\n",
    "    \n",
    "out3, bins3 =pd.qcut(mid_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_mid_rapidity = mid_rapidity[mid_rapidity['pT']<bins3[1]]\n",
    "mid_pT_mid_rapidity = mid_rapidity[(mid_rapidity['pT']>bins3[1]) & (mid_rapidity['pT']<bins3[2])]\n",
    "high_pT_mid_rapidity=mid_rapidity[(mid_rapidity['pT']>bins3[2])]\n",
    "    \n",
    "out4, bins4 =pd.qcut(high_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_high_rapidity = high_rapidity[high_rapidity['pT']<bins4[1]]\n",
    "mid_pT_high_rapidity = high_rapidity[(high_rapidity['pT']>bins4[1]) & (high_rapidity['pT']<bins4[2])]\n",
    "high_pT_high_rapidity=high_rapidity[(high_rapidity['pT']>bins4[2])]\n",
    "\n",
    "#del out0, lowest_rapidity, mid_rapidity, high_rapidity, out1, out2, out3, out4, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_selector(df):\n",
    "    df1 = df[(df['mass']<1.108)]\n",
    "    df2 = df[df['mass']>1.13]\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    return df3['mass'] \n",
    "\n",
    "list1 = [low_pT_lowest_rapidity, mid_pT_lowest_rapidity, high_pT_lowest_rapidity, low_pT_low_rapidity,\n",
    "         mid_pT_low_rapidity, high_pT_low_rapidity, low_pT_mid_rapidity, mid_pT_mid_rapidity,\n",
    "        high_pT_mid_rapidity, low_pT_high_rapidity, mid_pT_high_rapidity, high_pT_high_rapidity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4['mass'].describe()[1]-1.2*(df4['mass'].describe()[2])+0.2* (df4['mass'].describe()[2])\n",
    "df4['mass'].describe()[1]+1.2*(df4['mass'].describe()[2])+0.2* (df4['mass'].describe()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorentzian\n",
    "Lorenztian with second chebyshev 2nd order polynom\n",
    "\n",
    "The describe of BDT score > 70 shows that the sigma of the data mean is at 1.178052 with an std of 0.059818. So 1.55sigma below the mean is 1.0883250000000002 and 1.55 sigma above the mean is 1.267779. So let's choose 1.55, 1.5 and 1.45 below the mean and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol2 = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "pol3 = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "one_var_pol2=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "one_var_pol3=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lorentzian + second order pol\n",
    "lorentzian_pol2 = []\n",
    "pt_min=[]\n",
    "y_min = []\n",
    "#lorentzian_3rd_order_pol = []\n",
    "\n",
    "\n",
    "df = df4_urqmd\n",
    "\n",
    "\n",
    "mass_range_min = [df['mass'].describe()[1]-1.2*(df['mass'].describe()[2])]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "\n",
    "y_bin_low = -0.2\n",
    "y_bin_up =0.0\n",
    "for i in range(0,15,1):\n",
    "    y_bin_low = truncate(y_bin_low + 0.2)\n",
    "    y_bin_up = truncate(y_bin_up+0.2)\n",
    "    df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "    \n",
    "    pt_bin_low =-0.2\n",
    "    pt_bin_up =0\n",
    "    for i in range(0,15,1):\n",
    "        pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "        pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "        df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "        mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "        \n",
    "        for mm in mass_range_min:\n",
    "            for mmm in range(0,3,1):\n",
    "                \n",
    "                #canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "                #canvas.Draw()\n",
    "                #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "                binning = [40,70,100,130]\n",
    "                for b in binning:\n",
    "                    tot_sig_3_sigma = 0\n",
    "                    tot_bac_3_sigma = 0\n",
    "                    tot_sig_3_point_5_sigma = 0\n",
    "                    tot_bac_3_point_5_sigma = 0\n",
    "                    tot_sig_2_point_5_sigma = 0\n",
    "                    tot_bac_2_point_5_sigma = 0\n",
    "                    tot_sig_2_sigma = 0\n",
    "\n",
    "\n",
    "                    #step 0\n",
    "                    if df_pt.shape[0]>500:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0);\n",
    "                        #fb =TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"RIEMQN\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        h1.Fit(f1,\"RNIQ\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        #canvas .Clear ()\n",
    "                        #pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        #pad1 . Draw ()\n",
    "                        #pad1 . cd ()\n",
    "                        #pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetNpx(100000);\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3],par1[4]);\n",
    "                        #f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIRQ\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs.SetNpx(100000);\n",
    "                        #fs.SetLineColor(ROOT.kGreen)\n",
    "                        #fb.SetLineStyle(4)\n",
    "                        #fb.SetLineColor(ROOT.kBlue)\n",
    "                        #fb.SetNpx(100000);\n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5]);\n",
    "                        #fb.SetParameters(par2[3],par2[4],par2[5],par2[6]);\n",
    "\n",
    "\n",
    "                        #h1.SetTitleOffset(-1)\n",
    "                        #h1.SetFillStyle(3003);\n",
    "                        #h1.SetLineWidth(2)\n",
    "                        #h1.SetStats (0)\n",
    "                        #h1.SetYTitle(\"Entries\")\n",
    "                        #h1.SetLineColor(ROOT.kBlack)\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        #h3.SetLineWidth(2)\n",
    "                        #h3.SetStats (0)\n",
    "                        #h3.GetXaxis().SetTitle(\"Mass (GeV/c^2)\")\n",
    "\n",
    "                        #h1.Draw(\"pe\")\n",
    "                        #fs.Draw(\"SAME\")\n",
    "                        #fb.Draw(\"SAME\")\n",
    "                        #f2.Draw(\"SAME\")\n",
    "\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "                        #h2.Sumw2()\n",
    "\n",
    "                        integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "                        integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "                        binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "                        #tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                        #sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "\n",
    "                        signal_under_peak = (fs.Integral(integral_min,integral_max)/binwidth);\n",
    "                        if signal_under_peak>0:\n",
    "                            tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak                \n",
    "                        #sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "                        #man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "                        if sigma_signal_under_peak!=0:\n",
    "                            print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "                        \n",
    "                        backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "                        #if backgnd_under_peak<0:\n",
    "                            #print('Negative background')\n",
    "\n",
    "                        Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "\n",
    "                        signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_3_point_5_sigma>0:\n",
    "                            tot_sig_3_point_5_sigma= tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma                \n",
    "                        #tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_point_5_sigma>0:\n",
    "                            tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "                        #tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_sigm = (fs.Integral(par2[2] - (TMath.Abs(2*par2[1])),par2[2] + (TMath.Abs(2.*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_sigm>0:\n",
    "                            tot_sig_2_sigma = tot_sig_2_sigma+signal_under_peak_2_sigm\n",
    "\n",
    "                        #std = par2 [1]\n",
    "                        #estd = f2.GetParError(1)\n",
    "                        del h0, h1, h2, h3, f1, f2, fb, fs\n",
    "                        #latex = ROOT . TLatex ()\n",
    "                        #latex . SetNDC ()\n",
    "                        #latex . SetTextSize (0.02)\n",
    "                        #latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        #latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        #latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "                        #latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "\n",
    "                        #legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        #legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        #legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        #legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend . SetLineWidth (0)\n",
    "                        #legend.Draw()\n",
    "\n",
    "                        #canvas . cd ()\n",
    "                        #pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "                        #pad2 . Draw ()\n",
    "                        #pad2 . cd ()\n",
    "                        #pad2.Clear()\n",
    "\n",
    "\n",
    "                        #h3.SetLineColor(TColor.GetColor(5))\n",
    "                        #h3.SetYTitle(\"d-f/#Deltad\")\n",
    "                        #h3.Draw()\n",
    "                        #line = ROOT . TLine (mm,0 ,1.23 ,0)\n",
    "                        #line . SetLineColor ( ROOT . kRed )\n",
    "                        #line . SetLineWidth (2)\n",
    "                        #line . Draw (\" same \")\n",
    "\n",
    "\n",
    "                        #pad1 . SetBottomMargin (0)\n",
    "                        #pad2 . SetTopMargin (0)\n",
    "                        #pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "                        #h1 . GetXaxis (). SetLabelSize (0)\n",
    "                        #h1 . GetXaxis (). SetTitleSize (0)\n",
    "                        #h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "                        #h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "                        #h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "                        #h3 . SetTitle (\"\")\n",
    "                        #h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "                        #h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "                        #h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "                        #h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "                    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "                        #h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "                    #207,512 divisions\n",
    "                        #h3 . GetYaxis (). SetNdivisions (207)\n",
    "                        #h1 . GetYaxis (). SetRangeUser (0.5 ,3000)\n",
    "                        #h1 .GetYaxis().SetNdivisions(107)\n",
    "                        #h3 . GetXaxis (). SetNdivisions (207)\n",
    "                        gc.collect()\n",
    "                        #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "                    else:\n",
    "                        tot_sig_2_point_5_sigma=tot_sig_2_point_5_sigma+0\n",
    "                        tot_sig_3_sigma=tot_sig_3_sigma+0\n",
    "                        tot_sig_3_point_5_sigma=tot_sig_3_point_5_sigma+0\n",
    "                        #tot_sig_2_sigma = tot_sig_2_sigma+0\n",
    "            #lorentzian_pol2.append(tot_sig_2_sigma)\n",
    "                    lorentzian_pol2.append(tot_sig_2_point_5_sigma)\n",
    "                    lorentzian_pol2.append(tot_sig_3_sigma)\n",
    "                    lorentzian_pol2.append(tot_sig_3_point_5_sigma)\n",
    "                    pt_min.append(pt_bin_low+0.2)\n",
    "                    pt_min.append(pt_bin_low+0.2)\n",
    "                    pt_min.append(pt_bin_low+0.2)\n",
    "                    y_min.append(y_bin_low+0.2)\n",
    "                    y_min.append(y_bin_low+0.2)\n",
    "                    y_min.append(y_bin_low+0.2)\n",
    "                    \n",
    "            gc.collect()\n",
    "#canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")       \n",
    "print(y_bin_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total yield\n",
    "#lorentzian + second order pol\n",
    "lorentzian_pol2 = []\n",
    "#lorentzian_3rd_order_pol = []\n",
    "\n",
    "\n",
    "df = df4_urqmd\n",
    "\n",
    "\n",
    "mass_range_min = [df['mass'].describe()[1]-1.2*(df['mass'].describe()[2])]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,3,1):\n",
    "        #canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "        #canvas.Draw()\n",
    "        #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "\n",
    "\n",
    "        binning = [70,100,130]\n",
    "        for b in binning:\n",
    "            tot_sig_3_sigma = 0\n",
    "            tot_bac_3_sigma = 0\n",
    "            tot_sig_3_point_5_sigma = 0\n",
    "            tot_bac_3_point_5_sigma = 0\n",
    "            tot_sig_2_point_5_sigma = 0\n",
    "            tot_bac_2_point_5_sigma = 0\n",
    "            tot_sig_2_sigma = 0\n",
    "            y_bin_low=-0.2\n",
    "            y_bin_up =0\n",
    "            for i in range(0,15,1):\n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0\n",
    "                for i in range(0,15,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "                    #step 0\n",
    "                    if df_pt.shape[0]>500:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0);\n",
    "                        #fb =TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"RIEMQ\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        h1.Fit(f1,\"RNIQ\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        #canvas .Clear ()\n",
    "                        #pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        #pad1 . Draw ()\n",
    "                        #pad1 . cd ()\n",
    "                        #pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetNpx(100000);\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3],par1[4]);\n",
    "                        #f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIRQ\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs.SetNpx(100000);\n",
    "                        #fs.SetLineColor(ROOT.kGreen)\n",
    "                        #fb.SetLineStyle(4)\n",
    "                        #fb.SetLineColor(ROOT.kBlue)\n",
    "                        #fb.SetNpx(100000);\n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5]);\n",
    "                        #fb.SetParameters(par2[3],par2[4],par2[5],par2[6]);\n",
    "\n",
    "\n",
    "                        #h1.SetTitleOffset(-1)\n",
    "                        #h1.SetFillStyle(3003);\n",
    "                        #h1.SetLineWidth(2)\n",
    "                        #h1.SetStats (0)\n",
    "                        #h1.SetYTitle(\"Entries\")\n",
    "                        #h1.SetLineColor(ROOT.kBlack)\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        #h3.SetLineWidth(2)\n",
    "                        #h3.SetStats (0)\n",
    "                        #h3.GetXaxis().SetTitle(\"Mass (GeV/c^2)\")\n",
    "\n",
    "                        #h1.Draw(\"pe\")\n",
    "                        #fs.Draw(\"SAME\")\n",
    "                        #fb.Draw(\"SAME\")\n",
    "                        #f2.Draw(\"SAME\")\n",
    "\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "                        #h2.Sumw2()\n",
    "\n",
    "                        integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "                        integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "                        binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "                        #tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                        #sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "\n",
    "                        signal_under_peak = (fs.Integral(integral_min,integral_max)/binwidth);\n",
    "                        if signal_under_peak>0:\n",
    "                            tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak                \n",
    "                        #sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "                        #man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "                        if sigma_signal_under_peak!=0:\n",
    "                            print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "                        \n",
    "                        backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "                        #if backgnd_under_peak<0:\n",
    "                            #print('Negative background')\n",
    "\n",
    "                        Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "\n",
    "                        signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_3_point_5_sigma>0:\n",
    "                            tot_sig_3_point_5_sigma= tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma                \n",
    "                        #tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_point_5_sigma>0:\n",
    "                            tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "                        #tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_sigm = (fs.Integral(par2[2] - (TMath.Abs(2*par2[1])),par2[2] + (TMath.Abs(2.*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_sigm>0:\n",
    "                            tot_sig_2_sigma = tot_sig_2_sigma+signal_under_peak_2_sigm\n",
    "\n",
    "                        #std = par2 [1]\n",
    "                        #estd = f2.GetParError(1)\n",
    "                        del h0, h1, h2, h3, f1, f2, fb, fs\n",
    "                        #latex = ROOT . TLatex ()\n",
    "                        #latex . SetNDC ()\n",
    "                        #latex . SetTextSize (0.02)\n",
    "                        #latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        #latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        #latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "                        #latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "\n",
    "                        #legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        #legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        #legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        #legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend . SetLineWidth (0)\n",
    "                        #legend.Draw()\n",
    "\n",
    "                        #canvas . cd ()\n",
    "                        #pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "                        #pad2 . Draw ()\n",
    "                        #pad2 . cd ()\n",
    "                        #pad2.Clear()\n",
    "\n",
    "\n",
    "                        #h3.SetLineColor(TColor.GetColor(5))\n",
    "                        #h3.SetYTitle(\"d-f/#Deltad\")\n",
    "                        #h3.Draw()\n",
    "                        #line = ROOT . TLine (mm,0 ,1.23 ,0)\n",
    "                        #line . SetLineColor ( ROOT . kRed )\n",
    "                        #line . SetLineWidth (2)\n",
    "                        #line . Draw (\" same \")\n",
    "\n",
    "\n",
    "                        #pad1 . SetBottomMargin (0)\n",
    "                        #pad2 . SetTopMargin (0)\n",
    "                        #pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "                        #h1 . GetXaxis (). SetLabelSize (0)\n",
    "                        #h1 . GetXaxis (). SetTitleSize (0)\n",
    "                        #h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "                        #h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "                        #h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "                        #h3 . SetTitle (\"\")\n",
    "                        #h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "                        #h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "                        #h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "                        #h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "                    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "                        #h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "                    #207,512 divisions\n",
    "                        #h3 . GetYaxis (). SetNdivisions (207)\n",
    "                        #h1 . GetYaxis (). SetRangeUser (0.5 ,3000)\n",
    "                        #h1 .GetYaxis().SetNdivisions(107)\n",
    "                        #h3 . GetXaxis (). SetNdivisions (207)\n",
    "                        gc.collect()\n",
    "                        #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "                    else:\n",
    "                        tot_sig_2_point_5_sigma=tot_sig_2_point_5_sigma+0\n",
    "                        tot_sig_3_sigma=tot_sig_3_sigma+0\n",
    "                        #tot_sig_3_point_5_sigma=tot_sig_3_point_5_sigma+0\n",
    "                        #tot_sig_2_sigma = tot_sig_2_sigma+0\n",
    "            #lorentzian_pol2.append(tot_sig_2_sigma)\n",
    "            lorentzian_pol2.append(tot_sig_2_point_5_sigma)\n",
    "            lorentzian_pol2.append(tot_sig_3_sigma)\n",
    "            #lorentzian_pol2.append(tot_sig_3_point_5_sigma)\n",
    "            gc.collect()\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lorentzian_pol2)\n",
    "#lorentzian_pol2\n",
    "#lorentzian_3rd_order_pol\n",
    "#lorentzian_pol2.mean()\n",
    "#15*15*3*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = 15*15\n",
    "binning = 4\n",
    "size = configurations*3*3*binning\n",
    "#yields = {'yields':np.zeros(size)}\n",
    "#df_yields = pd.DataFrame(yields, columns = ['yields'])\n",
    "#df_yields['yields']= lorentzian_pol2\n",
    "#df_yields['pt_min']= pt_min\n",
    "#df_yields['y_min']= y_min\n",
    "new_yy = df_yields[(df_yields['pt_min']>0.4) & (df_yields['pt_min']<0.8)]\n",
    "new_yy[(new_yy['y_min']>1) & (new_yy['y_min']<1.4) & (new_yy['yields']>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27,1):\n",
    "    df_yields['yields'].iloc[i+2*27] = lorentzian_pol2[i]\n",
    "    df_yields['yields'].iloc[i+3*27] = lorentzian_3rd_order_pol[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_new = df_yields[(df_yields['yields']<119844+3000)&(df_yields['yields']>119844-3000)]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_yields[df_yields['yields']>0]['yields'].hist(bins=20)\n",
    "#df_yields[df_yields['yields']>100000]['yields'].mean()\n",
    "plt.hist(lorentzian_pol2, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = 3\n",
    "size = configurations*3*3*3*2\n",
    "yields = {'yields':np.zeros(size)}\n",
    "df_yields = pd.DataFrame(yields, columns = ['yields'])\n",
    "df_yields['sigma']=np.zeros(size)\n",
    "df_yields['fit_lim']=np.zeros(size)\n",
    "df_yields['bins'] = np.zeros(size)\n",
    "df_yields['numbering'] = np.arange(0,size,1)\n",
    "df_yields['function'] = np.zeros(size)\n",
    "df_yields['BDT_cut'] = np.zeros(size)\n",
    "for i in range(0,27,1):\n",
    "    df_yields['function'].iloc[i] = 'lorentzian_pol2'\n",
    "    df_yields['function'].iloc[i+27] = 'lorentzian_pol3'\n",
    "    df_yields['yields'].iloc[i] = lorentzian_pol2[i]\n",
    "    df_yields['yields'].iloc[i+27] = lorentzian_3rd_order_pol[i]\n",
    "    df_yields['function'].iloc[i+2*27] = 'lorentzian_pol2'\n",
    "    df_yields['function'].iloc[i+3*27] = 'lorentzian_pol3'\n",
    "        \n",
    "for i in range(0,size,3):\n",
    "    df_yields['sigma'].iloc[i]  ='2.5 sigma'\n",
    "    df_yields['sigma'].iloc[i+1]='3 sigma'\n",
    "    df_yields['sigma'].iloc[i+2]='3.5 sigma'\n",
    "#for i in range(0,162,1):\n",
    "#    df_yields['yields'].iloc[162+i] = third_order_pol[i]\n",
    "\n",
    "for k in range(0,3,1):\n",
    "    for l in range (0,12,1):\n",
    "        df_yields['bins'] .iloc[k+l*9] = 70\n",
    "        df_yields['bins'] .iloc[k+3+l*9] = 100\n",
    "        df_yields['bins'] .iloc[k+6+l*9] = 130\n",
    "\n",
    "        \n",
    "for i in range(0,4,1):\n",
    "    for j in range(0,9,1):\n",
    "            df_yields['fit_lim'].iloc[i*27+j]=mass_range_min[0]+fit_limit_low[0]\n",
    "            df_yields['fit_lim'].iloc[i*27+9+j]=mass_range_min[0]+fit_limit_low[1]\n",
    "            df_yields['fit_lim'].iloc[i*27+18+j]=mass_range_min[0]+fit_limit_low[2]\n",
    "\n",
    "for i in range(0,2*27,1):\n",
    "    df_yields['BDT_cut'].iloc[i] = 'test_best'\n",
    "    df_yields['BDT_cut'].iloc[i+2*27] = '0.9'\n",
    "    df_yields['BDT_cut'].iloc[i+4*27] = '0.8'\n",
    "#for aj in range(0,int(size/2),1):\n",
    "#    df_yields['function'].iloc[aj]='Lorentzian plus 2nd order chebyshev'\n",
    "#    df_yields['function'].iloc[aj+int(size/2)]='Lorentzian plus 3rd order chebyshev'\n",
    "\n",
    "    \n",
    "    \n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "def yield_plot(variable1, variable2):\n",
    "    fig, axs = plt.subplots(figsize=(12,10))\n",
    "    bins1 = 19\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(variable1)))\n",
    "    axs.plot(variable1, variable2,label='', alpha =0.3)\n",
    "        #axs.set_ylabel('Starting Mass')   \n",
    "\n",
    "    #axs.legend(loc=(1.04,0.7), fontsize=13)\n",
    "\n",
    "    \n",
    "    \n",
    "yield_plot(df_yields['numbering'],df_yields['yields'])\n",
    "\n",
    "\n",
    "#df_yields[(df_yields['yields']>(df_yields['yields'].mean()-10)) & (df_yields['yields']<(df_yields['yields'].mean()+10))]\n",
    "df_yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lorentzian 3rd order pol\n",
    "lorentzian_3rd_order_pol = []\n",
    "\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"EM\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",low_limit,upper_limit);\n",
    "                        #f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2], par[3]);\n",
    "                        h1.Fit(f1,\"RNI\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        canvas .Clear ()\n",
    "                        pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        pad1 . Draw ()\n",
    "                        pad1 . cd ()\n",
    "                        pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",low_limit,upper_limit)\n",
    "                        #f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetNpx(100000);\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3], par1[4]);\n",
    "                        f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",low_limit,upper_limit);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                        legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}+Ex^{3}\",\"l\");\n",
    "                        legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        legend.AddEntry(fb,\"B+Cx+Dx^{2}+Ex^{3}\",\"l\");\n",
    "                        legend . SetLineWidth (0)\n",
    "                        legend.Draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaussian + second order pol\n",
    "gaussian_2nd = []\n",
    "\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0);\n",
    "                        h0.Fit(fb,\"EM\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",low_limit,upper_limit);\n",
    "                        #f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        h1.Fit(f1,\"RNI\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        canvas .Clear ()\n",
    "                        pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        pad1 . Draw ()\n",
    "                        pad1 . cd ()\n",
    "                        pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",low_limit,upper_limit)\n",
    "                        #f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetNpx(100000);\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",low_limit,upper_limit);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                         latex . SetTextSize (0.02)\n",
    "                        latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                        legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd order chebyshev back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_order_pol = []\n",
    "\n",
    "                fb = TF1(\"fb\",\"[0]*x*x*x-[1]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0);\n",
    "                h0.Fit(fb,\"RNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]*x*x*x-[2]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "\n",
    "                f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ 0.25*[1]*[1]) +[3]*x*x*x-[4]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+Bx^{}-Cx\",\"l\");\n",
    "                legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"Bx^{3}-Cx\",\"l\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd order normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_pol = []\n",
    "\n",
    "                fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0,0);\n",
    "                h0.Fit(fb,\"EM\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1],par[2]);\n",
    "                h1.Fit(f1,\"EM\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "                f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.0001,1.115,par1[1],par1[2],par1[3]);\n",
    "\n",
    "                fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "\n",
    "                latex . SetTextSize (0.02)\n",
    "                latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h0, canvas, h1, h2, h3, pad1, pad2, f1,f2, fs, fb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                fb = TF1(\"fb\",\"[0]+[1]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0);\n",
    "                h0.Fit(fb,\"RNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1]);\n",
    "                h1.Fit(f1,\"RNI\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    " \n",
    "                f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.0001,1.115,par1[1],par1[2]);\n",
    "                f2.SetLineColor(ROOT.kRed)\n",
    "                h1.Fit(f2,\"MNIR\");\n",
    "                par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                fs.SetLineColor(ROOT.kGreen)\n",
    "                    latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx\",\"l\");\n",
    "                legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx\",\"l\");\n",
    "                legend . SetLineWidth (0)\n",
    "                legend.Draw()\n",
    "\n",
    "                canvas . cd ()\n",
    "                pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian with second order pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_2nd_order_pol = []\n",
    "mass_range_min = [1.0853341]\n",
    "fit_limit_low=[0,0.05*0.059818,0.1*0.059818,1.2707699000000001,1.2707699000000001+(0.05*0.059818),1.2707699000000001 +(0.1*0.059818)]\n",
    "                fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0,0);\n",
    "                h0.Fit(fb,\"LRNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x+[3]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1], par[2]);\n",
    "                h1.Fit(f1,\"LRNI\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "\n",
    "                f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x+[5]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.001,1.115,par1[1],par1[2], par1[3]);\n",
    "                f2.SetLineColor(ROOT.kRed)\n",
    "                h1.Fit(f2,\"LMNIR\");\n",
    "                par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                  bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                legend.AddEntry(fs,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                legend . SetLineWidth (0)\n",
    "                legend.Draw()\n",
    "\n",
    "                canvas . cd ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian with linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_linear = []\n",
    "\n",
    "                fb = TF1(\"fb\",\"[0]+[1]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0);\n",
    "                h0.Fit(fb,\"LRNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1]);\n",
    "                h1.Fit(f1,\"LRNI\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "                f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.001,1.115,par1[1],par1[2]);\n",
    "                f2.SetLineColor(ROOT.kRed)\n",
    "                h1.Fit(f2,\"LMNIR\");\n",
    "                par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                  latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}+B+Cx\",\"l\");\n",
    "                legend.AddEntry(fs,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx\",\"l\");\n",
    "                legend . SetLineWidth (0)\n",
    "                legend.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigma_signal_under_peak\n",
    "sigma_integral\n",
    "#sigma_backgnd_under_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(h[0].min(), h[0].max(), 4, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pT_vs_rapidity(df, var_xaxis , var_yaxis , range_var_xaxis, range_var_yaxis):\n",
    "    import matplotlib as mpl\n",
    "    fig, axs = plt.subplots(figsize=(8, 6),dpi = 300)\n",
    "    h=plt.hist2d(df[var_xaxis],df[var_yaxis],range=[range_var_xaxis,range_var_yaxis], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())\n",
    "    v1 = np.linspace(0, h[0].max(), 4, endpoint=True)\n",
    "    cbar = fig.colorbar(h[3], ticks = v1 )\n",
    "    #cbar.set_ticks([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    #cbar.set_ticklabels([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    \n",
    "    #v1 = np.linspace(Z.min(), Z.max(), 8, endpoint=True)\n",
    "    #cbar=plt.colorbar(ticks=v1)              # the mystery step ???????????\n",
    "    cbar.ax.set_yticklabels([ '0', '1784', '3568', '5353']) # add the labels\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.vlines(x=1.59,ymin=-1,ymax=2.4, color='r', linestyle='-')\n",
    "    #plt.hlines(y=bins4[1], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=bins4[2], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "\n",
    "    #plt.hlines(y=0.4, xmin=-0.1, xmax=df[var_xaxis].max(), colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.2, xmin=-0.1, xmax=1.5996, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.9, xmin=-0.1, xmax=3.5, colors='b', linestyles='solid', label='')\n",
    "    plt.xlabel('$y_{Lab}$', fontsize=20)\n",
    "    plt.ylabel('$p_{T}$ (GeV/$c$)', fontsize=18)\n",
    "    axs.text(0.02, 3, r'CBM Performance', fontsize=15)\n",
    "    axs.text(0.02, 2.8, r'DCM-QGSM-SMM, Au+Au @ 12 $A$GeV/$c$', fontsize=15, color ='r')\n",
    "    axs.text(1.2, 0.6, r'$y_{CM}$', fontsize=20, color ='r')\n",
    "    axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "    axs.grid(b=True, animated=True )\n",
    "    axs.set_xticks(np.arange(0,17)*0.2+0)\n",
    "    axs.set_xticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    axs.set_yticks(np.arange(0,16)*0.2+0)\n",
    "    axs.set_yticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    #plt.title(\"  y-$p_{T}$ plot for signal candidates (MC=1) with a cut = %.2f\"%0.95,  fontsize=18)\n",
    "    #plt.grid(which='both', ydata =yy)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity.png\")\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range1=[-0., 3.2]\n",
    "range2=[-0.01, 3.]\n",
    "\n",
    "h =pT_vs_rapidity(df4[df4['issignal']==1],'rapidity','pT', range1, range2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_rapidity_cut = df3_base[(df3_base['rapidity']<2) & (df3_base['rapidity']>0.8) &(df3_base['pT']>0.15)\n",
    "                           &(df3_base['pT']<0.9)]\n",
    "pT_vs_rapidity(pt_rapidity_cut,'rapidity','pT', range1, range2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pt_below_mid_rapidity_cut = df3_base[(df3_base['rapidity']<1.5996) & (df3_base['pT']>0.4)]\n",
    "pT_vs_rapidity(df3_base,'rapidity','pT', range1, range2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del  h1, h2 ,h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyf_tf1_params(x, p):\n",
    "    return p[0] * x[0] + p[1]\n",
    "\n",
    "npars = 2\n",
    "f = ROOT.TF1(\"tf1_params\", pyf_tf1_params, 0.0, 1.0, npars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenztian( x ,p):\n",
    "    return 0.5*p[0]*p[1] /( ((x[0]-p[2])**2) + ((0.5 * p[1])**2)) \n",
    "\n",
    "def gaus_fit( x ,p):\n",
    "    return p[0]*np.exp(-0.5*((x[0]-p[2])/p[1])**2)\n",
    "\n",
    "\n",
    "f2 = ROOT.TF1 (\" gaussfit\", \"[0]*exp(-0.5*((x-[2])/[1])^2)\"  ,1.1 ,1.13)\n",
    "\n",
    "#def lorenztian( x ,p):\n",
    "#    return p[0]*2*np.sqrt(2)*p[1]*p[2]*np.sqrt(p[2]*(p[2]**2 + p[1]**2)) /(np.pi*np.sqrt(p[2]+np.sqrt(p[2]*(p[2]**2 + p[1]**2)))) /( ((x[0]**2) - (p[2]**2))**2 +(p[1]*p[2])**2 )\n",
    "mm= 1.105\n",
    "bins =100\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "distribution = mid_pT_high_rapidity\n",
    "data = distribution['mass']\n",
    "#the minimum x (lower edge of the first bin)=mm        \n",
    "h1 = ROOT.TH1F(\"B_&_S\",\"\", bins,mm,1.13)\n",
    "for i in range(0,data.shape[0]):\n",
    "    h1.Fill(data.iloc[i])\n",
    "f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014)\",mm,1.13);\n",
    "#f1.SetParameters(1);\n",
    "h1.Fit(f1,\"RNI\");\n",
    "par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "#Step2\n",
    "canvas .Clear ()\n",
    "pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "pad1 . Draw ()\n",
    "pad1 . cd ()\n",
    "pad1. Clear()\n",
    "\n",
    "\n",
    "h1.SetTitleOffset(-1)\n",
    "h1.SetFillStyle(3003);\n",
    "h1.SetLineWidth(2)\n",
    "h1.SetStats (0)\n",
    "h1.SetYTitle(\"Entries\")\n",
    "h1.SetLineColor(ROOT.kBlack)\n",
    "h2 = ROOT.TH1F(\"h2\", \"\", bins, mm, 1.13);\n",
    "h3 = ROOT.TH1F(\"h2\", \"\", bins, mm, 1.13);\n",
    "h3.SetLineWidth(2)\n",
    "h3.SetStats (0)\n",
    "h3.GetXaxis().SetTitle(\"Mass (GeV/c^2)\")\n",
    "\n",
    "f2 = TF1(\"full\",lorenztian,mm,1.13,3);\n",
    "f2.SetNpx(100000);\n",
    "f2.SetParameters(par1[0],0.0001,1.115);\n",
    "f2.SetLineColor(ROOT.kRed)\n",
    "h1.Fit(f2,\"E\");\n",
    "par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "h1.Draw(\"pe\")\n",
    "\n",
    "f2.Draw(\"SAME\")\n",
    "\n",
    "bin1 = h1.FindBin(mm);\n",
    "bin2 = h1.FindBin(1.13);\n",
    "for i in range(bin1,bin2):\n",
    "    f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "    t_value = h1.GetBinContent(i)\n",
    "    h2.SetBinContent(i,f_value)\n",
    "    if (h1.GetBinError(i) > 0):\n",
    "        h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "h2.Sumw2()\n",
    "\n",
    "#To integrate over the gaussian peak we take the integral limits 3 sigmas (i.e. parameter 3) below the mean value\n",
    "#(i.e. par 1) of the gaussian as a minimum limit and 3 sigmas above the mean as a max limit of the integral*/\n",
    "integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "#To integrate area under the signal plus background curve we take 3 sigma and integrate\n",
    "binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "#To find the signal, we integrate just the gaussian peak with 3 sigma \n",
    "signal_under_peak = (fs.Integral(integral_min,integral_max)/binwidth);\n",
    "sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "if sigma_signal_under_peak!=0:\n",
    "    print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak\n",
    "#Background\n",
    "backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "sigma_backgnd_under_peak = fb.IntegralError(integral_min,integral_max);\n",
    "tot_bac_3_sigma = tot_bac_3_sigma+backgnd_under_peak\n",
    "#Significance = signal/(signal+background)^0.5\n",
    "#Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "\n",
    "#3.5 sigma\n",
    "signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "tot_sig_3_point_5_sigma = tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma\n",
    "tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "std =  f2.GetParameter(1)\n",
    "estd = f2.GetParError(1)\n",
    "\n",
    "latex = ROOT . TLatex ()\n",
    "latex . SetNDC ()\n",
    "latex . SetTextSize (0.02)\n",
    "#latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "#latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "#latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "latex . DrawLatex (0.2 ,0.75, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "latex . DrawLatex (0.2 ,0.70, \" m_{0} = %.4f #pm %.5f GeV\"%(par2 [2],f2.GetParError(2) ))\n",
    "#latex . DrawLatex (0.2 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "legend . SetLineWidth (0)\n",
    "legend.Draw()\n",
    "\n",
    "canvas . cd ()\n",
    "pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "pad2 . Draw ()\n",
    "pad2 . cd ()\n",
    "pad2.Clear()\n",
    "\n",
    "\n",
    "h3.SetLineColor(TColor.GetColor(5))\n",
    "h3.SetYTitle(\"d-f/#Deltad\")\n",
    "h3.Draw()\n",
    "line = ROOT . TLine (mm,0 ,1.125 ,0)\n",
    "line . SetLineColor ( ROOT . kRed )\n",
    "line . SetLineWidth (2)\n",
    "line . Draw (\" same \")\n",
    "\n",
    "\n",
    "pad1 . SetBottomMargin (0)\n",
    "pad2 . SetTopMargin (0)\n",
    "pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "h1 . GetXaxis (). SetLabelSize (0)\n",
    "h1 . GetXaxis (). SetTitleSize (0)\n",
    "h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "h3 . SetTitle (\"\")\n",
    "h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "h1 . GetXaxis (). SetRangeUser (1. ,1.126)\n",
    "h3 . GetXaxis (). SetRangeUser (1.11 ,1.122)\n",
    "#ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "#207,512 divisions\n",
    "h3 . GetYaxis (). SetNdivisions (207)\n",
    "h1 . GetYaxis (). SetRangeUser (0.5 ,1000)\n",
    "h1 .GetYaxis().SetNdivisions(107)\n",
    "h3 . GetXaxis (). SetNdivisions (207)\n",
    "\n",
    "canvas.Update()\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.png\")\n",
    "\n",
    "\n",
    "\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "#canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sgnal[sgnal['issignal']==1]\n",
    "lowest_rapidity = df[df['rapidity']<0.5]\n",
    "low_rapidity = df[(df['rapidity']>0.5)   & (df['rapidity']<1)]\n",
    "mid_rapidity = df[(df['rapidity']>1.5)   & (df['rapidity']<2)]\n",
    "high_rapidity = df[(df['rapidity']>2)    & (df['rapidity']<2.5)]\n",
    "higher_rapidity = df[(df['rapidity']>2.5)]\n",
    "\n",
    "    \n",
    "\n",
    "low_pT_lowest_rapidity = lowest_rapidity[lowest_rapidity['pT']<1]\n",
    "mid_pT_lowest_rapidity = lowest_rapidity[(lowest_rapidity['pT']>1) & (lowest_rapidity['pT']<2)]\n",
    "high_pT_lowest_rapidity =lowest_rapidity[(lowest_rapidity['pT']>2)]\n",
    "\n",
    "\n",
    "low_pT_low_rapidity = low_rapidity[low_rapidity['pT']<1]\n",
    "mid_pT_low_rapidity = low_rapidity[(low_rapidity['pT']>1) & (low_rapidity['pT']<2)]\n",
    "high_pT_low_rapidity= low_rapidity[(low_rapidity['pT']>2)]\n",
    "    \n",
    "\n",
    "low_pT_mid_rapidity = mid_rapidity[mid_rapidity['pT']<1]\n",
    "mid_pT_mid_rapidity = mid_rapidity[(mid_rapidity['pT']>1) & (mid_rapidity['pT']<2)]\n",
    "high_pT_mid_rapidity=mid_rapidity[(mid_rapidity['pT']>2)]\n",
    "    \n",
    "\n",
    "low_pT_high_rapidity = high_rapidity[high_rapidity['pT']<1]\n",
    "mid_pT_high_rapidity = high_rapidity[(high_rapidity['pT']>1) & (high_rapidity['pT']<2)]\n",
    "high_pT_high_rapidity=high_rapidity[(high_rapidity['pT']>2)]\n",
    "\n",
    "low_pT_higher_rapidity = higher_rapidity[higher_rapidity['pT']<1]\n",
    "mid_pT_higher_rapidity = higher_rapidity[(higher_rapidity['pT']>1) & (higher_rapidity['pT']<2)]\n",
    "high_pT_higher_rapidity=higher_rapidity[(higher_rapidity['pT']>2)]\n",
    "\n",
    "\n",
    "del  lowest_rapidity, mid_rapidity, high_rapidity, higher_rapidity, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [low_pT_lowest_rapidity, mid_pT_lowest_rapidity, high_pT_lowest_rapidity, low_pT_low_rapidity,\n",
    "         mid_pT_low_rapidity, high_pT_low_rapidity, low_pT_mid_rapidity, mid_pT_mid_rapidity,\n",
    "        high_pT_mid_rapidity, low_pT_high_rapidity, mid_pT_high_rapidity, high_pT_high_rapidity, \n",
    "         low_pT_higher_rapidity, mid_pT_higher_rapidity, high_pT_higher_rapidity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal[signal['issignal']==1]['rapidity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['mass'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('pt_y_yield_bdt_cut_0.95.root','recreate')\n",
    "t = TTree('t1','tree with df')\n",
    "\n",
    "\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'issignal/F')\n",
    "\n",
    "for i in range(len(df4_urqmd['mass'])):\n",
    "    rapidity[0] = df4_urqmd['rapidity'].iloc[i]\n",
    "    mass[0] = df4_urqmd['mass'].iloc[i]\n",
    "    pT[0] = df4_urqmd['pT'].iloc[i]\n",
    "    issignal[0] = df4_urqmd['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('pt_y_yield_bdt_cut_0.95.root','recreate')\n",
    "t = TTree('t1','tree with df')\n",
    "\n",
    "\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'pT/F')\n",
    "\n",
    "for i in range(len(df3_base['mass'])):\n",
    "    mass[0] = df3_base['mass'].iloc[i]\n",
    "    pT[0] = df4_urqmd['pT'].iloc[i]\n",
    "    issignal[0] = df4_urqmd['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[new['issignal']>2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[new['issignal']==1].shape\n",
    "new[new['issignal']==2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Root trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best way\n",
    "import uproot\n",
    "import awkward as ak\n",
    "file = uproot.recreate(\"signal_urqmd_uproot.root\")\n",
    "file[\"t1\"] = dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, x_train, y_train, df_scaled, dtest, dtrain, dtest1, dtest2, x_whole, x_test, y_test, x_whole_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "cut = 0.8\n",
    "df3 = df_clean[df_clean['xgb_preds']>=cut]\n",
    "df3 = df3[df3['issignal']>0]\n",
    "df3 = df3[['pT', 'rapidity', 'mass','issignal','xgb_preds']]\n",
    "df3.columns.values[[0,1,2,3,4]] = ['MCpT', 'MCrapidity','MCmass', 'MCissignal','MCxgb_preds']\n",
    "df3[\"MCissignal\"]=df3[\"MCissignal\"].astype(\"float\")\n",
    "df3[\"MCxgb_preds\"]=df3[\"MCxgb_preds\"].astype(\"float\")\n",
    "del df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3_base = df_clean_urqmd[df_clean_urqmd['xgb_preds']>=cut]\n",
    "df3_base3 = df3_base[['pT', 'rapidity', 'mass', 'issignal','xgb_preds']]\n",
    "df3_base3[\"issignal\"]=df3_base3[\"issignal\"].astype(\"double\")\n",
    "df3_base3[\"xgb_preds\"]=df3_base3[\"xgb_preds\"].astype(\"double\")\n",
    "del df_clean_urqmd, df3_base\n",
    "import uproot\n",
    "import awkward as ak\n",
    "file = uproot.recreate(\"new_c3_pt_y_y_yield_bdt_cut_0.8.root\")\n",
    "file[\"t1\"] = df3_base3\n",
    "file[\"t2\"] = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(7)\n",
    "df3 = uproot.open(\"new_c3_pt_y_y_yield_bdt_cut_0.8.root:t2\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df3 = df3[df3['MCxgb_preds']>0.8]\n",
    "urqmd = uproot.open(\"new_c3_pt_y_y_yield_bdt_cut_0.8.root:t1\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "urqmd = urqmd[(urqmd['xgb_preds']>0.8)&(urqmd['issignal']>0)]\n",
    "\n",
    "h2d_dcm = ROOT.TH2F(\"Mc\", \"Mc\", 10,1,2,10,0.4,1.4)\n",
    "for i in range(0,len( df3['MCrapidity'])):\n",
    "    h2d_dcm.Fill( df3['MCrapidity'].iloc[i],df3['MCpT'].iloc[i])\n",
    "h2d_dcm1=h2d_dcm.Clone()\n",
    "type(h2d_dcm)\n",
    "\n",
    "h2d_urqmd = ROOT.TH2F(\"urqmd\", \"urqmd\", 10,1,2,10,0.4,1.4)\n",
    "for i in range(0,len(urqmd['rapidity'])):\n",
    "    h2d_urqmd.Fill(urqmd['rapidity'].iloc[i],urqmd['pT'].iloc[i])\n",
    "type(h2d_urqmd)\n",
    "h2d_urqmd1=h2d_urqmd.Clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/new_c3/c3.root\")\n",
    "MC_DCM=file.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "file1 = ROOT.TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/urqmd/new_c3/c3.root\")\n",
    "MC_URQMD=file1.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "\n",
    "c1=ROOT.TCanvas(\"\")\n",
    "c1.Draw()\n",
    "ratio = h2d_dcm.Divide(MC_DCM)\n",
    "h2d_urqmd.Divide(h2d_dcm)\n",
    "h2d_urqmd.Divide(MC_URQMD)\n",
    "ROOT.gStyle.SetPaintTextFormat(\"4.2f\");\n",
    "h2d_urqmd.Draw(\"colz\")\n",
    "h2d_urqmd.Draw(\"TEXT SAME\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('dcm_prim_100k_cleaned.root','recreate')\n",
    "t = TTree('PlainTree','tree with df')\n",
    "\n",
    "chi2geo = array('f',[0])\n",
    "chi2primneg = array('f',[0])\n",
    "chi2primpos = array('f',[0])\n",
    "distance = array('f',[0])\n",
    "ldl = array('f',[0])\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('chi2geo', chi2geo,'chi2geo/F')\n",
    "t.Branch('chi2primneg', chi2primneg,'chi2primneg/F')\n",
    "t.Branch('chi2primpos', chi2primpos,'chi2primpos/F')\n",
    "t.Branch('distance', distance,'distance/F')\n",
    "t.Branch('ldl', ldl,'y/F')\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'issignal/F')\n",
    "\n",
    "for i in range(len(new['mass'])):\n",
    "    chi2geo[0] = new['chi2geo'].iloc[i]\n",
    "    chi2primneg[0] = new['chi2primneg'].iloc[i]\n",
    "    chi2primpos[0] = new['chi2primpos'].iloc[i]\n",
    "    distance[0] = new['distance'].iloc[i]\n",
    "    ldl[0] = new['ldl'].iloc[i]    \n",
    "    rapidity[0] = new['rapidity'].iloc[i]\n",
    "    mass[0] = new['mass'].iloc[i]\n",
    "    pT[0] = new['pT'].iloc[i]\n",
    "    issignal[0] = new['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r urqmd_100k_cleaned.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_filling(new, t):\n",
    "    for i in range(len(new['mass'])):\n",
    "        chi2geo[0] = new['chi2geo'].iloc[i]\n",
    "        chi2primneg[0] = new['chi2primneg'].iloc[i]\n",
    "        chi2primpos[0] = new['chi2primpos'].iloc[i]\n",
    "        distance[0] = new['distance'].iloc[i]\n",
    "        ldl[0] = new['ldl'].iloc[i]    \n",
    "        rapidity[0] = new['rapidity'].iloc[i]\n",
    "        mass[0] = new['mass'].iloc[i]\n",
    "        pT[0] = new['pT'].iloc[i]\n",
    "        issignal[0] = new['issignal'].iloc[i]\n",
    "        t.Fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "f = TFile('urqmd_100k_cleaned.root','recreate')\n",
    "t = TTree('PlainTree','tree with df')\n",
    "\n",
    "chi2geo = array('f',[0])\n",
    "chi2primneg = array('f',[0])\n",
    "chi2primpos = array('f',[0])\n",
    "distance = array('f',[0])\n",
    "ldl = array('f',[0])\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('chi2geo', chi2geo,'chi2geo/F')\n",
    "t.Branch('chi2primneg', chi2primneg,'chi2primneg/F')\n",
    "t.Branch('chi2primpos', chi2primpos,'chi2primpos/F')\n",
    "t.Branch('distance', distance,'distance/F')\n",
    "t.Branch('ldl', ldl,'y/F')\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'pT/F')\n",
    "\n",
    "for i in range(len(new['mass'])):\n",
    "    chi2geo[0] = new['chi2geo'].iloc[i]\n",
    "    chi2primneg[0] = new['chi2primneg'].iloc[i]\n",
    "    chi2primpos[0] = new['chi2primpos'].iloc[i]\n",
    "    distance[0] = new['distance'].iloc[i]\n",
    "    ldl[0] = new['ldl'].iloc[i]    \n",
    "    rapidity[0] = new['rapidity'].iloc[i]\n",
    "    mass[0] = new['mass'].iloc[i]\n",
    "    pT[0] = new['pT'].iloc[i]\n",
    "    issignal[0] = new['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_maker(new,'urqmd_100k_cleaned.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_importer import tree_importer\n",
    "dcm = tree_importer('dcm_1m_prim_signal.root','PlainTree',7)\n",
    "del dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[(df4['rapidity']>1.4)& (df4['pT']>1.4)]['pT'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot off\n",
    "#lorentzian + second order pol\n",
    "a = []\n",
    "pt_y_bin_for_yield_min=[]\n",
    "pt_y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_min=[]\n",
    "true_mc_in_recons =[]\n",
    "\n",
    "\n",
    "\n",
    "df = df4_urqmd\n",
    "\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,1,1):\n",
    "        canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "        canvas.Draw()\n",
    "\n",
    "        binning = [100]\n",
    "        for b in binning:\n",
    "\n",
    "            y_bin_low=1\n",
    "            y_bin_up =1.2\n",
    "            for i in range(0,1,1):\n",
    "                \n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0.\n",
    "                for i in range(0,1,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    #print(pt_bin_low)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']==1].shape[0]\n",
    "                    #step 0\n",
    "                    if df_pt.shape[0]>400:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"pol2\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fb =TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #fb.SetParameters(0,0,0);\n",
    "                        #fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"RIEM\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        h1.Fit(f1,\"RNI\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        canvas .Clear ()\n",
    "                        pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        pad1 . Draw ()\n",
    "                        pad1 . cd ()\n",
    "                        pad1. Clear()\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3], par1[4]);\n",
    "                        f2.SetNpx(100000);\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fs.SetNpx(100000);\n",
    "                        fs.SetLineColor(ROOT.kGreen)\n",
    "                        fb.SetLineStyle(4)\n",
    "                        fb.SetLineColor(ROOT.kBlue)\n",
    "                        fb.SetNpx(100000);\n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5], par2[6]);\n",
    "\n",
    "\n",
    "                        h1.SetTitleOffset(0)\n",
    "                        h1.SetFillStyle(3003);\n",
    "                        h1.SetLineWidth(2)\n",
    "                        h1.SetStats (0)\n",
    "                        h1.SetYTitle(\"Entries\")\n",
    "                        h1.SetLineColor(ROOT.kBlack)\n",
    "                        h1.GetYaxis().SetTitle(\"Counts\")\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h4 =  ROOT.TH1F(\"h2\", \"\", b, mm, 1.23)\n",
    "                        h5 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3.SetLineWidth(2)\n",
    "                        h3.SetStats (0)\n",
    "                        h3.GetXaxis().SetTitle(\"Mass (GeV/#it{c}^{2}]\")\n",
    "\n",
    "                        h1.Draw(\"pe\")\n",
    "                        h_mc.SetLineColor(ROOT.kMagenta)\n",
    "                        h_mc.SetLineWidth(2)\n",
    "                        #h_mc.Draw(\"SAMEpe\")\n",
    "                        fs.Draw(\"SAME\")\n",
    "                        fb.Draw(\"SAME\")\n",
    "                        f2.Draw(\"SAME\")\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            fs_values = fs.Eval(h3.GetBinCenter(i))\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            t_value_mc = h_mc.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            h4.SetBinContent(i,fs_values)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "                            if (h_mc.GetBinError(i) > 0):\n",
    "                                h5.SetBinContent(i,(t_value_mc-fs_values)/h_mc.GetBinError(i))\n",
    "\n",
    "\n",
    "                        h2.Sumw2()\n",
    "                        #h4.Sumw2()\n",
    "                        h5.SetLineColor(ROOT.kBlue)\n",
    "                        h5.SetLineWidth(2)\n",
    "\n",
    "                        integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "                        integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "\n",
    "                        binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "                        tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                        sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "                        #signal_under_peak = par2[0] * np.sqrt(2*3.1415*par2[1]*par2[1])/binwidth\n",
    "                        signal_under_peak = fs.Integral(integral_min,integral_max)/binwidth\n",
    "                        if signal_under_peak<0:\n",
    "                            signal_under_peak = 0\n",
    "                            print('Negative signal')                \n",
    "                        sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "                        man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "                        if sigma_signal_under_peak!=0:\n",
    "                            print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "                        tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak\n",
    "                    #Background\n",
    "                        backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "                        if backgnd_under_peak<0:\n",
    "                            print('Negative background')\n",
    "                        sigma_backgnd_under_peak = fb.IntegralError(integral_min,integral_max);\n",
    "                        tot_bac_3_sigma = tot_bac_3_sigma+backgnd_under_peak\n",
    "                    #Significance = signal/(signal+background)^0.5\n",
    "                        Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "                        #print(\"total - background = \",tot-backgnd_under_peak)\n",
    "                        #3.5 sigma\n",
    "                        signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        tot_sig_3_point_5_sigma = tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma\n",
    "                        tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "                        sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "                        man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "                        tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "                        sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "                        man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "\n",
    "                        std = par2 [1]\n",
    "                        estd = f2.GetParError(1)\n",
    "                        \n",
    "                        latex = ROOT . TLatex ()\n",
    "                        latex . SetNDC ()\n",
    "                        latex . SetTextSize (0.02)\n",
    "                        latex . DrawLatex (0.4 ,0.85, \"Significance in m_{0} #pm 2.5#Gamma  = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm 3#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        latex . DrawLatex (0.4 ,0.75, \"Significance in m_{0} #pm 3.5#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(par2 [1],f2.GetParError(1) ))\n",
    "                        latex . DrawLatex (0.4 ,0.65, \" m_{0} = %.4f #pm %.5f GeV\"%(par2 [2],f2.GetParError(2) ))\n",
    "                        latex . DrawLatex (0.4 ,0.6,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "                        latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "                        \n",
    "                        latex1 = ROOT . TLatex ()\n",
    "                        latex1 . SetNDC ()\n",
    "                        latex1 . SetTextSize (0.035)\n",
    "                        latex1. DrawLatex (0.4 ,0.25, \"CBM performance\")\n",
    "                        latex1. DrawLatex (0.4 ,0.15, \"URQMD, Au+Au @ 12#it{A} GeV/#it{c}\")\n",
    "                        latex1.Draw()\n",
    "\n",
    "                        \n",
    "                        legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                        legend . SetLineWidth (0)\n",
    "                        legend.Draw()\n",
    "\n",
    "                        canvas . cd ()\n",
    "                        pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "                        pad2 . Draw ()\n",
    "                        pad2 . cd ()\n",
    "                        pad2.Clear()\n",
    "\n",
    "\n",
    "                        h3.SetLineColor(TColor.GetColor(5))\n",
    "                        h3.SetYTitle(\"d-f/#Deltad\")\n",
    "                        #h5.Draw()\n",
    "                        h3.Draw(\"SAME\")\n",
    "                        line = ROOT . TLine (mm,0 ,1.2 ,0)\n",
    "                        line . SetLineColor ( ROOT . kRed )\n",
    "                        line . SetLineWidth (2)\n",
    "                        line . Draw (\" same \")\n",
    "\n",
    "\n",
    "                        pad1 . SetBottomMargin (0)\n",
    "                        pad2 . SetTopMargin (0)\n",
    "                        pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "                        h1 . GetXaxis (). SetLabelSize (0)\n",
    "                        #h1.SetTitle(\"\")\n",
    "                        h1 . GetXaxis (). SetTitleSize (0)\n",
    "                        h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "                        h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "                        h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "                        h3 . SetTitle (\"\")\n",
    "                        h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "                        h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "                        h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "                        h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "                    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "                        h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "                    #207,512 divisions\n",
    "                        h3 . GetYaxis (). SetNdivisions (207)\n",
    "                        #h_mc.GetYaxis (). SetRangeUser (0.5 ,700)\n",
    "                        h1 . GetYaxis (). SetRangeUser (0.5 ,400)\n",
    "                        h3 . GetXaxis (). SetRangeUser (1.08 ,1.2)\n",
    "                        h1 . GetXaxis (). SetRangeUser (1.08 ,1.2)\n",
    "                        h1 .GetYaxis().SetNdivisions(107)\n",
    "                        h3 . GetXaxis (). SetNdivisions (207)\n",
    "\n",
    "                        canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/c.png\")\n",
    "\n",
    "            #a.append(tot_sig_2_point_5_sigma)\n",
    "                        a.append(signal_under_peak)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low+0.2))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up+0.2))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "                    else:\n",
    "                        a.append(0)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low+0.2))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up+0.2))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "            #a.append(tot_sig_3_point_5_sigma)\n",
    "#canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency \n",
    "Efficieny correction on just one configuration i.e lorenztian + 2nd order pol, 100 mass binings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h1_set(h1):\n",
    "    h1 . SetTitleOffset(-1)\n",
    "    h1 . SetFillStyle(3003);\n",
    "    h1 . SetLineWidth(3)\n",
    "    h1 . SetStats (0)\n",
    "    h1 . SetYTitle(\"Counts\")\n",
    "    h1 . SetMarkerStyle(8)\n",
    "    #h1 . SetMarkerSize(2)\n",
    "    h1 . SetLineColor (ROOT . kBlack)\n",
    "    h1 . GetXaxis () . SetLabelSize (0)\n",
    "    h1 . GetXaxis () . SetTitleSize (0)\n",
    "    h1 . GetYaxis () . SetTitleSize (0.05)\n",
    "    h1 . GetYaxis() . CenterTitle();\n",
    "    h1 . GetYaxis () . SetLabelSize (0.05)\n",
    "    h1 . GetYaxis () . SetTitleOffset (1)\n",
    "    h1 . GetYaxis () . SetNdivisions(107)\n",
    "    #h1 . GetYaxis () . SetRangeUser(0.8,100)\n",
    "    #h1 . GetXaxis () . SetRangeUser(1.105,1.13)\n",
    "    #h1 . SetTitle (\"\")\n",
    "    return h1\n",
    "\n",
    "\n",
    "def h3_set(h3):   \n",
    "    h3 . SetLineWidth(3)\n",
    "    h3 . SetStats (0)\n",
    "    h3 . GetXaxis() . SetTitle(\"Mass [GeV/c^{2}]\")\n",
    "    h3 . SetTitle (\"\")\n",
    "    h3 . GetXaxis () . SetLabelSize (0.15)\n",
    "    h3 . GetXaxis() . CenterTitle();\n",
    "    h3 . GetXaxis () . SetTitleSize (0.15)\n",
    "    h3 . GetYaxis () . SetLabelSize (0.15)\n",
    "    h3 . GetYaxis () . SetTitleSize (0.15)\n",
    "    h3 . GetXaxis () . SetTitleOffset (1.1)\n",
    "    h3 . GetYaxis () . SetTitleOffset (0.4)\n",
    "    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "    #207,512 divisions\n",
    "    h3 . GetYaxis (). SetNdivisions (207)\n",
    "    h3.SetLineColor(TColor.GetColor(5))\n",
    "    h3.SetYTitle(\"d-f/#Deltad\")\n",
    "    h3 . GetXaxis () . SetRangeUser(1.105,1.13)\n",
    "    return h3\n",
    "\n",
    "\n",
    "def f_set(ftot, fs, fb):\n",
    "    ftot.SetNpx(100000);\n",
    "    ftot.SetLineColor(ROOT.kRed)\n",
    "    ftot.SetLineWidth(4)\n",
    "    \n",
    "    fs.SetLineStyle(4)\n",
    "    fs.SetNpx(100);\n",
    "    fs.SetLineColor(ROOT.kGreen)\n",
    "    fs.SetLineWidth(4)\n",
    "    fs.SetMarkerSize(2)\n",
    "    \n",
    "    fb.SetLineStyle(3)\n",
    "    fb.SetLineColor(ROOT.kBlue)\n",
    "    fb.SetNpx(100);\n",
    "    fb.SetLineWidth(4)\n",
    "    return ftot, fs, fb\n",
    "\n",
    "\n",
    "def draw_line():\n",
    "    line = ROOT . TLine (1.104,0 ,1.131 ,0)\n",
    "    line . SetLineColor ( ROOT . kRed )\n",
    "    line . SetLineWidth (2)\n",
    "    return line\n",
    "\n",
    "\n",
    "def draw_latex():\n",
    "    latex = ROOT . TLatex ()\n",
    "    latex . SetNDC ()\n",
    "    latex . SetTextSize (0.02)\n",
    "    #latex . DrawLatex (0.4 ,0.85, \"Significance in m_{0} #pm 2.5#Gamma  = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "    #latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm 3#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "    #latex . DrawLatex (0.4 ,0.75, \"Significance in m_{0} #pm 3.5#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "    #latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(par2 [1],f2.GetParError(1) ))\n",
    "    #latex . DrawLatex (0.4 ,0.85, \"Significance in m_{0} #pm 2.5#sigma  = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "    #latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm 3#sigma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "    #latex . DrawLatex (0.4 ,0.75, \"Significance in m_{0} #pm 3.5#sigma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "    latex . DrawLatex (0.17 ,0.80, \"#scale[1.5]{#sigma = %.4f #pm %.5f GeV}\"%(par2 [1],f2.GetParError(1) ))\n",
    "    latex . DrawLatex (0.17 ,0.75, \"#scale[1.5]{#mu = %.4f #pm %.5f GeV}\"%(par2 [2],f2.GetParError(2) ))\n",
    "    #latex . DrawLatex (0.4 ,0.6,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "    #latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "    latex . DrawLatex (0.55 ,0.85, \"#scale[1.7]{CBM performance}\")\n",
    "    latex . DrawLatex (0.65 ,0.8, \"#scale[1.7]{URQMD, Au+Au}\")\n",
    "    latex . DrawLatex (0.65 ,0.75, \"#scale[1.7]{@ 12#it{A} GeV/#it{c}}\")\n",
    "    \n",
    "    latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm %.1f#sigma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(sigma,signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,signal_under_peak/TMath.Sqrt(backgnd_under_peak+signal_under_peak) ))\n",
    "\n",
    "\n",
    "    return latex\n",
    "    \n",
    "    \n",
    "def draw_legend():\n",
    "    legend = ROOT.TLegend(0.85,0.22,0.58,0.7);\n",
    "    legend . AddEntry(h1,\"  #Lambda hyperons\",\"ep,X0\");\n",
    "    #legend . AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "    #legend . AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "    legend.AddEntry(f2,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}+B+Cx+Dx^{2}\",\"l\");\n",
    "    legend.AddEntry(fs,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}\",\"l\");\n",
    "    legend . AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "    legend . SetLineWidth (0)\n",
    "    legend . SetTextSize(0.038)\n",
    "    return legend\n",
    "\n",
    "def createCanvasPads():\n",
    "    c = ROOT . TCanvas (\" canvas \",\"\", 500,600)\n",
    "    \n",
    "    pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "    pad1 . SetBottomMargin (0)\n",
    "    pad1 . SetLeftMargin (0.15)\n",
    "    #pad1 . SetLogy()\n",
    "    pad1 . Draw ()\n",
    "    \n",
    "    pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "    pad2 . SetGrid()\n",
    "    pad2 . SetTopMargin (0)\n",
    "    pad2 . SetBottomMargin (0.5)\n",
    "    pad2 . SetLeftMargin (0.15)\n",
    "    pad2 . Draw ()\n",
    "    return c, pad1, pad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hist(h1, f2, fs, fb, h3):     \n",
    "\n",
    "    ROOT . gStyle . SetFillColor(-1);\n",
    "    ROOT . gStyle . SetFillStyle(4000);\n",
    "    ROOT . gStyle . SetLegendBorderSize(0);\n",
    "    ROOT . gStyle . SetFrameBorderSize(0);\n",
    "    ROOT . gStyle . SetFrameFillColor(-1);\n",
    "    c, pad1, pad2 = createCanvasPads ()\n",
    "    c . Draw ()\n",
    "    pad1 . cd ()\n",
    "    \n",
    "    h1 = h1_set (h1)\n",
    "    f2, fs, fb = f_set (f2, fs, fb)\n",
    "    \n",
    "    h1 . Draw(\"pe,X0\")\n",
    "    fb . Draw(\"SAME\")\n",
    "    f2 . Draw(\"SAME\")\n",
    "    fs . Draw(\"SAME\")\n",
    "    draw_latex()\n",
    "    legend = draw_legend ()\n",
    "    legend . Draw()\n",
    "    \n",
    "    c . cd ()\n",
    "    pad2 . cd ()\n",
    "\n",
    "    h3_set(h3) . Draw()\n",
    "    \n",
    "    \n",
    "    line = draw_line ()\n",
    "    line . Draw (\" same \")\n",
    "    c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "    \n",
    "    #c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_cal(h, f_tot, fs, fb, sigma):\n",
    "    tot_sig_sigma, tot_bac_sigma = 0, 0   \n",
    "    \n",
    "    binwidth = h.GetXaxis().GetBinWidth(1);\n",
    "    tot = f_tot.Integral(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])))/binwidth;\n",
    "    sigma_integral = f_tot.IntegralError(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])));\n",
    "    #params.integral = fit->GetParameter(0) * sqrt(2*3.1415) * fit->GetParameter(2) / h->GetBinWidth(1);\n",
    "    #signal_under_peak = par2[1] * np.sqrt(2*3.1415) *3 *par2[2]/ binwidth\n",
    "    signal_under_peak = fs.Integral(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])))/binwidth\n",
    "               \n",
    "    sigma_signal_under_peak = fs.IntegralError(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])));\n",
    "    man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "\n",
    "\n",
    "    tot_sig_sigma= tot_sig_sigma+signal_under_peak\n",
    "#Background\n",
    "    backgnd_under_peak = (fb.Integral(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])))/binwidth)\n",
    "\n",
    "    sigma_backgnd_under_peak = fb.IntegralError(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])));\n",
    "    tot_bac_sigma = tot_bac_sigma+backgnd_under_peak\n",
    "#Significance = signal/(signal+background)^0.5\n",
    "    Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "  \n",
    "    return tot, signal_under_peak, man_sigma_signal_under_peak,tot_sig_sigma, backgnd_under_peak, Significance, sigma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as ss\n",
    "ss.erf(3/np.sqrt(2))\n",
    "#(2/np.pi)* np.arctan(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "pt_y_bin_for_yield_min=[]\n",
    "pt_y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_min=[]\n",
    "true_mc_in_recons =[]\n",
    "red_chi = []\n",
    "\n",
    "df = df_clean[df_clean['xgb_preds']>0.8]\n",
    "\n",
    "mass_range_min = [1.08]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               1.23,\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "\n",
    "\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,1,1):\n",
    "\n",
    "        binning = [150]\n",
    "        for b in binning:\n",
    "\n",
    "            y_bin_low=-0.2\n",
    "            y_bin_up =0\n",
    "            for i in range(0,15,1):\n",
    "                tot_sig_3_point_5_sigma, tot_sig_3_sigma, tot_sig_2_point_5_sigma, tot_sig_2_sigma = 0, 0, 0, 0\n",
    "                tot_bac_3_sigma, tot_bac_3_point_5_sigma, tot_bac_2_point_5_sigma = 0, 0, 0\n",
    "                \n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0\n",
    "                \n",
    "                for i in range(0,15,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    #print(pt_bin_low)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "                    #print(y_bin_low, y_bin_up, \" pT \", pt_bin_low,pt_bin_up)\n",
    "                    if df_pt.shape[0]>200:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"pol2\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        h0.Fit(fb,\"RMN\");\n",
    "                        par = fb.GetParameters()\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        #f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        h1.Fit(f1,\"RN\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3], par1[4]);\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1],par1[2], par1[3])\n",
    "                        \n",
    "\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        #fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        \n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5], par2[6]);\n",
    "\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h3\", \"\", b, mm, 1.23);\n",
    "\n",
    "\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "                        h2.Sumw2()\n",
    "\n",
    "                        tot, signal_under_peak, man_sigma_signal_under_peak,tot_sig_sigma, backgnd_under_peak, Significance, sigma= signal_cal(h1, f2, fs, fb,3)\n",
    "\n",
    "                        \n",
    "                        draw_hist(h1, f2, fs, fb, h3)\n",
    "                        \n",
    "                        \n",
    "            #a.append(tot_sig_2_point_5_sigma)\n",
    "                        if f2.GetNDF()!= 0:\n",
    "                            red_chi.append(f2.GetChisquare() / f2.GetNDF() )\n",
    "                        else:\n",
    "                            red_chi.append(0)\n",
    "                        #a.append(signal_under_peak_2_sigma)\n",
    "                        #a.append(signal_under_peak_2_point_5_sigma)\n",
    "                        a.append(signal_under_peak)\n",
    "                        #a.append(signal_under_peak_3_point_5_sigma)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "                    else:\n",
    "                        a.append(0)\n",
    "                        red_chi.append(0)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_clean_mc = true_mc_in_recons\n",
    "#len(dcm_clean_mc)\n",
    "#sum(true_mc_in_recons)\n",
    "len(dcm_clean_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)\n",
    "#mc_counts\n",
    "#sum(a)\n",
    "#sum(true_mc_in_recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt[df_pt['issignal']==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "file =uproot.open(\"lambda_qa_dcm.root\")\n",
    "array1 = file[\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\"].to_numpy()\n",
    "#for i in range(0,14,1):\n",
    "array1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URQMD\n",
    "size = 15*15\n",
    "pt_y_yields = pd.DataFrame(data=np.arange(0,size,1),columns = ['numbering'])\n",
    "pt_y_yields['rapidity_min_MC'] = np.zeros(size)\n",
    "pt_y_yields['pT_min_MC'] = np.zeros(size)\n",
    "\n",
    "pt_y_yields['ratio_recons_sim']=np.zeros(size)\n",
    "pt_y_yields['ratio_recons_mc']=np.zeros(size)\n",
    "pt_y_yields['pT_min'] = np.zeros(size)\n",
    "pt_y_yields ['pt_y_yields_MC']=np.zeros(size)\n",
    "pt_y_yields['pt_y_yields_recons']=a\n",
    "pt_y_yields['true_mc_in_recons'] = true_mc_in_recons\n",
    "#pt_y_yields['total_mc_in_recons'] = dcm_clean_mc\n",
    "\n",
    "for i in range(0,15):\n",
    "    for j in range(0,15):\n",
    "        pt_y_yields['rapidity_min_MC'].iloc[i+j*15]=0+j*0.2\n",
    "    \n",
    "\n",
    "for i in range(0,15):    \n",
    "    pt_y_yields['pT_min_MC'].iloc[i]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+1*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+2*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+3*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+4*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+5*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+6*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+7*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+8*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+9*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+10*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+11*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+12*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+13*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+14*15]=i/5\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(0,15,1):\n",
    "    pt_y_yields ['pt_y_yields_MC'].iloc[i]=array1[0][0][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+1*15]=array1[0][1][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+2*15]=array1[0][2][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+3*15]=array1[0][3][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+4*15]=array1[0][4][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+5*15]=array1[0][5][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+6*15]=array1[0][6][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+7*15]=array1[0][7][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+8*15]=array1[0][8][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+9*15]=array1[0][9][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+10*15]=array1[0][10][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+11*15]=array1[0][11][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+12*15]=array1[0][12][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+13*15]=array1[0][13][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+14*15]=array1[0][14][i]\n",
    "\n",
    "for i in range(0,15*15,1):\n",
    "    pt_y_yields['ratio_recons_mc'].iloc[i]=a[i]/pt_y_yields['true_mc_in_recons'].iloc[i]\n",
    "    pt_y_yields['ratio_recons_sim'].iloc[i]=a[i]/pt_y_yields['pt_y_yields_MC'].iloc[i]\n",
    "    pt_y_yields['pT_min'].iloc[i] = pt_y_bin_for_yield_min[i]\n",
    "    #print(\"%.2f\"%pt_y_yields['rapidity_min_MC'].iloc[i],\"       \",pt_y_yields['pT_min_MC'].iloc[i],\"    \", pt_y_yields['ratio'].iloc[i] )\n",
    "#plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Reconstructed/Sim')\n",
    "plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_mc'], label='Rencostructed/MC')\n",
    "plt.legend()\n",
    "plt.ylim([0.9,1.1])\n",
    "plt.savefig(\"hists\")\n",
    "#pt_y_yields[(pt_y_yields['rapidity_min_MC']>1) & (pt_y_yields['rapidity_min_MC']<1.4) &(pt_y_yields['pT_min_MC']<1)&(pt_y_yields['pT_min_MC']>0)]\n",
    "pt_y_yields[(pt_y_yields['numbering']>20) & (pt_y_yields['numbering']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcm\n",
    "size = 15*15\n",
    "pt_y_yields = pd.DataFrame(data=np.arange(0,size,1),columns = ['numbering'])\n",
    "pt_y_yields['rapidity_min_MC'] = np.zeros(size)\n",
    "pt_y_yields['pT_min_MC'] = np.zeros(size)\n",
    "\n",
    "pt_y_yields['ratio_recons_sim']=np.zeros(size)\n",
    "pt_y_yields['ratio_recons_mc']=np.zeros(size)\n",
    "pt_y_yields['pT_min'] = np.zeros(size)\n",
    "pt_y_yields ['pt_y_yields_MC']=np.zeros(size)\n",
    "pt_y_yields['pt_y_yields_recons']=dcm_clean_mc\n",
    "pt_y_yields['true_mc_in_recons'] = true_mc_in_recons\n",
    "#pt_y_yields['total_mc_in_recons'] = dcm_clean_mc\n",
    "\n",
    "for i in range(0,15):\n",
    "    for j in range(0,15):\n",
    "        pt_y_yields['rapidity_min_MC'].iloc[i+j*15]=0+j*0.2\n",
    "    \n",
    "\n",
    "for i in range(0,15):    \n",
    "    pt_y_yields['pT_min_MC'].iloc[i]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+1*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+2*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+3*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+4*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+5*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+6*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+7*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+8*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+9*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+10*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+11*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+12*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+13*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+14*15]=i/5\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(0,15,1):\n",
    "    pt_y_yields ['pt_y_yields_MC'].iloc[i]=array1[0][0][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+1*15]=array1[0][1][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+2*15]=array1[0][2][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+3*15]=array1[0][3][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+4*15]=array1[0][4][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+5*15]=array1[0][5][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+6*15]=array1[0][6][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+7*15]=array1[0][7][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+8*15]=array1[0][8][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+9*15]=array1[0][9][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+10*15]=array1[0][10][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+11*15]=array1[0][11][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+12*15]=array1[0][12][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+13*15]=array1[0][13][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+14*15]=array1[0][14][i]\n",
    "\n",
    "for i in range(0,15*15,1):\n",
    "#    pt_y_yields['ratio_recons_mc'].iloc[i]=dcm_clean_mc[i]/pt_y_yields['true_mc_in_recons'].iloc[i]\n",
    "    pt_y_yields['ratio_recons_sim'].iloc[i]=dcm_clean_mc[i]/pt_y_yields['pt_y_yields_MC'].iloc[i]\n",
    "#    pt_y_yields['pT_min'].iloc[i] = pt_y_bin_for_yield_min[i]\n",
    "    #print(\"%.2f\"%pt_y_yields['rapidity_min_MC'].iloc[i],\"       \",pt_y_yields['pT_min_MC'].iloc[i],\"    \", pt_y_yields['ratio'].iloc[i] )\n",
    "#plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Reconstructed/Sim')\n",
    "plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_mc'], label='Rencostructed/MC')\n",
    "plt.legend()\n",
    "plt.ylim([0.9,1.1])\n",
    "plt.savefig(\"hists\")\n",
    "#pt_y_yields[(pt_y_yields['rapidity_min_MC']>1) & (pt_y_yields['rapidity_min_MC']<1.4) &(pt_y_yields['pT_min_MC']<1)&(pt_y_yields['pT_min_MC']>0)]\n",
    "pt_y_yields[(pt_y_yields['numbering']>20) & (pt_y_yields['numbering']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3);\n",
    "h6 = ROOT.TH2F(\"Mc in reconstructed\", \"Mc in reconstructed\", 15,0,3,15,0,3);\n",
    "h7 = ROOT.TH2F(\"DCM Efficiency\", \"DCM Efficiency\", 15,0,3,15,0,3);\n",
    "#h8 = ROOT.TH2F(\"total mc in reconstructed\", \"total mc in reconstructed\", 15,0,3,15,0,3);\n",
    "\n",
    "\n",
    "h4.SetStats(0)\n",
    "h5.SetStats(0)\n",
    "h6.SetStats(0)\n",
    "\n",
    "c = ROOT . TCanvas (\" canvas \",\"\", 950,800)\n",
    "c.Draw()\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h6.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h7.SetBinContent(y_bin, pT_bin, a[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, pt_y_yields['total_mc_in_recons'].iloc[i]);\n",
    "\n",
    "c.SetGrid()\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h8)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h4.Divide(h5)\n",
    "h4.Draw('colz')\n",
    "\n",
    "h4.GetZaxis().SetRangeUser (0 ,0.4)\n",
    "h4.SetTitleOffset(-1)\n",
    "latex = ROOT . TLatex ()\n",
    "latex . SetNDC ()\n",
    "latex . SetTextSize (0.039)\n",
    "#latex . DrawLatex (0.4 ,0.7, \"#Lambda_{Reconstructed} / #Lambda_{MC} =  %.f / %.f = %.3f\"%(sum(a),df4[df4['issignal']>0].shape[0], sum(a) / (df4[df4['issignal']>0].shape[0])))\n",
    "latex . DrawLatex (0.12 ,0.68, \"ML algorithm Efficiency = #Lambda_{Reconstructed} / #Lambda_{Reconstructable} \" )\n",
    "\n",
    "latex.Draw()\n",
    "\n",
    "latex1 = ROOT . TLatex ()\n",
    "latex1 . SetNDC ()\n",
    "latex1 . SetTextSize (0.035)\n",
    "latex1. DrawLatex (0.12 ,0.84, \"CBM performance\")\n",
    "latex1. DrawLatex (0.12 ,0.76, \"DCM-QGSM-SMM, Au+Au @ 12#it{A} GeV/#it{c}\")\n",
    "#latex1 . DrawLatex (0.45 ,.61, \"= %.f / %.f = %.3f\"%(sum(a),sum(pt_y_yields ['total_mc_in_recons']), sum(a) / (sum(pt_y_yields ['total_mc_in_recons']))))\n",
    "latex1.Draw()\n",
    "\n",
    "\n",
    "h4 . SetTitle (\"\")\n",
    "h4 .GetXaxis().SetTitle(\"#it{y}_{Lab}\")\n",
    "h4. GetXaxis().SetTitleSize(0.06)\n",
    "h4 .GetXaxis().SetTitleOffset(0.7)\n",
    "h4 .GetXaxis().SetLabelSize(0.05)\n",
    "h4 .GetYaxis().SetTitle(\"p_{T} (GeV/#it{c})\")\n",
    "h4. GetYaxis().SetTitleSize(0.06)\n",
    "h4 .GetYaxis().SetTitleOffset(0.7)\n",
    "h4 .GetYaxis().SetLabelSize(0.05)\n",
    "h4 .GetZaxis().SetLabelSize(0.05)\n",
    "\n",
    "c.SetRightMargin(0.13);\n",
    "c. Update()\n",
    "c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "f = TFile('xgb_vs_kfpf.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h1 = ROOT.TH1D(\"xgb_recons_urqmd\",\"xgb_recons_urqmd\",200,1.08,1.2)\n",
    "h2 = ROOT.TH1D(\"KFPF_urqmd\", \"kfpf_urqmd\", 200,1.08,1.2);\n",
    "h3 = ROOT.TH1D(\"ratio\", \"ratio\", 200,1.08,1.2);\n",
    "h1.SetStats(0)\n",
    "h2.SetStats(0)\n",
    "h3.SetStats(0)\n",
    "\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "\n",
    "for i in range(0,df3_base.shape[0]):\n",
    "    h1.Fill(df3_base['mass'].iloc[i])\n",
    "    h3.Fill(df3_base['mass'].iloc[i])\n",
    "for i in range(0,new_check_set.shape[0]):\n",
    "    h2.Fill(new_check_set['mass'].iloc[i])\n",
    "\n",
    "canvas.SetGrid()\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h1.Divide(h2)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "h3.Divide(h2)\n",
    "h1.Draw()\n",
    "h2.Draw(\"same\")\n",
    "\n",
    "\n",
    "\n",
    "h11.SetTitleOffset(-1)\n",
    "\n",
    "\n",
    "h1 . SetTitle (\"\")\n",
    "h1 .GetXaxis().SetTitle(\"mass\")\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h8 = ROOT.TH2F(\"recons_urqmd\", \"recons_urqmd\", 15,0,3,15,0,3);\n",
    "h9 = ROOT.TH2F(\"Mc_urqmd\", \"Mc_urqmd\", 15,0,3,15,0,3);\n",
    "h10 = ROOT.TH2F(\"Mc in reconstructed_urqmd\", \"Mc in reconstructed_urqmd\", 15,0,3,15,0,3);\n",
    "h11 = ROOT.TH2F(\"urqmd_Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h11.SetStats(0)\n",
    "h9.SetStats(0)\n",
    "h10.SetStats(0)\n",
    "\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "bin1 = h8.FindBin(0);\n",
    "bin2 = h8.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h8.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h9.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h10.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h11.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "\n",
    "canvas.SetGrid()\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "ratio_recons_to_recons_mc=h11.Divide(h9)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "#ratio_recons_to_mc=h4.Divide(h5)\n",
    "h11.Draw('colz')\n",
    "\n",
    "h11.GetZaxis().SetLabelSize (0.02)\n",
    "\n",
    "h11.SetTitleOffset(-1)\n",
    "latex = ROOT . TLatex ()\n",
    "latex . SetNDC ()\n",
    "latex . SetTextSize (0.03)\n",
    "#latex . DrawLatex (0.4 ,0.7, \"#Lambda_{Reconstructed} / #Lambda_{MC} =  %.f / %.f = %.3f\"%(sum(a),df4[df4['issignal']>0].shape[0], sum(a) / (df4[df4['issignal']>0].shape[0])))\n",
    "latex . DrawLatex (0.3 ,0.7, \"#Lambda_{Reconstructed} / #Lambda_{Simulated} =  %.f / %.f = %.3f\"%(sum(a),sum(pt_y_yields ['pt_y_yields_MC']), sum(a) / (sum(pt_y_yields ['pt_y_yields_MC']))))\n",
    "latex . DrawLatex (0.3 ,0.6, \"URQMD\")\n",
    "\n",
    "latex.Draw()\n",
    "\n",
    "\n",
    "h11 . SetTitle (\"\")\n",
    "h11 .GetXaxis().SetTitle(\"y_{Lab}\")\n",
    "h11 .GetXaxis().SetTitleOffset(0)\n",
    "h11 .GetYaxis().SetTitle(\"p_{T} GeV/c\")\n",
    "h11 .GetXaxis().SetTitleOffset(0)\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('new_urqmd_efficiency_pt_y_yield_bdt_cut_0.9.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h8 = ROOT.TH2F(\"recons_urqmd\", \"recons_urqmd\", 15,0,3,15,0,3);\n",
    "h9 = ROOT.TH2F(\"Mc_urqmd\", \"Mc_urqmd\", 15,0,3,15,0,3);\n",
    "h10 = ROOT.TH2F(\"Mc in reconstructed_urqmd\", \"Mc in reconstructed_urqmd\", 15,0,3,15,0,3);\n",
    "h11 = ROOT.TH2F(\"urqmd_Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8.SetStats(0)\n",
    "h9.SetStats(0)\n",
    "h10.SetStats(0)\n",
    "\n",
    "\n",
    "bin1 = h8.FindBin(0);\n",
    "bin2 = h8.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h8.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h9.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h10.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h11.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h8.Divide(h9)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h11.Divide(h9)\n",
    "#h8.Draw('colz')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h8 . SetTitle (\"\")\n",
    "h8 .GetXaxis().SetTitle(\"y_{Lab}\")\n",
    "h8 .GetXaxis().SetTitleOffset(0)\n",
    "h8 .GetYaxis().SetTitle(\"p_{T} GeV/c\")\n",
    "h8 .GetXaxis().SetTitleOffset(0)\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('new_dcm_100_efficiency_pt_y_yield_bdt_cut_0.8.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3);\n",
    "h6 = ROOT.TH2F(\"Mc in reconstructed\", \"Mc in reconstructed\", 15,0,3,15,0,3);\n",
    "h7 = ROOT.TH2F(\"Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8 = ROOT.TH2F(\"reconstructable_mc\", \"reconstructable_mc\", 15,0,3,15,0,3);\n",
    "\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h6.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h7.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, dcm_clean_mc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h5)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h7.Divide(h5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(df_clean['rapidity'],df_clean['pT'] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((14.03+18.58)  /2)- (np.sqrt(0.59*0.59+3.22*3.22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(3267.93*3267.93+1622.55*1622.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2994-3267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inFile = ROOT . TFile . Open ( \"lambda_qa_urqmd.root\" ,\" READ \")\n",
    "inFile.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_2d = inFile.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inFile.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h4.SetStats(0)\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (df4['rapidity'].iloc[i])\n",
    "    pT=(df4['pT'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, df4['issignal'].iloc[i]);\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h4.Draw('colz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "fig, axs =plt.subplots(figsize=(12,10))\n",
    "h =axs.hist2d(df4[df4['issignal']==1]['rapidity'], df4[df4['issignal']==1]['pT'], bins=(bins1, bins1),norm=mpl.colors.LogNorm())\n",
    "cbar=fig.colorbar(h[3], ax=axs)\n",
    "plt.show()\n",
    "fig.savefig('hists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins1 = np.linspace(0,3,16)\n",
    "bins1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic as b_s\n",
    "bin_means, bin_edges, binnumber = b_s(df[variable_xaxis],df[variable_yaxis], statistic='mean', bins=non_uniform_binning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "x_whole1 = x_whole_1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mkl\n",
    "mkl.set_num_threads(6)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "snn_classifier = MLPClassifier(hidden_layer_sizes = [300]*3)\n",
    "snn_classifier.fit(x_train, y_train)\n",
    "#snn_predictions = snn_classifier.predict(training_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_train = pd.DataFrame(snn_classifier.predict_proba(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(bst_train[0], bins=100)\n",
    "plt.hist(bst_train[1], bins=100)\n",
    "plt.show()\n",
    "\n",
    "#bst_train.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_test['mlp_preds'] = snn_classifier.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aba = snn_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(aba,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = [100]*5)\n",
    "\n",
    "grid = GridSearchCV(mlp, {}, n_jobs=6, cv=ShuffleSplit(n_splits=1),\n",
    "                    verbose=2)\n",
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
