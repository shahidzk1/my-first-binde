{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Cuts Optimization using Extra Gradient Boosting\n",
    "<br></p><br>\n",
    "\n",
    "Over the last years, **Machine Learning** tools have been successfully applied to problems in high-energy physics. For example, for the classification of physics objects. Supervised machine learning algorithms allow for significant improvements in classification problems by taking into account observable correlations and by learning the optimal selection from examples, e.g. from Monte Carlo simulations.\n",
    "\n",
    "\n",
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook.\n",
    "\n",
    "**Matplotlib** comes handy in plotting data while the machine learning is performed by **XGBOOST**. We will import data splitter from **Scikit-learn** as *train_test_split*. **Evaluation metrics** such as *confusion matrix*, *Receiver operating characteristic (ROC)*, and *Area Under the Receiver Operating Characteristic Curve (ROC AUC)*  will be used to asses our models.\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import weakref \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "#from root_pandas import read_root\n",
    "\n",
    "\n",
    "from data_cleaning import clean_df\n",
    "from KFPF_lambda_cuts import KFPF_lambda_cuts\n",
    "from plot_tools import AMS, preds_prob, plot_confusion_matrix, plt_sig_back\n",
    "import tree_importer \n",
    "import uproot\n",
    "\n",
    "\n",
    "#To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") \n",
    "        \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(7)\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The quality_cuts_plus_other_cuts function applies quality selection criteria with other selection criteria to reduce data size\n",
    "\"\"\"\n",
    "def quality_cuts_plus_other_cuts():\n",
    "    #The following quality selection criteria is applied\n",
    "    mass_cut = \"(LambdaCandidates_mass > 1.077) &\"\n",
    "    \n",
    "    coordinate_cut = \"(LambdaCandidates_x>-50) & (LambdaCandidates_x<50) & (LambdaCandidates_y>-50) & (LambdaCandidates_y<50) & (LambdaCandidates_z>-1) & (LambdaCandidates_z<80) &\"\n",
    "    \n",
    "    chi_2_positive_cut =\"(LambdaCandidates_chi2geo>0) & (LambdaCandidates_chi2topo>0) & (LambdaCandidates_chi2primpos>0) & (LambdaCandidates_chi2primneg > 0) &\"\n",
    "    \n",
    "    distance_cut = \"(LambdaCandidates_ldl>0) & (LambdaCandidates_l<80) & (LambdaCandidates_distance>0) & (LambdaCandidates_distance<100) &\"\n",
    "    \n",
    "    pz_cut = \"(LambdaCandidates_pz>0) & \"\n",
    "    #Other cuts\n",
    "    pseudo_rapidity_cut_based_on_acceptance = \"(LambdaCandidates_eta>1) & (LambdaCandidates_eta<6.5) &\"\n",
    "    \n",
    "    angular_cut = \"(LambdaCandidates_cosineneg>0.1) & (LambdaCandidates_cosinepos>0.1) &\"\n",
    "    \n",
    "    data_reducing_cut = \"(LambdaCandidates_mass < 1.3) &  (LambdaCandidates_p<20)  &   (LambdaCandidates_chi2geo<1000) &  (LambdaCandidates_chi2primpos<1e6) & (LambdaCandidates_chi2primneg < 3e7) &  (LambdaCandidates_ldl<5000) & (LambdaCandidates_chi2topo < 100000)\"\n",
    "    \n",
    "    cuts= mass_cut+coordinate_cut+chi_2_positive_cut+distance_cut+pz_cut+pseudo_rapidity_cut_based_on_acceptance+angular_cut+data_reducing_cut\n",
    "    return cuts\n",
    "\n",
    "\"\"\"\n",
    "This tree_importer_with_cuts imports tree and also applies quality selection criteria on the data along with some further data reducing cuts.\n",
    "\"\"\"\n",
    "\n",
    "def tree_importer_with_cuts(path,treename, n):\n",
    "    \n",
    "    #This part changes the labels of the root tree's branches \n",
    "    labels=[\"LambdaCandidates_chi2geo\", \"LambdaCandidates_chi2primneg\", \"LambdaCandidates_chi2primpos\",\n",
    "         \"LambdaCandidates_distance\", \"LambdaCandidates_ldl\",\"LambdaCandidates_mass\", \"LambdaCandidates_pT\",\n",
    "            \"LambdaCandidates_rapidity\", \"LambdaCandidates_is_signal\"]\n",
    "    \n",
    "    new_labels=['chi2geo', 'chi2primneg','chi2primpos', 'distance', 'ldl','mass', 'pT', 'rapidity','issignal']\n",
    "    \n",
    "    cuts = quality_cuts_plus_other_cuts()\n",
    "    \n",
    "    #The number of parallel processors\n",
    "    executor = ThreadPoolExecutor(n)\n",
    "    \n",
    "    #To open the \n",
    "    file = uproot.open(path+':'+treename, library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(labels,cuts, library='np',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "    df= pd.DataFrame(data=file)\n",
    "    df.columns = new_labels\n",
    "    #df['issignal']=((df['issignal']>0)*1)\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_importer(path,treename, n):    \n",
    "    #The number of parallel processors\n",
    "    executor = ThreadPoolExecutor(n)\n",
    "    \n",
    "    #To open the root file and convert it to a pandas dataframe\n",
    "    df = uproot.open(path+':'+treename, library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The quality_cuts_plus_other_cuts function applies quality selection criteria with other selection criteria to reduce data size\n",
    "\"\"\"\n",
    "def quality_cuts_plus_other_cuts():\n",
    "    #The following quality selection criteria is applied\n",
    "    mass_cut = \"(LambdaCandidates_mass > 1.07) &\"\n",
    "    \n",
    "    coordinate_cut = \"(LambdaCandidates_x>-50) & (LambdaCandidates_x<50) & (LambdaCandidates_y>-50) & (LambdaCandidates_y<50) & (LambdaCandidates_z>-1) & (LambdaCandidates_z<80) &\"\n",
    "    \n",
    "    chi_2_positive_cut =\"(LambdaCandidates_chi2geo>0) & (LambdaCandidates_chi2topo>0) & (LambdaCandidates_chi2primpos>0) & (LambdaCandidates_chi2primneg > 0) &\"\n",
    "    \n",
    "    distance_cut = \"(LambdaCandidates_ldl>0) & (LambdaCandidates_l<80) & (LambdaCandidates_distance>0) & (LambdaCandidates_distance<100) &\"\n",
    "    \n",
    "    pz_cut = \"(LambdaCandidates_pz>0) & \"\n",
    "    #Other cuts\n",
    "    pseudo_rapidity_cut_based_on_acceptance = \"(LambdaCandidates_eta>1) & (LambdaCandidates_eta<6.5) &\"\n",
    "    \n",
    "    #angular_cut = \"(LambdaCandidates_cosineneg>0.1) & (LambdaCandidates_cosinepos>0.1) &\"\n",
    "    \n",
    "    data_reducing_cut = \"(LambdaCandidates_mass < 1.2) &  (LambdaCandidates_p<20)  &   (LambdaCandidates_chi2geo<1000) &  (LambdaCandidates_chi2primpos<1e6) & (LambdaCandidates_chi2primneg < 3e7) &  (LambdaCandidates_ldl<5000) & (LambdaCandidates_chi2topo < 100000)\"\n",
    "    \n",
    "    cuts= mass_cut+coordinate_cut+chi_2_positive_cut+distance_cut+pz_cut+pseudo_rapidity_cut_based_on_acceptance+data_reducing_cut\n",
    "    return cuts\n",
    "\n",
    "\"\"\"\n",
    "This tree_importer_with_cuts imports tree and also applies quality selection criteria on the data along with some further data reducing cuts.\n",
    "\"\"\"\n",
    "\n",
    "def tree_importer_with_cuts(path,treename, n):\n",
    "    \n",
    "    #This part changes the labels of the root tree's branches \n",
    "    labels=['LambdaCandidates_chi2geo', 'LambdaCandidates_chi2primneg',\n",
    " 'LambdaCandidates_chi2primpos', 'LambdaCandidates_chi2topo', 'LambdaCandidates_cosineneg', 'LambdaCandidates_cosinepos',\n",
    " 'LambdaCandidates_cosinetopo', 'LambdaCandidates_distance',\n",
    " 'LambdaCandidates_eta', 'LambdaCandidates_l', 'LambdaCandidates_ldl', 'LambdaCandidates_mass', 'LambdaCandidates_p',\n",
    " 'LambdaCandidates_pT', 'LambdaCandidates_phi',  \n",
    " 'LambdaCandidates_rapidity',   'LambdaCandidates_is_signal']\n",
    "    \n",
    "    new_labels=['chi2geo', 'chi2primneg','chi2primpos', 'distance', 'ldl','mass', 'pT', 'rapidity','issignal']\n",
    "    \n",
    "    cuts = quality_cuts_plus_other_cuts()\n",
    "    \n",
    "    #The number of parallel processors\n",
    "    executor = ThreadPoolExecutor(n)\n",
    "    \n",
    "    #To open the \n",
    "    file = uproot.open(path+':'+treename, library='pd', decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(labels,cuts,library='np',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "    df= pd.DataFrame(data=file)\n",
    "    df[\"LambdaCandidates_is_signal\"]=df[\"LambdaCandidates_is_signal\"].astype(\"int8\")\n",
    "    \n",
    "    #df.columns = new_labels\n",
    "    #df['issignal']=((df['issignal']>0)*1)\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = uproot.open('/home/shahid/Mount/gsi/u/dcm_5m_signal.root:plain_tree',library='pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/c6_pt_y_0_1.59_yield_bdt_cut_0.8.root:t2',library='pd').arrays(library='pd')\n",
    "df2 = uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/c6_pt_y_1.59_y_yield_bdt_cut_0.8.root:t2',library='pd').arrays(library='pd')\n",
    "dfs = [df1,df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urqmd = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dcm = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[dff['Candidates_mass']<0.7]['Candidates_mass'].hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = uproot.open('/home/shahid/Mount/gsi/u/analysis_plain_ttree.root:plain_tree').arrays(library='pd')\n",
    "#print(df[df['Candidates_plain_generation']>0].shape)\n",
    "#print(df[df['Candidates_plain_generation']==0].shape)\n",
    "df['Candidates_plain_mass'].hist(bins=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file, for example a URQMD 12 AGeV, is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **tree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "In this example, we download three Analysis Trees. The first one contains mostly background candidates for lambda i.e. protons and pions which do not come from a lambda. The second file contains mostly signal candidates of lamba i.e. it contains protons and pions which come from a lambda decay. The third one contains 10k events generated using URQMD generator with 12 AGeV energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['index', 'LambdaCandidates_chi2geo', 'LambdaCandidates_chi2primneg',\n",
    "       'LambdaCandidates_chi2primpos', 'LambdaCandidates_chi2topo',\n",
    "       'LambdaCandidates_cosineneg', 'LambdaCandidates_cosinepos',\n",
    "       'LambdaCandidates_cosinetopo', 'LambdaCandidates_distance',\n",
    "       'LambdaCandidates_eta', 'LambdaCandidates_l', 'LambdaCandidates_ldl',\n",
    "       'LambdaCandidates_mass', 'LambdaCandidates_p', 'LambdaCandidates_pT',\n",
    "       'LambdaCandidates_phi', 'LambdaCandidates_rapidity','LambdaCandidates_issignal']\n",
    "new_labels = []\n",
    "\n",
    "for i in a:\n",
    "    if 'LambdaCandidates_' in i:\n",
    "        new_labels.append(i.replace('LambdaCandidates_',''))\n",
    "    else:\n",
    "        new_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_labels=['Candidates_plain_b','Candidates_plain_chi2_geo','Candidates_plain_chi2_prim_first','Candidates_plain_chi2_prim_second','Candidates_plain_distance','Candidates_plain_l','Candidates_plain_l_over_dl','Candidates_plain_mass','Candidates_plain_pT','Candidates_plain_rapidity','Candidates_plain_generation']\n",
    "df_clean_signal = uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm_5m_signal.root:plain_tree',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(select_labels,library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "new_labels_selected = ['b','chi2geo', 'chi2primneg','chi2primpos', 'distance','l', 'ldl','mass', 'pT','rapidity','issignal']\n",
    "df_clean_signal.columns = new_labels_selected\n",
    "signal = df_clean_signal[ (df_clean_signal['mass']>df_clean_signal['mass'].mean()-3*df_clean_signal['mass'].std())\n",
    "               & (df_clean_signal['mass']<df_clean_signal['mass'].mean()+3*df_clean_signal['mass'].std()) ]\n",
    "#signal['issignal']=((signal['issignal']>0)*1)\n",
    "signal = signal[signal['issignal']==1]\n",
    "signal = signal[(signal['b']>=3)&(signal['b']<=8)]\n",
    "signal[\"issignal\"]=signal[\"issignal\"].astype(\"int8\")\n",
    "del df_clean_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal = uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/dcm_signal.root:plain_tree',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "aa = ['b', 'chi2geo','chi2primneg', 'chi2primpos', 'chi2_topo', 'cosine_first', 'cosine_second', 'cosine_topo', 'distance', 'eta', 'l', 'ldl', 'mass', 'p', 'pT', 'phi', 'px', 'py', 'pz', 'rapidity', 'vtx_chi2_first', 'vtx_chi2_second',\n",
    " 'z_first', 'z_second', 'z_smaller', 'M', 'issignal', 'id', 'nhits_mvd_first', 'nhits_mvd_second', 'nhits_mvd_sum', 'nhits_tot_first', 'nhits_tot_second', 'nhits_tot_sum', 'pid']\n",
    "df_clean_signal.columns=aa\n",
    "signal = df_clean_signal[ (df_clean_signal['mass']>df_clean_signal['mass'].mean()-1.5*df_clean_signal['mass'].std())\n",
    "               & (df_clean_signal['mass']<df_clean_signal['mass'].mean()+1.5*df_clean_signal['mass'].std()) & (df_clean_signal['M']>320) & (df_clean_signal['M']<340)]\n",
    "del df_clean_signal\n",
    "signal[\"issignal\"].replace({3: 2, 4: 2, 5:2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal['mass'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = df_clean_signal[ (df_clean_signal['mass']>df_clean_signal['mass'].mean()-3*df_clean_signal['mass'].std())\n",
    "               & (df_clean_signal['mass']<df_clean_signal['mass'].mean()+3*df_clean_signal['mass'].std()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal = tree_importer.tree_importer_with_cuts('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm_5m_signal.root','plain_tree',0,0,9,0,9,0,100,7)\n",
    "gc.collect()\n",
    "\n",
    "signal = df_clean_signal[ (df_clean_signal['mass']>df_clean_signal['mass'].mean()-3*df_clean_signal['mass'].std())\n",
    "               & (df_clean_signal['mass']<df_clean_signal['mass'].mean()+3*df_clean_signal['mass'].std()) ]\n",
    "\n",
    "#signal['issignal']=((signal['issignal']<2)*0 )\n",
    "signal['issignal']=((signal['issignal']>0)*1)\n",
    "signal[\"issignal\"]=signal[\"issignal\"].astype(\"int8\")\n",
    "#signal[\"issignal\"].replace({1: 0, 2: 1}, inplace=True)\n",
    "\n",
    "del df_clean_signal,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "with PdfPages('multipage_pdf.pdf') as pdf:\n",
    "    for j, k in [(3, 0), (6, 3),(9, 6),(20, 9)]:\n",
    "        for i in np.unique(df_scaled['issignal']):\n",
    "            df = df_scaled[(df_scaled['issignal']==i)&(df_scaled['b']<j)&(df_scaled['b']>k)]\n",
    "            h = plt.hist2d(df['rapidity'],df['pT'], bins=100, norm=mpl.colors.LogNorm())\n",
    "            v1 = np.linspace(0, h[0].max(), 4, endpoint=True)\n",
    "            cbar = plt.colorbar()\n",
    "            plt.title(\"URQMD %0.1f<\"%k+ \"b < %0.1f\"%j+\"; MC pid = %0.1f\"%i)\n",
    "            plt.xlabel('rapidity', fontsize=18)\n",
    "            plt.ylabel('pT', fontsize=18)\n",
    "            #plt.legend(\"%f\"%i, fontsize=18)\n",
    "            plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "            del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(df_clean_urqmd.isnull().any( axis = 1 ))\n",
    "np.unique(df_clean_urqmd.isin( [np.inf, -np.inf]).any(axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['index', 'chi2geo','chi2primneg', 'chi2primpos','distance', 'ldl','l','cosine_first', 'cosine_second', 'chi2_topo', 'cosine_topo',\n",
    " 'mass', 'pT', 'b', 'eta', 'p', 'phi', 'rapidity', 'vtx_chi2_first', 'vtx_chi2_second', 'z_first', 'z_second', 'z_smaller', 'M', 'issignal', 'nhits_mvd_first',\n",
    " 'nhits_mvd_second', 'nhits_mvd_sum', 'nhits_tot_first', 'nhits_tot_second', 'nhits_tot_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean_urqmd = uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/urqmd/c_0_pt_0_9_y_0_9_M_320_340_urqmd_125.root:plain_tree',decompression_executor=executor,\n",
    "#                                  interpretation_executor=executor).arrays(library='pd',decompression_executor=executor,\n",
    "#                                  interpretation_executor=executor)\n",
    "df_clean_urqmd = uproot.open('/home/shahid/Mount/gsi/u/Mount/lustre/khan/cbmsoft/at_tree_plainer/install/bin/c_0_pt_0_9_y_0_9_M_200_250_dcm_126_250.root:plain_tree',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "#df_clean_urqmd['issignal']=((df_clean_urqmd['issignal']>0)*1)\n",
    "\n",
    "#df_clean_urqmd[\"issignal\"]=df_clean_urqmd[\"issignal\"].astype(\"int8\")\n",
    "df_clean_urqmd.columns = a\n",
    "#df_clean_urqmd[\"issignal\"].replace({3: 2, 4: 2, 5:2}, inplace=True)\n",
    "df_clean_urqmd= df_clean_urqmd[df_clean_urqmd['issignal']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean =  uproot.open('/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/c_0_pt_0_9_y_0_9_M_320_340_dcm_125.root:plain_tree',decompression_executor=executor,\n",
    "#                                  interpretation_executor=executor).arrays(library='pd',decompression_executor=executor,\n",
    "#                                  interpretation_executor=executor)\n",
    "df_clean =  uproot.open('/home/shahid/Mount/gsi/u/Mount/lustre/khan/cbmsoft/at_tree_plainer/install/bin/dcm/c_0_pt_0_9_y_0_9_M_200_250.root:plain_tree',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor).arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df_clean.columns = a\n",
    "#df_clean['issignal']=((df_clean['issignal']>0)*1)\n",
    "\n",
    "#df_clean[\"issignal\"]=df_clean[\"issignal\"].astype(\"int8\")\n",
    "#df_clean[\"issignal\"].replace({3: 2, 4: 2, 5:2}, inplace=True)\n",
    "df_clean= df_clean[df_clean['issignal']==1]\n",
    "#df_clean1= df_clean[df_clean['issignal']==1]\n",
    "#df_clean2= df_clean[df_clean['issignal']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean = df_clean[(df_clean['xgb_preds1']>0.6)&(df_clean['issignal']>0)]\n",
    "#df_clean_urqmd = df_clean_urqmd[(df_clean_urqmd['xgb_preds1']>0.6)&(df_clean_urqmd['issignal']>0)]\n",
    "df_clean0= df_clean[df_clean['issignal']==0]\n",
    "df_clean1= df_clean[df_clean['issignal']==1]\n",
    "df_clean2= df_clean[df_clean['issignal']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "fin_range = 400\n",
    "bins = 10\n",
    "\n",
    "h1d_dcm0 = ROOT.TH1D(\"Mc1\", \"MC Model\", bins,0,fin_range)\n",
    "h1d_dcm0.SetLineColor(ROOT.kBlack)\n",
    "h1d_dcm0.GetXaxis().SetTitle(\"#chi^{2}_{topo}\")\n",
    "h1d_dcm0.GetYaxis().SetTitle(\"log (counts)\")\n",
    "for i in range(0,len( df_clean0['chi2_topo'])):\n",
    "    h1d_dcm0.Fill( df_clean0['chi2_topo'].iloc[i])\n",
    "\n",
    "h1d_dcm1 = ROOT.TH1D(\"Mc1\", \"MC Model\", bins,0,fin_range)\n",
    "h1d_dcm1.GetXaxis().SetTitle(\"#chi^{2}_{topo}\")\n",
    "h1d_dcm1.GetYaxis().SetTitle(\"log (counts)\")\n",
    "for i in range(0,len( df_clean1['chi2_topo'])):\n",
    "    h1d_dcm1.Fill( df_clean1['chi2_topo'].iloc[i])\n",
    "    \n",
    "h1d_dcm2 = ROOT.TH1D(\"Mc2\", \"Mc2\", bins,0,fin_range)\n",
    "h1d_dcm2.SetLineColor(ROOT.kRed)\n",
    "for i in range(0,len( df_clean2['chi2_topo'])):\n",
    "    h1d_dcm2.Fill( df_clean2['chi2_topo'].iloc[i])\n",
    "    \n",
    "data = ROOT.TH1D(\"data\", \"data\", bins,0,fin_range)\n",
    "data.SetLineColor(ROOT.kBlack)\n",
    "data.SetLineWidth(2)\n",
    "data.SetMarkerSize(0.9)\n",
    "for i in range(0,len( df_clean_urqmd['chi2_topo'])):\n",
    "    data.Fill( df_clean_urqmd['chi2_topo'].iloc[i])\n",
    "    \n",
    "legend = ROOT.TLegend(0.6,0.6,0.8,0.7)\n",
    "legend.AddEntry(h1d_dcm0,\"#Back\",\"l\")\n",
    "legend.AddEntry(h1d_dcm1,\"#Lambda_{primary}\",\"l\")\n",
    "legend.AddEntry(h1d_dcm2,\"#Lambda_{secondary}\",\"l\")\n",
    "c = ROOT.TCanvas()\n",
    "c.SetLogy()\n",
    "c.Draw()\n",
    "\n",
    "h1d_dcm1.Draw()\n",
    "h1d_dcm0.Draw(\"same\")\n",
    "h1d_dcm2.Draw(\"SAME\")\n",
    "legend.Draw()\n",
    "c.Print(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_clean[(df_clean['issignal']==2)&(df_clean['chi2_topo']<fin_range)].shape[0])/ (df_clean[(df_clean['issignal']==1)&(df_clean['chi2_topo']<fin_range)].shape[0]+df_clean[(df_clean['issignal']>1)&(df_clean['chi2_topo']<fin_range)].shape[0]+df_clean[(df_clean['issignal']==0)&(df_clean['chi2_topo']<fin_range)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "ROOT.gStyle.SetOptStat(0)\n",
    "mc = ROOT.TObjArray(2)\n",
    "mc.Add(h1d_dcm0)\n",
    "mc.Add(h1d_dcm1)\n",
    "mc.Add(h1d_dcm2)\n",
    "fit = ROOT.TFractionFitter(data, mc)\n",
    "fitter = fit.GetFitter()\n",
    "fitter.Config().ParSettings(0).Set(\"back\", 0.34, 0.0001, 0.2, 0.5)\n",
    "fitter.Config().ParSettings(1).Set(\"prim\", 0.49, 0.0001, 0.3, 7)\n",
    "fitter.Config().ParSettings(2).Set(\"sec\", 0.16, 0.0001, 0.05, 0.3)\n",
    "#fit.SetRangeX(1,27)\n",
    "status = fit.Fit()\n",
    "if (status == 0):\n",
    "    result = fit.GetPlot()\n",
    "    result.SetLineColor(ROOT.kGreen)\n",
    "    c = ROOT.TCanvas(\"c\",\"\",800,600)\n",
    "    c.Draw()\n",
    "    c.SetLogy()\n",
    "    #data.Draw(\"pe\")\n",
    "    #result.Draw(\"pesame\")\n",
    "    rp = ROOT.TRatioPlot(data,result)\n",
    "    data.SetTitle(\"URQMD\")\n",
    "    data.GetYaxis().SetTitle(\"Counts\")\n",
    "    data.GetXaxis().SetTitle(\"#chi^{2}_{topo}\")\n",
    "    rp.Draw()\n",
    "    legend = ROOT.TLegend(0.6,0.7,0.7,0.8)\n",
    "    legend.AddEntry(data,\"#Lambda hyperon\",\"l\")\n",
    "    legend.AddEntry(result,\"Fit\",\"l\")\n",
    "    legend.Draw(\"same\")\n",
    "    c.Update()\n",
    "    c.Print(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import *\n",
    "value = c_double(0)\n",
    "error = c_double(0)\n",
    "fit.GetResult(0, value, error)\n",
    "par0 =[value.value,error.value]\n",
    "cut_value = par0[0]\n",
    "cut_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value= fin_range\n",
    "(df_clean_urqmd[(df_clean_urqmd['issignal']==2)&(df_clean_urqmd['chi2_topo']<value)].shape[0])/ (df_clean_urqmd[(df_clean_urqmd['issignal']==1)&(df_clean_urqmd['chi2_topo']<value)].shape[0]+df_clean_urqmd[(df_clean_urqmd['issignal']>1)&(df_clean_urqmd['chi2_topo']<value)].shape[0]+df_clean_urqmd[(df_clean_urqmd['issignal']==0)&(df_clean_urqmd['chi2_topo']<value)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.700021951723989\n",
    "0.7194598687531549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.GetChisquare()/fit.GetNDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "#df_clean = df_clean[(df_clean['issignal']==1)&(df_clean['chi2_topo']<value)]\n",
    "#df_clean_urqmd = df_clean_urqmd[(df_clean_urqmd['issignal']>0)&(df_clean_urqmd['chi2_topo']<value)]\n",
    "\n",
    "h2d_dcm = ROOT.TH2F(\"Mc\", \"Mc\", 5,0,3,5,0,3)\n",
    "for i in range(0,len( df_clean['rapidity'])):\n",
    "    h2d_dcm.Fill( df_clean['rapidity'].iloc[i],df_clean['pT'].iloc[i])\n",
    "h2d_dcm1=h2d_dcm.Clone()\n",
    "type(h2d_dcm)\n",
    "\n",
    "h2d_urqmd = ROOT.TH2F(\"urqmd\", \"urqmd\", 5,0,3,5,0,3)\n",
    "for i in range(0,len(df_clean_urqmd['rapidity'])):\n",
    "    h2d_urqmd.Fill(df_clean_urqmd['rapidity'].iloc[i],df_clean_urqmd['pT'].iloc[i])\n",
    "type(h2d_urqmd)\n",
    "h2d_urqmd1=h2d_urqmd.Clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = uproot.recreate(\"new_c0_pt_y_y_yield_bdt_cut_0.8.root\")\n",
    "file[\"t1\"] = df_clean_urqmd\n",
    "file[\"t2\"] = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(7)\n",
    "df_clean = uproot.open(\"new_c0_pt_y_y_yield_bdt_cut_0.8.root:t2\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df_clean_urqmd = uproot.open(\"new_c0_pt_y_y_yield_bdt_cut_0.8.root:t1\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_clean_urqmd = df_clean_urqmd.sample(int(df_clean_urqmd.shape[0]*0.7194598687531549))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "bins =15\n",
    "h2d_dcm = ROOT.TH2F(\"Mc DCM\", \"Reconstructed_{DCM1}/Simulated_{DCM1}\", bins,0,3,bins,0,3)\n",
    "for i in range(0,len( df_clean['rapidity'])):\n",
    "    h2d_dcm.Fill( df_clean['rapidity'].iloc[i],df_clean['pT'].iloc[i])\n",
    "type(h2d_dcm)\n",
    "\n",
    "h2d_urqmd = ROOT.TH2F(\"urqmd\", \"Reconstructed_{DCM2}/Simulated_{DCM2} \", bins,0,3,bins,0,3)\n",
    "for i in range(0,len(df_clean_urqmd['rapidity'])):\n",
    "    h2d_urqmd.Fill(df_clean_urqmd['rapidity'].iloc[i],df_clean_urqmd['pT'].iloc[i])\n",
    "type(h2d_urqmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_prop(h1,h2):\n",
    "    import numpy as np\n",
    "    for i in range(1,h1.GetNbinsX()):\n",
    "        for j in range(1,h1.GetNbinsY()):\n",
    "            a  = h1 . GetBinContent(i, j)\n",
    "            b  = h2 . GetBinContent(i, j)\n",
    "            da = h1 . GetBinError(i, j)\n",
    "            db = h2 . GetBinError(i, j)\n",
    "            if (a!=0 and b!=0):\n",
    "                error = h1 . GetBinContent(i, j) * np.sqrt( ((da * da) / (a * a)) + ((db * db) / (b * b)) )\n",
    "                h1 . SetBinError(i, j, error)\n",
    "                return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/m_200_250_dcm_prim_15bins_125.root\")\n",
    "MC_DCM=file.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "file1 = ROOT.TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/m_200_250_dcm_primaries_126_250.root\")\n",
    "MC_URQMD=file1.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "\n",
    "\n",
    "\n",
    "#c1.SetLogy()\n",
    "\n",
    "h2d_dcm.Divide(MC_DCM)\n",
    "#h2d_dcm1 = error_prop(h2d_dcm,MC_DCM)\n",
    "h2d_urqmd.Divide(MC_URQMD)\n",
    "#h2d_urqmd.Divide(MC_URQMD)\n",
    "h2d_urqmd1 = error_prop(h2d_urqmd, h2d_dcm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.gStyle.SetPaintTextFormat(\"4.2f\")\n",
    "c = ROOT.TCanvas(\"\",\"\")\n",
    "c.Draw()\n",
    "#h2d_urqmd.Divide(MC_URQMD)\n",
    "h2d_dcm.Draw(\"colz\")\n",
    "h2d_dcm.Draw(\"TEXT SAME\")\n",
    "c.Print(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.gInterpreter.Declare('''\n",
    "void Print_start(TCanvas* c) {\n",
    "c->Print(\"hists.pdf(\",\"pdf\");\n",
    "}\n",
    "''')\n",
    "\n",
    "ROOT.gInterpreter.Declare('''\n",
    "void Print_continue(TCanvas* c) {\n",
    "c->Print(\"hists.pdf\",\"pdf\");\n",
    "}\n",
    "''')\n",
    "\n",
    "ROOT.gInterpreter.Declare('''\n",
    "void Print_end(TCanvas* c) {\n",
    "c->Print(\"hists.pdf)\",\"pdf\");\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable = [ProjectionX,ProjectionY]\n",
    "c = ROOT.TCanvas(\"\",\"\")\n",
    "for i in range(0,16):\n",
    "    bin1 = bin2 =i\n",
    "    ROOT.gStyle.SetPaintTextFormat(\"4.2f\")\n",
    "    corr_urqmd_y = h2d_urqmd1.ProjectionX(\"corr_urqmd_y\",bin1,bin2)\n",
    "    corr_urqmd_y . SetLineColor(ROOT.kRed)\n",
    "    corr_urqmd_y . SetMarkerColor(ROOT.kRed)\n",
    "    corr_urqmd_y . SetLineWidth(2)\n",
    "    corr_urqmd_y . SetMarkerStyle(21)\n",
    "\n",
    "    mc_urqmd_y = MC_URQMD.ProjectionX(\"mc_urqmd_y\", bin1,bin2)\n",
    "    mc_urqmd_y . SetLineWidth(2)\n",
    "    mc_urqmd_y . SetMarkerStyle(22)\n",
    "    \n",
    "    c.Clear()\n",
    "    mc_urqmd_y . Draw(\"pe1\")\n",
    "    corr_urqmd_y . Draw(\"pe1same\")\n",
    "\n",
    "    mc_urqmd_y.SetStats (0)\n",
    "    #ROOT.Print_continue(c)\n",
    "    if (i==0):\n",
    "        ROOT.Print_start(c)\n",
    "    else:\n",
    "        ROOT.Print_continue(c)\n",
    "\n",
    "    #h2d_urqmd.Draw(\"TEXT SAME\")\n",
    "\n",
    "#ROOT.Print_end(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.SaveAs (\"hists.pdf ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_clean.isnull().any( axis = 1 ))\n",
    "np.unique(df_clean.isin( [np.inf, -np.inf]).any(axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels=['$\\chi^{2}_{geometrical}$', '$\\chi^{2}_{primary\\ \\pi^-}$','$\\chi^{2}_{primary\\ proton}$', 'DCA (cm)', 'L/$\\Delta$L','mass', '$p_{T}$', '$y_{LAB}$','issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_bac_dist(df1):\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    with PdfPages('multipage_pdf.pdf') as pdf:\n",
    "        df = df1.copy()\n",
    "        for i in df1.columns:\n",
    "            list =['chi2geo', 'chi2primneg', 'chi2primpos', 'chi2topo', 'cosinepos', 'cosinetopo', 'distance', 'l', 'ldl', 'b', 'rapidity','mass', 'pT']\n",
    "            if i in df1.columns:\n",
    "                df[i]=np.log(df[i])\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            with pd.option_context('mode.use_inf_as_na', True):\n",
    "                df = df.dropna()\n",
    "            bin1 = 300 \n",
    "            plt.hist(df[df['issignal']==0].sample(int(df[df['issignal']>1].shape[0]))[i],bins = bin1, color = 'blue',alpha = 0.3,label='Background')\n",
    "            plt.hist(df[df['issignal']==1].sample(int(df[df['issignal']>1].shape[0]))[i],bins = bin1, color = 'red',label='Primaries', alpha =0.3)\n",
    "            plt.hist(df[df['issignal']>1][i],bins = bin1, color = 'green',label='Secondaries', alpha =0.3)\n",
    "            #plt.hist(df[df['issignal']>2][i],bins = bin1, color = 'yellow',label='>2', alpha =0.3)\n",
    "            plt.yscale('log')\n",
    "            #plt.grid()\n",
    "            plt.ylabel('counts (log scale)', fontsize = 18)\n",
    "            #plt.xlabel('$\\chi^{2}_{geometrical}$', fontsize = 18)\n",
    "            plt.legend(loc='upper left',fontsize=15)\n",
    "            plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "            #ax.text(0, 1500, r'CBM Performance', fontsize=15)\n",
    "            #ax.text(0, 500, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "            plt.title('DCM, Au+Au @ 12 $A$GeV/$c$', fontsize=18)\n",
    "            plt.xlabel(\"log \"+i, fontsize = 18)\n",
    "            #plt.xlim([1.07,1.2])\n",
    "            #plt.show()\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        del df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'chi2_topo'\n",
    "df = df_clean\n",
    "#bins1 =100\n",
    "range1 = (0,1000)\n",
    "\n",
    "df[df['issignal']==0].sample(int(df[df['issignal']>1].shape[0]))[variable].hist(range = range1, histtype='step', label='back', figsize=(12,8))\n",
    "df[df['issignal']==1].sample(int(df[df['issignal']>1].shape[0]))[variable].hist(range = range1,histtype='step', label='prim', figsize=(12,8))\n",
    "df[df['issignal']>1][variable].hist(range = range1, label='sec',histtype='step', figsize=(12,8))\n",
    "plt.legend(loc='upper right',fontsize=15)\n",
    "#plt.xlim(-10,100)\n",
    "#plt.ylim(0,1000000)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "#plt.ylabel('log (counts)', fontsize = 18)\n",
    "plt.text(300, 50000, r'CBM Performance', fontsize=15)\n",
    "plt.text(300,30000, r'DCM, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(variable,fontsize = 18 )\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal['mass'].hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "bin1 = 300 \n",
    "range1=[0,4000]\n",
    "plt.hist(df_urqmd_5k[df_urqmd_5k['issignal']==0]['ldl'],bins = bin1,range=range1, color = 'red',alpha = 0.3,label='Background')\n",
    "plt.hist(df_urqmd_5k[df_urqmd_5k['issignal']==1]['ldl'],bins = bin1, range=range1, color = 'blue',label='Signal', alpha =0.3)\n",
    "#plt.vlines(x=4,ymin=-1,ymax=10000, color='r', linestyle='-')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.ylabel('counts (log scale)', fontsize = 18)\n",
    "plt.xlabel(plot_labels[4], fontsize = 18)\n",
    "plt.legend(loc='upper right',fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.text(0.3, 15000, r'CBM Performance', fontsize=15)\n",
    "ax.text(0.3, 5000, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "#plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "#ax.text(4, 10000, r'$PFSimple$', fontsize=20, color ='r')\n",
    "#plt.xlim([0,20])\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig('hists.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data frame object has some columns/features and for them at the very last column the true Monte Carlos information is available. This MC information tells us whether this reconstructed particle was originally produced as a decaying particle or not. So a value of 1 means that it is a true candidate and 0 means that it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which do not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. Ofcourse, we lose some data points but these outliers sometimes cause problems when we perform analysis. \n",
    "\n",
    "Since our experiment is a fixed target experiment so there are certain constraints which have to be applied on the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_bac(MC,data):\n",
    "    signal_selected= MC[MC['issignal']]\n",
    "    background_selected = data[(data['issignal'] == 0)\n",
    "                    & ((data['mass'] > 1.07)\n",
    "                    & (data['mass'] < 1.108) | (data['mass']>1.1227) \n",
    "                       & (data['mass'] < 1.3))].sample(n=3*(signal_selected.shape[0]))\n",
    "    dfs = [signal_selected, background_selected]\n",
    "    df_scaled = pd.concat(dfs)\n",
    "    df_scaled = df_scaled.sample(frac=1)\n",
    "    del dfs, signal_selected, background_selected\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac(signal,df_clean_urqmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back = df_clean_urqmd[(df_clean_urqmd['issignal'] == 0)\n",
    "                & ((df_clean_urqmd['mass'] > 1.077)\n",
    "                & (df_clean_urqmd['mass'] < 1.1) | (df_clean_urqmd['mass']>1.135) \n",
    "                   & (df_clean_urqmd['mass'] < 1.2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Background and Signal\n",
    "Our sample contains a lot of background (2178718) and somewhat signal candidates (36203). For analysis we will use a signal set of 4000 candidates and a background set of 12000 candidates. The background and signal candidates will be selected by using MC information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_selected= signal[(signal['mass']>1.1) & (signal['mass']<1.135)]\n",
    "#background_selected = df_clean_urqmd[(df_clean_urqmd['issignal'] == 0)\n",
    "#                & ((df_clean_urqmd['mass'] > 1.077)\n",
    "#                & (df_clean_urqmd['mass'] < 1.1) | (df_clean_urqmd['mass']>1.135) \n",
    "#                   & (df_clean_urqmd['mass'] < 1.3))].sample(n=3*(signal_selected.shape[0]))\n",
    "background_selected = back.sample(n=3*(signal_selected.shape[0]))\n",
    "del back\n",
    "gc.collect()\n",
    "\n",
    "#Let's combine signal and background\n",
    "dfs = [signal_selected, background_selected]\n",
    "df_scaled = pd.concat(dfs)\n",
    "\n",
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "#del dfs, signal_selected, background_selected, signal\n",
    "# Let's take a look at the top 10 entries of the df\n",
    "df_scaled.iloc[0:10,:]\n",
    "del signal, signal_selected, background_selected, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaled.shape)\n",
    "print(df_scaled[df_scaled['issignal']==1].shape)\n",
    "print(df_scaled[df_scaled['issignal']==2].shape)\n",
    "(5805268, 18)\n",
    "(1056684, 18)\n",
    "(394633, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(df_clean_urqmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 0.5 * (np.log(E+P/E-P))\n",
    "\n",
    "\n",
    "https://cbm-wiki.gsi.de/foswiki/bin/view/PWG/CbmCollisionEnergies\n",
    "\n",
    "\n",
    "y = 0.5 * (np.log((12+10)/(12-10)))\n",
    "\n",
    "\n",
    "using this the rapidity is 3.1992 for Ebeam =12.04 and pbeam =12 and mid rapidity is y/2=  1.5996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt_sig_back(df_scaled)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(8)\n",
    "axs.text(1.13, 6000, r'DCM-QGSM-SMM', color = 'magenta',  fontsize=15)\n",
    "axs.text(1.13, 4000, r'Au+Au @ 12 $A$GeV/$c$', color = 'magenta',  fontsize=15)\n",
    "axs.text(1.13, 2000, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "fig.savefig(\"hists.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1 = signal_selected[signal_selected['issignal']==0]\n",
    "new2 = signal_selected[signal_selected['issignal']==1]\n",
    "new3 = new1.sample(n=1*(new2.shape[0]))\n",
    "dfs = [new2, new3]\n",
    "df_scaled = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_selected.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "#cuts = [ 'chi2geo', 'chi2primneg', 'chi2primpos', 'distance', 'ldl']\n",
    "cuts = [ 'chi2geo', 'chi2primneg', 'chi2primpos', 'chi2_topo', 'cosine_topo', 'distance', 'ldl', 'nhits_mvd_first','nhits_mvd_second','nhits_tot_first','nhits_tot_second']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "x_whole = df_clean[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "#y_whole = pd.DataFrame(df_clean['issignal'], dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFPF_lambda_cuts(df):\n",
    "    KFPF_lambda= df.copy()\n",
    "    KFPF_lambda['new_signal']=0\n",
    "    mask1 = (KFPF_lambda['chi2primpos'] > 18.4) & (KFPF_lambda['chi2primneg'] > 18.4)\n",
    "\n",
    "    mask2 = (KFPF_lambda['ldl'] > 5) & (KFPF_lambda['distance'] < 1)\n",
    "\n",
    "    mask3 = (KFPF_lambda['chi2geo'] < 3) \n",
    "\n",
    "    KFPF_lambda = KFPF_lambda[(mask1) & (mask2) & (mask3)] \n",
    "\n",
    "    #After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "    # column by 1\n",
    "    KFPF_lambda['new_signal'] = 1\n",
    "    return KFPF_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a new df \n",
    "\n",
    "#new_check_set=KFPF_lambda_cuts(df_original)\n",
    "new_check_set=KFPF_lambda_cuts(df_clean_urqmd)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>XGB Boost \n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient. Stratify so that both train and test get the same ratio of signal to background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=324, stratify=y)\n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "#dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "#dtest1=xgb.DMatrix(x_test, label = y_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_1 = df_clean_urqmd[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "#y_whole_1 = pd.DataFrame(df_clean_urqmd['issignal'], dtype='int8')\n",
    "#dtest2 = xgb.DMatrix(x_whole_1, label = y_whole_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_test, y_test, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_1 = df_clean_urqmd_bac[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "#y_whole_1 = pd.DataFrame(df_clean_urqmd_bac['issignal'], dtype='int')\n",
    "#dtest2 = xgb.DMatrix(x_whole_1, label = y_whole_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate,scale_pos_weight):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,'scale_pos_weight':scale_pos_weight,\n",
    "              'subsample': 0.8,\n",
    "              'eval_metric': 'auc','tree_method':'hist','objective':'binary:logistic', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0.01,1),\n",
    "                                             'n_estimators':(100,500),'scale_pos_weight':(1,10)\n",
    "                                            })\n",
    "\n",
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")\n",
    "#0.9983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds absolute value of magnitude of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "\n",
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8, \n",
    "              'num_class':np.unique(dtrain.get_label()).shape[0], \n",
    "              'eval_metric': 'auc','tree_method':'hist', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0.01,1),\n",
    "                                             'n_estimators':(100,1000)\n",
    "                                            })\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")\n",
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')\n",
    "#0.9951\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import sqrt, log, argmax\n",
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its\n",
    "discrimination threshold is varied. This function requires the true binary value and the target scores, which can either be probability estimates of\n",
    "the positive class, confidence values, or binary decisions.\n",
    "The function roc_auc_score computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "To find the best threshold which results more signal to background ratio for lambda candidates we use the parameter S0 called the approximate median significance\n",
    "by the higgs boson  ML challenge (http://higgsml.lal.in2p3.fr/documentation,9.)\n",
    "\"\"\"\n",
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linewidth=3 ,linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random guess')\n",
    "    #plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n AMS = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', s=80, color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n AMS = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 18)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('hists.png')\n",
    "    plt.show()\n",
    "    return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict, average='macro', multi_class='raise')\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1, average='macro', multi_class='raise')\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linewidth=3 ,linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random guess')\n",
    "    #plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n AMS = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', s=80, color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n AMS = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 18)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('hists.png')\n",
    "    plt.show()\n",
    "    return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_urqmd_bac['xgb_preds'] = bst.predict(dtest2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), \n",
    "         'objective':'binary:logistic','tree_method':'hist','nthread' : 7}\n",
    "\n",
    "#Fit/train on training data\n",
    "bst = xgb.train(param, dtrain, num_boost_round=20)\n",
    "\n",
    "#predicitions on training set\n",
    "bst_train= pd.DataFrame(data=bst.predict(dtrain),  columns=[\"xgb_preds\"])\n",
    "y_train=y_train.set_index(np.arange(0,bst_train.shape[0]))\n",
    "bst_train['issignal']=y_train['issignal']\n",
    "\n",
    "#predictions on test set\n",
    "bst_test = pd.DataFrame(data=bst.predict(dtest1),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']\n",
    "\n",
    "#ROC cures for the predictions on train and test sets\n",
    "train_best, test_best = AMS(y_train, bst_train['xgb_preds'],y_test, bst_test['xgb_preds'])\n",
    "\n",
    "#The first argument should be a data frame, the second a column in it, in the form 'preds'\n",
    "preds_prob(bst_train,'xgb_preds', 'issignal',bst_test,'xgb_preds', 'issignal')\n",
    "\n",
    "#Applying XGB on the 100k events data-set\n",
    "df_clean['xgb_preds'] = bst.predict(dtest)\n",
    "#preds_prob(df_clean,'xgb_preds', 'issignal','test')\n",
    "\n",
    "df_clean_urqmd['xgb_preds'] = bst.predict(dtest2)\n",
    "#del x_test, y_test, x_train, y_train, dtrain, dtest, x_whole, y_whole, x_whole_1, y_whole_1, dtest1, df_scaled\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_test, y_test, x_train, y_train, dtrain, dtest, x_whole, y_whole, x_whole_1, y_whole_1, dtest1, dtest2, df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del dtest, dtest1, dtest2, dtrain, df_scaled\n",
    "#del \n",
    "del x_test, y_test, x_train, y_train, x_whole, y_whole, x_whole_1, y_whole_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), \n",
    "         'objective':'binary:logistic','tree_method':'hist','nthread' : 7}\n",
    "\n",
    "#Fit/train on training data\n",
    "bst = xgb.XGBClassifier(**param).fit(x_train, y_train)\n",
    "\n",
    "#predicitions on training set\n",
    "bst_train= pd.DataFrame(data=bst.predict_proba(x_train))\n",
    "y_train=y_train.set_index(np.arange(0,bst_train.shape[0]))\n",
    "bst_train['issignal']=y_train['issignal']\n",
    "\n",
    "bst_test = pd.DataFrame(data=bst.predict_proba(x_test))\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']\n",
    "\n",
    "#Applying XGB on the 100k events data-set\n",
    "bst_test1 = pd.DataFrame(data=bst.predict_proba(x_whole))\n",
    "df_clean['xgb_preds0'], df_clean['xgb_preds1'], df_clean['xgb_preds2']= bst_test1[0], bst_test1[1], bst_test1[2]\n",
    "#preds_prob(df_clean,'xgb_preds', 'issignal','test')\n",
    "\n",
    "bst_test2 = pd.DataFrame(data=bst.predict_proba(x_whole_1))\n",
    "df_clean_urqmd['xgb_preds0'], df_clean_urqmd['xgb_preds1'], df_clean_urqmd['xgb_preds2']= bst_test2[0], bst_test2[1], bst_test2[2]\n",
    "\n",
    "\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), \n",
    "         'objective':'binary:logistic','tree_method':'hist','nthread' : 7}\n",
    "\n",
    "#Fit/train on training data\n",
    "bst = xgb.XGBClassifier(**param).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying XGB on the 100k events data-set\n",
    "\n",
    "x_whole = df_clean[cuts].copy()\n",
    "bst_test1 = pd.DataFrame(data=bst.predict_proba(x_whole))\n",
    "df_clean['xgb_preds0'], df_clean['xgb_preds1'], df_clean['xgb_preds2']= bst_test1[0], bst_test1[1], bst_test1[2]\n",
    "#preds_prob(df_clean,'xgb_preds', 'issignal','test')\n",
    "del x_whole, bst_test1\n",
    "\n",
    "x_whole_1 = df_clean_urqmd[cuts].copy()\n",
    "bst_test2 = pd.DataFrame(data=bst.predict_proba(x_whole_1))\n",
    "df_clean_urqmd['xgb_preds0'], df_clean_urqmd['xgb_preds1'], df_clean_urqmd['xgb_preds2']= bst_test2[0], bst_test2[1], bst_test2[2]\n",
    "del x_whole_1,  bst_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following graph will show us that which features are important for the model\n",
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df3_base['xgb_preds1'], bins=300, label=0)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#plt.hist(bst_test['xgb_preds'], bins=300)\n",
    "#plt.yscale('log')\n",
    "bst_test['xgb_preds'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_clean['xgb_preds0'], bins=300, label=0)\n",
    "plt.hist(df_clean['xgb_preds1'], bins=300, label=1)\n",
    "plt.hist(df_clean['xgb_preds2'], bins=300, label=2)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_clean_urqmd_bac['xgb_preds'], bins=300, label=0)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = preds_prob(bst_test,'xgb_preds0', 'issignal','test')\n",
    "fig.savefig('hists.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_test = pd.DataFrame(data=bst.predict(x_test),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_prob(df,preds,true,df1,preds1, true1):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bins1=100\n",
    "    TP = df[(df[true]==1)]\n",
    "    TN = df[(df[true]==0)]\n",
    "    \n",
    "    plt.hist(TN[preds], bins=bins1,facecolor='blue',alpha = 0.3, label='background in train')\n",
    "    plt.hist(TP[preds], bins=bins1,facecolor='red',alpha = 0.3, label='signal in train')\n",
    "    del TP, TN\n",
    "    \n",
    "    TP1 = df1[(df1[true1]==1)]\n",
    "    TN1 = df1[(df1[true1]==0)]\n",
    "    \n",
    "    hist1, bins1 = np.histogram(TN1[preds1], bins=bins1)\n",
    "    err1 = np.sqrt(hist1)\n",
    "    center1 = (bins1[:-1] + bins1[1:]) / 2\n",
    "    plt.errorbar(center1, hist1, yerr=err1, fmt='o',\n",
    "                 c='blue', label='background in test')\n",
    "    \n",
    "    hist, bins = np.histogram(TP1[preds1], bins=bins1)\n",
    "    err = np.sqrt(hist)\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o',\n",
    "                 c='red', label='signal in test')\n",
    "    del TP1, TN1\n",
    "    \n",
    "   # ax.annotate('cut on probability', xy=(0, 90),  xycoords='data',xytext=(0.13,0.5), textcoords='axes fraction',\n",
    "    #            fontsize=15,arrowprops=dict(facecolor='black', shrink=0.05),horizontalalignment='right', verticalalignment='top')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if df[true].unique().shape[0]>2:\n",
    "        TP2= df[df[true]>1]\n",
    "        plt.hist(TP2[preds], bins=bins1,facecolor='green',alpha = 0.3, label='secondaries in train')\n",
    "        TP2= df1[df1[true1]>1]\n",
    "        hist2, bins2 = np.histogram(TP2[preds1], bins=bins1)\n",
    "        center2 = (bins2[:-1] + bins2[1:]) / 2\n",
    "        err2 = np.sqrt(hist2)\n",
    "        plt.errorbar(center2, hist2,yerr=err2, fmt='o',c='green',label='secondaries in test')\n",
    "\n",
    "    del TP2\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Probability',fontsize=18)\n",
    "    plt.ylabel('Counts', fontsize=18)\n",
    "    ax.set_xticks(np.arange(0,1.1,0.1))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    plt.legend(fontsize=18)\n",
    "    fig.show()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "    #fig.savefig('Lambda_XGB_prediction_0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "h=plt.hist2d(df_clean['rapidity'],df_clean['pT'], bins=15*25, norm=mpl.colors.LogNorm())\n",
    "plt.xlabel('$y_{Lab}$',fontsize=15)\n",
    "plt.ylabel(\"pT\",fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('hists.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_train1 = bst_train[(bst_train[0]<0.35) ]\n",
    "bst_test1 = bst_test[(bst_test[0]<0.35) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base = df_clean[(df_clean['rapidity']<0.5) & (df_clean['pT']<0.5) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3_base[df3_base['issignal']==0].shape[0]/529256.0)\n",
    "print(df3_base[df3_base['issignal']==1].shape[0]/101)\n",
    "print(df3_base[df3_base['issignal']>1].shape[0]/35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_urqmd = df_clean_urqmd[df_clean_urqmd['xgb_preds1']>0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[df_clean['xgb_preds1']>0.6]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_prob(bst_train,0, 'issignal',bst_test,0, 'issignal')\n",
    "cut = 0\n",
    "#df = df_clean[df_clean['xgb_preds0']>=cut]\n",
    "#df = df[df['xgb_preds1']>0.6]\n",
    "for i in ['xgb_preds0','xgb_preds1','xgb_preds2']:\n",
    "#for i in ['xgb_preds2']:\n",
    "    fig, ax = preds_prob(df_clean_urqmd,i, 'issignal',df_clean_urqmd,i, 'issignal')\n",
    "    plt.legend([\"back\",\"prim\",\"second\"],fontsize=18)\n",
    "    plt.\n",
    "    fig.savefig(\"hists\"+str(i)+\".png\")\n",
    "\n",
    "#del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import sqrt, log, argmax\n",
    "import itertools\n",
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    #roc_auc=roc_auc_score(y_true, y_predict, multi_class='ovo', average='weighted')\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    #S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    #S0 = S0[~np.isnan(S0)]\n",
    "    #xi = argmax(S0)\n",
    "    #S0_best_threshold = (thresholds[xi])\n",
    "\n",
    "    #roc_auc1=roc_auc_score(y_true1, y_predict1, multi_class='ovo', average='weighted')\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    #S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    #S01 = S01[~np.isnan(S01)]\n",
    "    #xi1 = argmax(S01)\n",
    "    #S0_best_threshold1 = (thresholds[xi1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linewidth=3 ,linestyle=':',color='darkorange')\n",
    "    plt.plot(fpr1, tpr1, color='green')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random guess')\n",
    "    #plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n AMS = '+ \"%.2f\" % S0[xi])\n",
    "    #plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', s=80, color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n AMS = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 18)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 18)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 18)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 18)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('hists.png')\n",
    "    plt.show()\n",
    "    #return S0_best_threshold, S0_best_threshold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best, test_best = AMS(y_train, bst_train[1],y_test, bst_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will display the inavriant mass histogram of the original 10k event set along with the mass histoigram after we apply a cut\n",
    "# on the probability prediction of xgb\n",
    "def cut_visualization(df, variable,cut, range1=(1.09, 1.15), bins1= 300 ):\n",
    "    mask1 = df[variable]>cut\n",
    "    df3=df[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(8, 6), dpi = 300)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(df['mass'],bins = bins1, range=range1, facecolor='blue' ,alpha = 0.35, label='no selection')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc=(0.55, 0.55))\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax2.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "    \n",
    "    \n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label=\"XGB  > %.2f\"%cut+'')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax1.legend( fontsize = 15,loc=(0.55, 0.47) )\n",
    "\n",
    "    #plt.title(\"The original sample's Invariant Mass along with mass after selection of XGB\", fontsize = 15)\n",
    "    plt.text(1.09, 6000, 'CBM Performance', fontsize=15)\n",
    "    plt.text(1.09, 5500, 'URQMD, Au+Au$', fontsize=15)\n",
    "    plt.text(1.09, 5000, '@ 12A GeV/$c$', fontsize=15)\n",
    "    plt.text(1.13, 6000, '$\\Lambda$ hyperons', fontsize=15)\n",
    "    #plt.text(0.02, 0.1, r'cut > %.4f'%cut, fontsize=15)\n",
    "    plt.show()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"Fig2_Lambda_XGB_selection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_visualization(df_clean_urqmd,'xgb_preds',test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range1= (1.11, 1.122)\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.1) ]['mass'].plot.hist(bins = bins1, range=range1, histtype='step', Fill=False,sharey=True, label='BDT > 0.1 XGB selected $\\Lambda$s')\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.5) ]['mass'].plot.hist(bins = bins1, range=range1, histtype='step', Fill=False,sharey=True, label='BDT > 0.5 XGB selected $\\Lambda$s')\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.8) ]['mass'].plot.hist(bins = bins1, range=range1, histtype='step', Fill=False,sharey=True, label='BDT > 0.8 XGB selected $\\Lambda$s')\n",
    "df_clean_urqmd[(df_clean_urqmd['xgb_preds']>0.9) ]['mass'].plot.hist(bins = bins1, range=range1, color = 'red', histtype='step', Fill=False,sharey=True, label='BDT > 0.9 XGB selected $\\Lambda$s')\n",
    "\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.93\n",
    "#df3_base=df_clean_urqmd[(df_clean_urqmd['xgb_preds']>cut3) ]\n",
    "df3_base=df_clean_urqmd[df_clean_urqmd['xgb_preds1']>0.6]\n",
    "#df3_base=df3_base[(df3_base['xgb_preds2']<0.45)&(df3_base['xgb_preds2']>0.15)]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "#xgb\n",
    "\n",
    "#issignal has 0,1,2 . So we convert all signals above zero to 1\n",
    "\n",
    "\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True, '\\n True positives = \\n (MC =1)\\n signal in \\n the distribution')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = bins1, range=range1,facecolor='magenta',alpha = 0.3,grid=True,sharey=True )\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"XGB selected $\\Lambda$ candidates with a cut of %.3f \"%cut3 +\"on the XGB back probability distribution\", fontsize = 18)\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "#axs.text(1.123, 4000, 'CBM Performance', fontsize=18)\n",
    "#axs.text(1.123, 3500, 'URQMD, Au+Au @ 12A GeV/$c$', fontsize=18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_bac_dist(df3_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base[(df3_base['issignal']==0)&(df3_base['chi2topo']<50)]['chi2topo'].hist(bins =100)\n",
    "df3_base[(df3_base['issignal']==1) &(df3_base['chi2topo']<50)]['chi2topo'].hist(bins =100)\n",
    "plt.xlim(0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3_base[(df3_base['issignal']==0)&(df3_base['chi2topo']<5)].shape[0])\n",
    "print(df3_base[(df3_base['issignal']==1) &(df3_base['chi2topo']<5)].shape[0])\n",
    "print(df3_base[df3_base['issignal']>1].shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base['cosinetopo'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.9\n",
    "#mask1 = df_clean_urqmd['xgb_preds']>cut3\n",
    "#df3_base0=df_clean_urqmd[mask1]\n",
    "#df3_base = pd.concat([df_clean_urqmd,df3_base0]).drop_duplicates(keep=False)\n",
    "mask1 = df_clean_urqmd['xgb_preds']>cut3\n",
    "df3_base=df_clean_urqmd[mask1]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True, label ='\\n True positives = \\n (MC =1)\\n signal in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "plt.title(\"XGB selected $\\Lambda$ candidates with a cut of %.3f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.text(1.123, 5000, 'CBM Performance', fontsize=18)\n",
    "axs.text(1.123, 4000, 'URQMD, Au+Au @ 12A GeV/$c$', fontsize=18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.9\n",
    "mask1 = df_clean_urqmd['xgb_preds']>cut3\n",
    "df3_base=df_clean_urqmd[mask1]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "plt.title(\"XGB selected $K_{s}$ candidates with a cut of %.3f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficiency_plot_mass(df,signal_column, predictions_column, cut_value, range_min, range_max, bin1):\n",
    "    from matplotlib import gridspec\n",
    "    x_min, x_max = range_min , range_max\n",
    "    range1= (x_min, x_max)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1,figsize=(10,10), sharex=True, constrained_layout=True,  gridspec_kw={'width_ratios': [10],\n",
    "                               'height_ratios': [8,4]})\n",
    "    \n",
    "    ns, bins, patches=axs[0].hist((df[(df[predictions_column]>cut_value) & (df[signal_column]==1)]['mass']),bins = bin1,histtype='step', range=range1,Fill=False, color='red', facecolor='red', linewidth=2)\n",
    "    ns1, bins1, patches1=axs[0].hist((df[df[signal_column]==1]['mass']),bins = bin1,histtype='step', Fill=False, range=range1,facecolor='blue',linewidth=2)\n",
    "\n",
    "    #plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "    axs[0].set_ylabel(\"log(counts)\", fontsize = 18)\n",
    "    axs[0].legend(('XGBoost TP','MC TP'), fontsize = 18, loc='upper right')\n",
    "    axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "    axs[0].set_yscale('log')\n",
    "\n",
    "    err = np.std(ns)\n",
    "    err1 = np.std(ns1)\n",
    "    corr_ns_ns1 = np.corrcoef(ns,ns1)[[0],[1]][0]\n",
    "    err_dif = (ns / ns1) * (np.sqrt( ((err/ns)**2) + ((err1/ns1)**2)\n",
    "                                      -2* ((corr_ns_ns1*err*err1)/(ns*ns1))))\n",
    "\n",
    "\n",
    "    axs[1].hlines(y=1, xmin=x_min, xmax=x_max, colors='black', linestyles='dashed', label='')\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    axs[1].errorbar(center,  ns / ns1, yerr=err_dif,  fmt='o',\n",
    "                     c='Blue', label='Background in predictions')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "    axs[1].set_ylabel(\"XGB / MC\", fontsize = 18)\n",
    "    axs[1].grid()\n",
    "    axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "    fig.show()\n",
    "    fig.tight_layout()\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_plot_mass(df_clean_urqmd,'issignal', 'xgb_preds', 0.96, 1.112, 1.12, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pT_vs_rapidity(df, var_xaxis , var_yaxis , range_var_xaxis, range_var_yaxis,signal_column, predictions_column, cut_value):\n",
    "    import matplotlib as mpl\n",
    "    fig, axs = plt.subplots(figsize=(6, 4),dpi = 300)\n",
    "    h=plt.hist2d(df[(df[predictions_column]>cut_value) & (df[signal_column]==1)][var_xaxis],\n",
    "                                 df[(df[predictions_column]>cut_value) & (df[signal_column]==1)][var_yaxis],\n",
    "                                 range=[range_var_xaxis,range_var_yaxis], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())\n",
    "    cbar = plt.colorbar()\n",
    "    h=plt.hist2d(df[df[signal_column]==1][var_xaxis],df[df[signal_column]==1][var_yaxis],range=[range_var_xaxis,range_var_yaxis], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())\n",
    "    axs.hist2d(h[0],h[1])\n",
    "    #v1 = np.linspace(0, h[0].max(), 4, endpoint=True)\n",
    "    #cbar = fig.colorbar(h[3], ticks = v1 )\n",
    "    #cbar.set_ticks([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    #cbar.set_ticklabels([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    \n",
    "    #v1 = np.linspace(Z.min(), Z.max(), 8, endpoint=True)\n",
    "    #cbar=plt.colorbar(ticks=v1)              # the mystery step ???????????\n",
    "    #cbar.ax.set_yticklabels([ '0', '1784', '3568', '5353']) # add the labels\n",
    "    \n",
    "\n",
    "    \n",
    "    #plt.vlines(x=1.59,ymin=-1,ymax=2.4, color='r', linestyle='-')\n",
    "    #plt.hlines(y=bins4[1], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=bins4[2], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "\n",
    "    #plt.hlines(y=0.4, xmin=-0.1, xmax=df[var_xaxis].max(), colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.2, xmin=-0.1, xmax=1.5996, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.9, xmin=-0.1, xmax=3.5, colors='b', linestyles='solid', label='')\n",
    "    plt.xlabel('$y_{Lab}$', fontsize=20)\n",
    "    plt.ylabel('$p_{T}$ (GeV/$c$)', fontsize=18)\n",
    "    axs.text(0.02, 3, r'CBM Performance', fontsize=15)\n",
    "    axs.text(0.02, 2.8, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15, color ='r')\n",
    "    axs.text(1.2, 0.6, r'$y_{CM}$', fontsize=20, color ='r')\n",
    "    axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "    axs.grid(b=True, animated=True )\n",
    "    axs.set_xticks(np.arange(0,17)*0.2+0)\n",
    "    axs.set_xticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    axs.set_yticks(np.arange(0,16)*0.2+0)\n",
    "    axs.set_yticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    #plt.title(\"  y-$p_{T}$ plot for signal candidates (MC=1) with a cut = %.2f\"%0.95,  fontsize=18)\n",
    "    #plt.grid(which='both', ydata =yy)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity.png\")\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "h=plt.hist2d(df_clean_urqmd['rapidity'],df_clean_urqmd['pT'],range=[3,3], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pT_vs_rapidity(df_clean_urqmd,'rapidity','pT',3,3, 'issignal','xgb_preds', 0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base[df3_base['issignal']==2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ 97718      0]\n",
    " [186553      0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(32947+76208)/97718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "Thus in binary classification, the count of true positives is $C_{0,0}$, false positives is $C_{1,0}$, true negatives is $C_{1,1}$ and false negatives is $C_{0,1}$.\n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "cut3 = test_best\n",
    "df_clean['xgb_preds'] = ((df_clean['xgb_preds']>cut3)*1)\n",
    "cnf_matrix = confusion_matrix(df_clean['issignal'], df_clean['xgb_preds'], labels=[2,1,0])\n",
    "#cnf_matrix = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['secondaries','primaries','background'], title='Confusion Matrix for XGB for cut > '+str(cut3))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "df_cm = pd.DataFrame(cnf_matrix, ['secondaries','primaries','background'], ['secondaries','primaries','background'])\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion matrix, without normalization\n",
    "[[       0    37483     3232]\n",
    " [       0   101579     8955]\n",
    " [       0   437569 45809781]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cnf_matrix[[0]][0][1])/ (cnf_matrix[[0]][0][0]+cnf_matrix[[0]][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(bst,num_trees=2)\n",
    "plt.rcParams['figure.figsize'] = [80, 160]\n",
    "plt.rcParams['figure.dpi']=300\n",
    "#plt.show()\n",
    "plt.savefig(\"hists.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.to_graphviz(xg_reg, fmap='', num_trees=0, rankdir=None, yes_color=None, no_color=None, condition_node_params=None, leaf_node_params=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.08, 1.15)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(10,10), sharex=True, constrained_layout=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base['mass']),bins = 50,histtype='step', range=range1,Fill=False, color='red', facecolor='red', linewidth=2)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 50,histtype='step', Fill=False, range=range1,facecolor='blue',linewidth=2)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 18)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost','KFPF '), fontsize = 18, loc='upper right')\n",
    "\n",
    "\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "axs[1].hlines(y=1, xmin=1.112, xmax=1.12, colors='black', linestyles='dashed', label='')\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "err = np.std(ns)\n",
    "err1 = np.std(ns1)\n",
    "corr_ns_ns1 = np.corrcoef(ns,ns1)[[0],[1]][0]\n",
    "err_dif = (ns / ns1) * (np.sqrt( ((err/ns)**2) + ((err1/ns1)**2)\n",
    "                                      -2* ((corr_ns_ns1*err*err1)/(ns*ns1))))\n",
    "axs[1].errorbar(center,  ns / ns1,   fmt='o',\n",
    "                 c='Blue', label='Background in predictions')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 18)\n",
    "#axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "fig.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base[df3_base['issignal']>0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = plt.figure(constrained_layout=True)\n",
    "widths = [2, 3, 1.5]\n",
    "heights = [1, 3, 2]\n",
    "spec5 = fig5.add_gridspec(ncols=3, nrows=3, width_ratios=widths,\n",
    "                          height_ratios=heights)\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtest, dtrain, dtest1, df_scaled, x, y, x_whole, y_whole, x_train, x_test, y_train, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_100k = df_clean.copy()\n",
    "del df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bdt cut 0.7\n",
    "df0 = df3_base\n",
    "df0 = df0[(df0['mass']>1.07)&(df0['mass']<1.3)]\n",
    "df0 = df0[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bdt cut 0.8\n",
    "df1 = df3_base\n",
    "df1 = df1[(df1['mass']>1.07)&(df1['mass']<1.3)]\n",
    "df1 = df1[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bdt cut 0.9\n",
    "df2 = df3_base\n",
    "df2 = df2[(df2['mass']>1.07)&(df2['mass']<1.3)]\n",
    "df2 = df2[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.92\n",
    "df3 = df3_base\n",
    "df3 = df3[(df3['mass']>1.07)&(df3['mass']<1.3)]\n",
    "df3 = df3[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best\n",
    "df4 = df3_base\n",
    "df4 = df4[(df4['mass']>1.07)&(df4['mass']<1.3)]\n",
    "df4 = df4[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best\n",
    "df4_urqmd = df3_base\n",
    "df4_urqmd = df4_urqmd[(df4_urqmd['mass']>1.07)&(df4_urqmd['mass']<1.3)]\n",
    "df4_urqmd = df4_urqmd[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df4[df4['issignal']==1]\n",
    "#df4 = df4[['rapidity', 'mass', 'pT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyRoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, ROOT\n",
    "from ROOT import TF1, TCanvas,TMath, TColor\n",
    "\n",
    "class Linear:\n",
    "    def __call__( self, x, par ):\n",
    "        return par[0] + x[0]*par[1]\n",
    "\n",
    "class lorenztian:\n",
    "    def _call_(self, x, p):\n",
    "        return 0.5*p[0]*p[1] /( ((x[0]-p[2])**2) + ((0.5 * (p[1])**2))) \n",
    "\n",
    "class gaus:\n",
    "    def _call_(self, x ,p):\n",
    "        return p[0]*np.exp(-0.5*((x[0]-p[2])/p[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def truncate(number, decimals=2):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "\n",
    "def background_selector(df):\n",
    "    df1 = df[(df['mass']<1.108)]\n",
    "    df2 = df[df['mass']>1.13]\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    return df3['mass'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate(0.39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentile binning\n",
    "#df0 = df3_base\n",
    "#df = df0[(df0['mass']>1.07)&(df0['mass']<1.3)]\n",
    "df = df4\n",
    "out0, bins0 =pd.qcut(df['rapidity'], q=4, retbins=1)\n",
    "lowest_rapidity = df[df['rapidity']<bins0[1]]\n",
    "low_rapidity = df[(df['rapidity']>bins0[1])&(df['rapidity']<bins0[2])]\n",
    "mid_rapidity = df[(df['rapidity']>bins0[2])&(df['rapidity']<bins0[3])]\n",
    "high_rapidity = df[(df['rapidity']>bins0[3])]\n",
    "    \n",
    "out1, bins1 =pd.qcut(lowest_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_lowest_rapidity = lowest_rapidity[lowest_rapidity['pT']<bins1[1]]\n",
    "mid_pT_lowest_rapidity = lowest_rapidity[(lowest_rapidity['pT']>bins1[1]) & (lowest_rapidity['pT']<bins1[2])]\n",
    "high_pT_lowest_rapidity =lowest_rapidity[(lowest_rapidity['pT']>bins1[2])]\n",
    "\n",
    "out2, bins2 =pd.qcut(low_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_low_rapidity = low_rapidity[low_rapidity['pT']<bins2[1]]\n",
    "mid_pT_low_rapidity = low_rapidity[(low_rapidity['pT']>bins2[1]) & (low_rapidity['pT']<bins2[2])]\n",
    "high_pT_low_rapidity= low_rapidity[(low_rapidity['pT']>bins2[2])]\n",
    "    \n",
    "out3, bins3 =pd.qcut(mid_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_mid_rapidity = mid_rapidity[mid_rapidity['pT']<bins3[1]]\n",
    "mid_pT_mid_rapidity = mid_rapidity[(mid_rapidity['pT']>bins3[1]) & (mid_rapidity['pT']<bins3[2])]\n",
    "high_pT_mid_rapidity=mid_rapidity[(mid_rapidity['pT']>bins3[2])]\n",
    "    \n",
    "out4, bins4 =pd.qcut(high_rapidity['pT'], q=3, retbins=1)\n",
    "low_pT_high_rapidity = high_rapidity[high_rapidity['pT']<bins4[1]]\n",
    "mid_pT_high_rapidity = high_rapidity[(high_rapidity['pT']>bins4[1]) & (high_rapidity['pT']<bins4[2])]\n",
    "high_pT_high_rapidity=high_rapidity[(high_rapidity['pT']>bins4[2])]\n",
    "\n",
    "#del out0, lowest_rapidity, mid_rapidity, high_rapidity, out1, out2, out3, out4, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_selector(df):\n",
    "    df1 = df[(df['mass']<1.108)]\n",
    "    df2 = df[df['mass']>1.13]\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    return df3['mass'] \n",
    "\n",
    "list1 = [low_pT_lowest_rapidity, mid_pT_lowest_rapidity, high_pT_lowest_rapidity, low_pT_low_rapidity,\n",
    "         mid_pT_low_rapidity, high_pT_low_rapidity, low_pT_mid_rapidity, mid_pT_mid_rapidity,\n",
    "        high_pT_mid_rapidity, low_pT_high_rapidity, mid_pT_high_rapidity, high_pT_high_rapidity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4['mass'].describe()[1]-1.2*(df4['mass'].describe()[2])+0.2* (df4['mass'].describe()[2])\n",
    "df4['mass'].describe()[1]+1.2*(df4['mass'].describe()[2])+0.2* (df4['mass'].describe()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorentzian\n",
    "Lorenztian with second chebyshev 2nd order polynom\n",
    "\n",
    "The describe of BDT score > 70 shows that the sigma of the data mean is at 1.178052 with an std of 0.059818. So 1.55sigma below the mean is 1.0883250000000002 and 1.55 sigma above the mean is 1.267779. So let's choose 1.55, 1.5 and 1.45 below the mean and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol2 = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "pol3 = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "one_var_pol2=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "one_var_pol3=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lorentzian + second order pol\n",
    "lorentzian_pol2 = []\n",
    "pt_min=[]\n",
    "y_min = []\n",
    "#lorentzian_3rd_order_pol = []\n",
    "\n",
    "\n",
    "df = df4_urqmd\n",
    "\n",
    "\n",
    "mass_range_min = [df['mass'].describe()[1]-1.2*(df['mass'].describe()[2])]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "\n",
    "y_bin_low = -0.2\n",
    "y_bin_up =0.0\n",
    "for i in range(0,15,1):\n",
    "    y_bin_low = truncate(y_bin_low + 0.2)\n",
    "    y_bin_up = truncate(y_bin_up+0.2)\n",
    "    df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "    \n",
    "    pt_bin_low =-0.2\n",
    "    pt_bin_up =0\n",
    "    for i in range(0,15,1):\n",
    "        pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "        pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "        df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "        mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "        \n",
    "        for mm in mass_range_min:\n",
    "            for mmm in range(0,3,1):\n",
    "                \n",
    "                #canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "                #canvas.Draw()\n",
    "                #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "                binning = [40,70,100,130]\n",
    "                for b in binning:\n",
    "                    tot_sig_3_sigma = 0\n",
    "                    tot_bac_3_sigma = 0\n",
    "                    tot_sig_3_point_5_sigma = 0\n",
    "                    tot_bac_3_point_5_sigma = 0\n",
    "                    tot_sig_2_point_5_sigma = 0\n",
    "                    tot_bac_2_point_5_sigma = 0\n",
    "                    tot_sig_2_sigma = 0\n",
    "\n",
    "\n",
    "                    #step 0\n",
    "                    if df_pt.shape[0]>500:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0);\n",
    "                        #fb =TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"RIEMQN\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        h1.Fit(f1,\"RNIQ\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        #canvas .Clear ()\n",
    "                        #pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        #pad1 . Draw ()\n",
    "                        #pad1 . cd ()\n",
    "                        #pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetNpx(100000);\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3],par1[4]);\n",
    "                        #f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIRQ\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs.SetNpx(100000);\n",
    "                        #fs.SetLineColor(ROOT.kGreen)\n",
    "                        #fb.SetLineStyle(4)\n",
    "                        #fb.SetLineColor(ROOT.kBlue)\n",
    "                        #fb.SetNpx(100000);\n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5]);\n",
    "                        #fb.SetParameters(par2[3],par2[4],par2[5],par2[6]);\n",
    "\n",
    "\n",
    "                        #h1.SetTitleOffset(-1)\n",
    "                        #h1.SetFillStyle(3003);\n",
    "                        #h1.SetLineWidth(2)\n",
    "                        #h1.SetStats (0)\n",
    "                        #h1.SetYTitle(\"Entries\")\n",
    "                        #h1.SetLineColor(ROOT.kBlack)\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        #h3.SetLineWidth(2)\n",
    "                        #h3.SetStats (0)\n",
    "                        #h3.GetXaxis().SetTitle(\"Mass (GeV/c^2)\")\n",
    "\n",
    "                        #h1.Draw(\"pe\")\n",
    "                        #fs.Draw(\"SAME\")\n",
    "                        #fb.Draw(\"SAME\")\n",
    "                        #f2.Draw(\"SAME\")\n",
    "\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "                        #h2.Sumw2()\n",
    "\n",
    "                        integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "                        integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "                        binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "                        #tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                        #sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "\n",
    "                        signal_under_peak = (fs.Integral(integral_min,integral_max)/binwidth);\n",
    "                        if signal_under_peak>0:\n",
    "                            tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak                \n",
    "                        #sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "                        #man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "                        if sigma_signal_under_peak!=0:\n",
    "                            print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "                        \n",
    "                        backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "                        #if backgnd_under_peak<0:\n",
    "                            #print('Negative background')\n",
    "\n",
    "                        Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "\n",
    "                        signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_3_point_5_sigma>0:\n",
    "                            tot_sig_3_point_5_sigma= tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma                \n",
    "                        #tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_point_5_sigma>0:\n",
    "                            tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "                        #tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_sigm = (fs.Integral(par2[2] - (TMath.Abs(2*par2[1])),par2[2] + (TMath.Abs(2.*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_sigm>0:\n",
    "                            tot_sig_2_sigma = tot_sig_2_sigma+signal_under_peak_2_sigm\n",
    "\n",
    "                        #std = par2 [1]\n",
    "                        #estd = f2.GetParError(1)\n",
    "                        del h0, h1, h2, h3, f1, f2, fb, fs\n",
    "                        #latex = ROOT . TLatex ()\n",
    "                        #latex . SetNDC ()\n",
    "                        #latex . SetTextSize (0.02)\n",
    "                        #latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        #latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        #latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "                        #latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "\n",
    "                        #legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        #legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        #legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        #legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend . SetLineWidth (0)\n",
    "                        #legend.Draw()\n",
    "\n",
    "                        #canvas . cd ()\n",
    "                        #pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "                        #pad2 . Draw ()\n",
    "                        #pad2 . cd ()\n",
    "                        #pad2.Clear()\n",
    "\n",
    "\n",
    "                        #h3.SetLineColor(TColor.GetColor(5))\n",
    "                        #h3.SetYTitle(\"d-f/#Deltad\")\n",
    "                        #h3.Draw()\n",
    "                        #line = ROOT . TLine (mm,0 ,1.23 ,0)\n",
    "                        #line . SetLineColor ( ROOT . kRed )\n",
    "                        #line . SetLineWidth (2)\n",
    "                        #line . Draw (\" same \")\n",
    "\n",
    "\n",
    "                        #pad1 . SetBottomMargin (0)\n",
    "                        #pad2 . SetTopMargin (0)\n",
    "                        #pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "                        #h1 . GetXaxis (). SetLabelSize (0)\n",
    "                        #h1 . GetXaxis (). SetTitleSize (0)\n",
    "                        #h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "                        #h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "                        #h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "                        #h3 . SetTitle (\"\")\n",
    "                        #h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "                        #h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "                        #h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "                        #h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "                    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "                        #h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "                    #207,512 divisions\n",
    "                        #h3 . GetYaxis (). SetNdivisions (207)\n",
    "                        #h1 . GetYaxis (). SetRangeUser (0.5 ,3000)\n",
    "                        #h1 .GetYaxis().SetNdivisions(107)\n",
    "                        #h3 . GetXaxis (). SetNdivisions (207)\n",
    "                        gc.collect()\n",
    "                        #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "                    else:\n",
    "                        tot_sig_2_point_5_sigma=tot_sig_2_point_5_sigma+0\n",
    "                        tot_sig_3_sigma=tot_sig_3_sigma+0\n",
    "                        tot_sig_3_point_5_sigma=tot_sig_3_point_5_sigma+0\n",
    "                        #tot_sig_2_sigma = tot_sig_2_sigma+0\n",
    "            #lorentzian_pol2.append(tot_sig_2_sigma)\n",
    "                    lorentzian_pol2.append(tot_sig_2_point_5_sigma)\n",
    "                    lorentzian_pol2.append(tot_sig_3_sigma)\n",
    "                    lorentzian_pol2.append(tot_sig_3_point_5_sigma)\n",
    "                    pt_min.append(pt_bin_low+0.2)\n",
    "                    pt_min.append(pt_bin_low+0.2)\n",
    "                    pt_min.append(pt_bin_low+0.2)\n",
    "                    y_min.append(y_bin_low+0.2)\n",
    "                    y_min.append(y_bin_low+0.2)\n",
    "                    y_min.append(y_bin_low+0.2)\n",
    "                    \n",
    "            gc.collect()\n",
    "#canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")       \n",
    "print(y_bin_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total yield\n",
    "#lorentzian + second order pol\n",
    "lorentzian_pol2 = []\n",
    "#lorentzian_3rd_order_pol = []\n",
    "\n",
    "\n",
    "df = df4_urqmd\n",
    "\n",
    "\n",
    "mass_range_min = [df['mass'].describe()[1]-1.2*(df['mass'].describe()[2])]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2]),\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,3,1):\n",
    "        #canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "        #canvas.Draw()\n",
    "        #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "\n",
    "\n",
    "        binning = [70,100,130]\n",
    "        for b in binning:\n",
    "            tot_sig_3_sigma = 0\n",
    "            tot_bac_3_sigma = 0\n",
    "            tot_sig_3_point_5_sigma = 0\n",
    "            tot_bac_3_point_5_sigma = 0\n",
    "            tot_sig_2_point_5_sigma = 0\n",
    "            tot_bac_2_point_5_sigma = 0\n",
    "            tot_sig_2_sigma = 0\n",
    "            y_bin_low=-0.2\n",
    "            y_bin_up =0\n",
    "            for i in range(0,15,1):\n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0\n",
    "                for i in range(0,15,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "                    #step 0\n",
    "                    if df_pt.shape[0]>500:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0);\n",
    "                        #fb =TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"RIEMQ\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        h1.Fit(f1,\"RNIQ\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        #canvas .Clear ()\n",
    "                        #pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        #pad1 . Draw ()\n",
    "                        #pad1 . cd ()\n",
    "                        #pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetNpx(100000);\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3],par1[4]);\n",
    "                        #f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIRQ\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs.SetNpx(100000);\n",
    "                        #fs.SetLineColor(ROOT.kGreen)\n",
    "                        #fb.SetLineStyle(4)\n",
    "                        #fb.SetLineColor(ROOT.kBlue)\n",
    "                        #fb.SetNpx(100000);\n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5]);\n",
    "                        #fb.SetParameters(par2[3],par2[4],par2[5],par2[6]);\n",
    "\n",
    "\n",
    "                        #h1.SetTitleOffset(-1)\n",
    "                        #h1.SetFillStyle(3003);\n",
    "                        #h1.SetLineWidth(2)\n",
    "                        #h1.SetStats (0)\n",
    "                        #h1.SetYTitle(\"Entries\")\n",
    "                        #h1.SetLineColor(ROOT.kBlack)\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        #h3.SetLineWidth(2)\n",
    "                        #h3.SetStats (0)\n",
    "                        #h3.GetXaxis().SetTitle(\"Mass (GeV/c^2)\")\n",
    "\n",
    "                        #h1.Draw(\"pe\")\n",
    "                        #fs.Draw(\"SAME\")\n",
    "                        #fb.Draw(\"SAME\")\n",
    "                        #f2.Draw(\"SAME\")\n",
    "\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "                        #h2.Sumw2()\n",
    "\n",
    "                        integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "                        integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "                        binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "                        #tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                        #sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "\n",
    "                        signal_under_peak = (fs.Integral(integral_min,integral_max)/binwidth);\n",
    "                        if signal_under_peak>0:\n",
    "                            tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak                \n",
    "                        #sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "                        #man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "                        if sigma_signal_under_peak!=0:\n",
    "                            print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "                        \n",
    "                        backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "                        #if backgnd_under_peak<0:\n",
    "                            #print('Negative background')\n",
    "\n",
    "                        Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "\n",
    "                        signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_3_point_5_sigma>0:\n",
    "                            tot_sig_3_point_5_sigma= tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma                \n",
    "                        #tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_point_5_sigma>0:\n",
    "                            tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "                        #tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "                        #sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "                        #man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_sigm = (fs.Integral(par2[2] - (TMath.Abs(2*par2[1])),par2[2] + (TMath.Abs(2.*par2[1])))/binwidth);\n",
    "                        #bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        if signal_under_peak_2_sigm>0:\n",
    "                            tot_sig_2_sigma = tot_sig_2_sigma+signal_under_peak_2_sigm\n",
    "\n",
    "                        #std = par2 [1]\n",
    "                        #estd = f2.GetParError(1)\n",
    "                        del h0, h1, h2, h3, f1, f2, fb, fs\n",
    "                        #latex = ROOT . TLatex ()\n",
    "                        #latex . SetNDC ()\n",
    "                        #latex . SetTextSize (0.02)\n",
    "                        #latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        #latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        #latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        #latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "                        #latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "\n",
    "                        #legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        #legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        #legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        #legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                        #legend . SetLineWidth (0)\n",
    "                        #legend.Draw()\n",
    "\n",
    "                        #canvas . cd ()\n",
    "                        #pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "                        #pad2 . Draw ()\n",
    "                        #pad2 . cd ()\n",
    "                        #pad2.Clear()\n",
    "\n",
    "\n",
    "                        #h3.SetLineColor(TColor.GetColor(5))\n",
    "                        #h3.SetYTitle(\"d-f/#Deltad\")\n",
    "                        #h3.Draw()\n",
    "                        #line = ROOT . TLine (mm,0 ,1.23 ,0)\n",
    "                        #line . SetLineColor ( ROOT . kRed )\n",
    "                        #line . SetLineWidth (2)\n",
    "                        #line . Draw (\" same \")\n",
    "\n",
    "\n",
    "                        #pad1 . SetBottomMargin (0)\n",
    "                        #pad2 . SetTopMargin (0)\n",
    "                        #pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "                        #h1 . GetXaxis (). SetLabelSize (0)\n",
    "                        #h1 . GetXaxis (). SetTitleSize (0)\n",
    "                        #h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "                        #h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "                        #h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "                        #h3 . SetTitle (\"\")\n",
    "                        #h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "                        #h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "                        #h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "                        #h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "                    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "                        #h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "                    #207,512 divisions\n",
    "                        #h3 . GetYaxis (). SetNdivisions (207)\n",
    "                        #h1 . GetYaxis (). SetRangeUser (0.5 ,3000)\n",
    "                        #h1 .GetYaxis().SetNdivisions(107)\n",
    "                        #h3 . GetXaxis (). SetNdivisions (207)\n",
    "                        gc.collect()\n",
    "                        #canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "                    else:\n",
    "                        tot_sig_2_point_5_sigma=tot_sig_2_point_5_sigma+0\n",
    "                        tot_sig_3_sigma=tot_sig_3_sigma+0\n",
    "                        #tot_sig_3_point_5_sigma=tot_sig_3_point_5_sigma+0\n",
    "                        #tot_sig_2_sigma = tot_sig_2_sigma+0\n",
    "            #lorentzian_pol2.append(tot_sig_2_sigma)\n",
    "            lorentzian_pol2.append(tot_sig_2_point_5_sigma)\n",
    "            lorentzian_pol2.append(tot_sig_3_sigma)\n",
    "            #lorentzian_pol2.append(tot_sig_3_point_5_sigma)\n",
    "            gc.collect()\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lorentzian_pol2)\n",
    "#lorentzian_pol2\n",
    "#lorentzian_3rd_order_pol\n",
    "#lorentzian_pol2.mean()\n",
    "#15*15*3*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = 15*15\n",
    "binning = 4\n",
    "size = configurations*3*3*binning\n",
    "#yields = {'yields':np.zeros(size)}\n",
    "#df_yields = pd.DataFrame(yields, columns = ['yields'])\n",
    "#df_yields['yields']= lorentzian_pol2\n",
    "#df_yields['pt_min']= pt_min\n",
    "#df_yields['y_min']= y_min\n",
    "new_yy = df_yields[(df_yields['pt_min']>0.4) & (df_yields['pt_min']<0.8)]\n",
    "new_yy[(new_yy['y_min']>1) & (new_yy['y_min']<1.4) & (new_yy['yields']>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27,1):\n",
    "    df_yields['yields'].iloc[i+2*27] = lorentzian_pol2[i]\n",
    "    df_yields['yields'].iloc[i+3*27] = lorentzian_3rd_order_pol[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_new = df_yields[(df_yields['yields']<119844+3000)&(df_yields['yields']>119844-3000)]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_yields[df_yields['yields']>0]['yields'].hist(bins=20)\n",
    "#df_yields[df_yields['yields']>100000]['yields'].mean()\n",
    "plt.hist(lorentzian_pol2, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = 3\n",
    "size = configurations*3*3*3*2\n",
    "yields = {'yields':np.zeros(size)}\n",
    "df_yields = pd.DataFrame(yields, columns = ['yields'])\n",
    "df_yields['sigma']=np.zeros(size)\n",
    "df_yields['fit_lim']=np.zeros(size)\n",
    "df_yields['bins'] = np.zeros(size)\n",
    "df_yields['numbering'] = np.arange(0,size,1)\n",
    "df_yields['function'] = np.zeros(size)\n",
    "df_yields['BDT_cut'] = np.zeros(size)\n",
    "for i in range(0,27,1):\n",
    "    df_yields['function'].iloc[i] = 'lorentzian_pol2'\n",
    "    df_yields['function'].iloc[i+27] = 'lorentzian_pol3'\n",
    "    df_yields['yields'].iloc[i] = lorentzian_pol2[i]\n",
    "    df_yields['yields'].iloc[i+27] = lorentzian_3rd_order_pol[i]\n",
    "    df_yields['function'].iloc[i+2*27] = 'lorentzian_pol2'\n",
    "    df_yields['function'].iloc[i+3*27] = 'lorentzian_pol3'\n",
    "        \n",
    "for i in range(0,size,3):\n",
    "    df_yields['sigma'].iloc[i]  ='2.5 sigma'\n",
    "    df_yields['sigma'].iloc[i+1]='3 sigma'\n",
    "    df_yields['sigma'].iloc[i+2]='3.5 sigma'\n",
    "#for i in range(0,162,1):\n",
    "#    df_yields['yields'].iloc[162+i] = third_order_pol[i]\n",
    "\n",
    "for k in range(0,3,1):\n",
    "    for l in range (0,12,1):\n",
    "        df_yields['bins'] .iloc[k+l*9] = 70\n",
    "        df_yields['bins'] .iloc[k+3+l*9] = 100\n",
    "        df_yields['bins'] .iloc[k+6+l*9] = 130\n",
    "\n",
    "        \n",
    "for i in range(0,4,1):\n",
    "    for j in range(0,9,1):\n",
    "            df_yields['fit_lim'].iloc[i*27+j]=mass_range_min[0]+fit_limit_low[0]\n",
    "            df_yields['fit_lim'].iloc[i*27+9+j]=mass_range_min[0]+fit_limit_low[1]\n",
    "            df_yields['fit_lim'].iloc[i*27+18+j]=mass_range_min[0]+fit_limit_low[2]\n",
    "\n",
    "for i in range(0,2*27,1):\n",
    "    df_yields['BDT_cut'].iloc[i] = 'test_best'\n",
    "    df_yields['BDT_cut'].iloc[i+2*27] = '0.9'\n",
    "    df_yields['BDT_cut'].iloc[i+4*27] = '0.8'\n",
    "#for aj in range(0,int(size/2),1):\n",
    "#    df_yields['function'].iloc[aj]='Lorentzian plus 2nd order chebyshev'\n",
    "#    df_yields['function'].iloc[aj+int(size/2)]='Lorentzian plus 3rd order chebyshev'\n",
    "\n",
    "    \n",
    "    \n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "def yield_plot(variable1, variable2):\n",
    "    fig, axs = plt.subplots(figsize=(12,10))\n",
    "    bins1 = 19\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(variable1)))\n",
    "    axs.plot(variable1, variable2,label='', alpha =0.3)\n",
    "        #axs.set_ylabel('Starting Mass')   \n",
    "\n",
    "    #axs.legend(loc=(1.04,0.7), fontsize=13)\n",
    "\n",
    "    \n",
    "    \n",
    "yield_plot(df_yields['numbering'],df_yields['yields'])\n",
    "\n",
    "\n",
    "#df_yields[(df_yields['yields']>(df_yields['yields'].mean()-10)) & (df_yields['yields']<(df_yields['yields'].mean()+10))]\n",
    "df_yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lorentzian 3rd order pol\n",
    "lorentzian_3rd_order_pol = []\n",
    "\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"EM\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",low_limit,upper_limit);\n",
    "                        #f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2], par[3]);\n",
    "                        h1.Fit(f1,\"RNI\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        canvas .Clear ()\n",
    "                        pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        pad1 . Draw ()\n",
    "                        pad1 . cd ()\n",
    "                        pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",low_limit,upper_limit)\n",
    "                        #f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetNpx(100000);\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3], par1[4]);\n",
    "                        f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",low_limit,upper_limit);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                        legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}+Ex^{3}\",\"l\");\n",
    "                        legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        legend.AddEntry(fb,\"B+Cx+Dx^{2}+Ex^{3}\",\"l\");\n",
    "                        legend . SetLineWidth (0)\n",
    "                        legend.Draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaussian + second order pol\n",
    "gaussian_2nd = []\n",
    "\n",
    "                        fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fb.SetParameters(0,0,0);\n",
    "                        h0.Fit(fb,\"EM\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",low_limit,upper_limit);\n",
    "                        #f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        h1.Fit(f1,\"RNI\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        canvas .Clear ()\n",
    "                        pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        pad1 . Draw ()\n",
    "                        pad1 . cd ()\n",
    "                        pad1. Clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",low_limit,upper_limit)\n",
    "                        #f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetNpx(100000);\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",low_limit,upper_limit);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                         latex . SetTextSize (0.02)\n",
    "                        latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                        latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                        legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd order chebyshev back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_order_pol = []\n",
    "\n",
    "                fb = TF1(\"fb\",\"[0]*x*x*x-[1]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0);\n",
    "                h0.Fit(fb,\"RNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]*x*x*x-[2]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "\n",
    "                f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ 0.25*[1]*[1]) +[3]*x*x*x-[4]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+Bx^{}-Cx\",\"l\");\n",
    "                legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"Bx^{3}-Cx\",\"l\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd order normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_pol = []\n",
    "\n",
    "                fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0,0);\n",
    "                h0.Fit(fb,\"EM\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1],par[2]);\n",
    "                h1.Fit(f1,\"EM\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "                f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.0001,1.115,par1[1],par1[2],par1[3]);\n",
    "\n",
    "                fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "\n",
    "                latex . SetTextSize (0.02)\n",
    "                latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h0, canvas, h1, h2, h3, pad1, pad2, f1,f2, fs, fb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                fb = TF1(\"fb\",\"[0]+[1]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0);\n",
    "                h0.Fit(fb,\"RNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1]);\n",
    "                h1.Fit(f1,\"RNI\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    " \n",
    "                f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.0001,1.115,par1[1],par1[2]);\n",
    "                f2.SetLineColor(ROOT.kRed)\n",
    "                h1.Fit(f2,\"MNIR\");\n",
    "                par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                fs.SetLineColor(ROOT.kGreen)\n",
    "                    latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx\",\"l\");\n",
    "                legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx\",\"l\");\n",
    "                legend . SetLineWidth (0)\n",
    "                legend.Draw()\n",
    "\n",
    "                canvas . cd ()\n",
    "                pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian with second order pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_2nd_order_pol = []\n",
    "mass_range_min = [1.0853341]\n",
    "fit_limit_low=[0,0.05*0.059818,0.1*0.059818,1.2707699000000001,1.2707699000000001+(0.05*0.059818),1.2707699000000001 +(0.1*0.059818)]\n",
    "                fb = TF1(\"fb\",\"[0]+[1]*x+[2]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0,0);\n",
    "                h0.Fit(fb,\"LRNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x+[3]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1], par[2]);\n",
    "                h1.Fit(f1,\"LRNI\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "\n",
    "                f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x+[5]*x*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.001,1.115,par1[1],par1[2], par1[3]);\n",
    "                f2.SetLineColor(ROOT.kRed)\n",
    "                h1.Fit(f2,\"LMNIR\");\n",
    "                par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                  bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                legend.AddEntry(fs,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                legend . SetLineWidth (0)\n",
    "                legend.Draw()\n",
    "\n",
    "                canvas . cd ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian with linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_linear = []\n",
    "\n",
    "                fb = TF1(\"fb\",\"[0]+[1]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fb.SetParameters(0,0);\n",
    "                h0.Fit(fb,\"LRNIFCWW\");\n",
    "                par = fb.GetParameters()\n",
    "\n",
    "            #Step 1\n",
    "                data = distribution['mass']\n",
    "        #the minimum x (lower edge of the first bin)=mm        \n",
    "                h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(distribution['rapidity'].min(),distribution['rapidity'].max(),distribution['pT'].min(),distribution['pT'].max(), mm, bins),bins,mm,1.23)\n",
    "                for i in range(0,data.shape[0]):\n",
    "                    h1.Fill(data.iloc[i])\n",
    "                f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2)+[1]+[2]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                f1.SetParameters(1,par[0],par[1]);\n",
    "                h1.Fit(f1,\"LRNI\");\n",
    "                par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "                f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2)+[3]+[4]*x\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3])\n",
    "                f2.SetNpx(100000);\n",
    "                f2.SetParameters(par1[0],0.001,1.115,par1[1],par1[2]);\n",
    "                f2.SetLineColor(ROOT.kRed)\n",
    "                h1.Fit(f2,\"LMNIR\");\n",
    "                par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "                h1.Draw(\"pe\")\n",
    "                fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",mm+fit_limit_low[mmm],fit_limit_low[mmm+3]);\n",
    "                fs.SetNpx(100000);\n",
    "                  latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "                latex . DrawLatex (0.4 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "                legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                legend.AddEntry(f2,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}+B+Cx\",\"l\");\n",
    "                legend.AddEntry(fs,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}\",\"l\");\n",
    "                legend.AddEntry(fb,\"B+Cx\",\"l\");\n",
    "                legend . SetLineWidth (0)\n",
    "                legend.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigma_signal_under_peak\n",
    "sigma_integral\n",
    "#sigma_backgnd_under_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(h[0].min(), h[0].max(), 4, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pT_vs_rapidity(df, var_xaxis , var_yaxis , range_var_xaxis, range_var_yaxis):\n",
    "    import matplotlib as mpl\n",
    "    fig, axs = plt.subplots(figsize=(8, 6),dpi = 300)\n",
    "    h=plt.hist2d(df[var_xaxis],df[var_yaxis],range=[range_var_xaxis,range_var_yaxis], bins=np.arange(0,17)*0.2+0, norm=mpl.colors.LogNorm())\n",
    "    v1 = np.linspace(0, h[0].max(), 4, endpoint=True)\n",
    "    cbar = fig.colorbar(h[3], ticks = v1 )\n",
    "    #cbar.set_ticks([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    #cbar.set_ticklabels([h[0].min(),(h[0].max()-h[0].min())/2,h[0].max()])\n",
    "    \n",
    "    #v1 = np.linspace(Z.min(), Z.max(), 8, endpoint=True)\n",
    "    #cbar=plt.colorbar(ticks=v1)              # the mystery step ???????????\n",
    "    cbar.ax.set_yticklabels([ '0', '1784', '3568', '5353']) # add the labels\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.vlines(x=1.59,ymin=-1,ymax=2.4, color='r', linestyle='-')\n",
    "    #plt.hlines(y=bins4[1], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=bins4[2], xmin=bins0[3], xmax=3.162, colors='b', linestyles='solid', label='')\n",
    "\n",
    "    #plt.hlines(y=0.4, xmin=-0.1, xmax=df[var_xaxis].max(), colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.2, xmin=-0.1, xmax=1.5996, colors='b', linestyles='solid', label='')\n",
    "    #plt.hlines(y=0.9, xmin=-0.1, xmax=3.5, colors='b', linestyles='solid', label='')\n",
    "    plt.xlabel('$y_{Lab}$', fontsize=20)\n",
    "    plt.ylabel('$p_{T}$ (GeV/$c$)', fontsize=18)\n",
    "    axs.text(0.02, 3, r'CBM Performance', fontsize=15)\n",
    "    axs.text(0.02, 2.8, r'DCM-QGSM-SMM, Au+Au @ 12 $A$GeV/$c$', fontsize=15, color ='r')\n",
    "    axs.text(1.2, 0.6, r'$y_{CM}$', fontsize=20, color ='r')\n",
    "    axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "    axs.grid(b=True, animated=True )\n",
    "    axs.set_xticks(np.arange(0,17)*0.2+0)\n",
    "    axs.set_xticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    axs.set_yticks(np.arange(0,16)*0.2+0)\n",
    "    axs.set_yticklabels(['0' ,'' ,'' ,'0.6','','', '1.2','','', '1.8','' ,'' ,'2.4','','' ,'3' , ''])\n",
    "    #plt.title(\"  y-$p_{T}$ plot for signal candidates (MC=1) with a cut = %.2f\"%0.95,  fontsize=18)\n",
    "    #plt.grid(which='both', ydata =yy)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity.png\")\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range1=[-0., 3.2]\n",
    "range2=[-0.01, 3.]\n",
    "\n",
    "h =pT_vs_rapidity(df4[df4['issignal']==1],'rapidity','pT', range1, range2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_rapidity_cut = df3_base[(df3_base['rapidity']<2) & (df3_base['rapidity']>0.8) &(df3_base['pT']>0.15)\n",
    "                           &(df3_base['pT']<0.9)]\n",
    "pT_vs_rapidity(pt_rapidity_cut,'rapidity','pT', range1, range2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pt_below_mid_rapidity_cut = df3_base[(df3_base['rapidity']<1.5996) & (df3_base['pT']>0.4)]\n",
    "pT_vs_rapidity(df3_base,'rapidity','pT', range1, range2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del  h1, h2 ,h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyf_tf1_params(x, p):\n",
    "    return p[0] * x[0] + p[1]\n",
    "\n",
    "npars = 2\n",
    "f = ROOT.TF1(\"tf1_params\", pyf_tf1_params, 0.0, 1.0, npars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenztian( x ,p):\n",
    "    return 0.5*p[0]*p[1] /( ((x[0]-p[2])**2) + ((0.5 * p[1])**2)) \n",
    "\n",
    "def gaus_fit( x ,p):\n",
    "    return p[0]*np.exp(-0.5*((x[0]-p[2])/p[1])**2)\n",
    "\n",
    "\n",
    "f2 = ROOT.TF1 (\" gaussfit\", \"[0]*exp(-0.5*((x-[2])/[1])^2)\"  ,1.1 ,1.13)\n",
    "\n",
    "#def lorenztian( x ,p):\n",
    "#    return p[0]*2*np.sqrt(2)*p[1]*p[2]*np.sqrt(p[2]*(p[2]**2 + p[1]**2)) /(np.pi*np.sqrt(p[2]+np.sqrt(p[2]*(p[2]**2 + p[1]**2)))) /( ((x[0]**2) - (p[2]**2))**2 +(p[1]*p[2])**2 )\n",
    "mm= 1.105\n",
    "bins =100\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "distribution = mid_pT_high_rapidity\n",
    "data = distribution['mass']\n",
    "#the minimum x (lower edge of the first bin)=mm        \n",
    "h1 = ROOT.TH1F(\"B_&_S\",\"\", bins,mm,1.13)\n",
    "for i in range(0,data.shape[0]):\n",
    "    h1.Fill(data.iloc[i])\n",
    "f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014)\",mm,1.13);\n",
    "#f1.SetParameters(1);\n",
    "h1.Fit(f1,\"RNI\");\n",
    "par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "#Step2\n",
    "canvas .Clear ()\n",
    "pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "pad1 . Draw ()\n",
    "pad1 . cd ()\n",
    "pad1. Clear()\n",
    "\n",
    "\n",
    "h1.SetTitleOffset(-1)\n",
    "h1.SetFillStyle(3003);\n",
    "h1.SetLineWidth(2)\n",
    "h1.SetStats (0)\n",
    "h1.SetYTitle(\"Entries\")\n",
    "h1.SetLineColor(ROOT.kBlack)\n",
    "h2 = ROOT.TH1F(\"h2\", \"\", bins, mm, 1.13);\n",
    "h3 = ROOT.TH1F(\"h2\", \"\", bins, mm, 1.13);\n",
    "h3.SetLineWidth(2)\n",
    "h3.SetStats (0)\n",
    "h3.GetXaxis().SetTitle(\"Mass (GeV/c^2)\")\n",
    "\n",
    "f2 = TF1(\"full\",lorenztian,mm,1.13,3);\n",
    "f2.SetNpx(100000);\n",
    "f2.SetParameters(par1[0],0.0001,1.115);\n",
    "f2.SetLineColor(ROOT.kRed)\n",
    "h1.Fit(f2,\"E\");\n",
    "par2 = f2.GetParameters()\n",
    "\n",
    "\n",
    "h1.Draw(\"pe\")\n",
    "\n",
    "f2.Draw(\"SAME\")\n",
    "\n",
    "bin1 = h1.FindBin(mm);\n",
    "bin2 = h1.FindBin(1.13);\n",
    "for i in range(bin1,bin2):\n",
    "    f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "    t_value = h1.GetBinContent(i)\n",
    "    h2.SetBinContent(i,f_value)\n",
    "    if (h1.GetBinError(i) > 0):\n",
    "        h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "h2.Sumw2()\n",
    "\n",
    "#To integrate over the gaussian peak we take the integral limits 3 sigmas (i.e. parameter 3) below the mean value\n",
    "#(i.e. par 1) of the gaussian as a minimum limit and 3 sigmas above the mean as a max limit of the integral*/\n",
    "integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "#To integrate area under the signal plus background curve we take 3 sigma and integrate\n",
    "binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "#To find the signal, we integrate just the gaussian peak with 3 sigma \n",
    "signal_under_peak = (fs.Integral(integral_min,integral_max)/binwidth);\n",
    "sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "if sigma_signal_under_peak!=0:\n",
    "    print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak\n",
    "#Background\n",
    "backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "sigma_backgnd_under_peak = fb.IntegralError(integral_min,integral_max);\n",
    "tot_bac_3_sigma = tot_bac_3_sigma+backgnd_under_peak\n",
    "#Significance = signal/(signal+background)^0.5\n",
    "#Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "\n",
    "#3.5 sigma\n",
    "signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "tot_sig_3_point_5_sigma = tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma\n",
    "tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "std =  f2.GetParameter(1)\n",
    "estd = f2.GetParError(1)\n",
    "\n",
    "latex = ROOT . TLatex ()\n",
    "latex . SetNDC ()\n",
    "latex . SetTextSize (0.02)\n",
    "#latex . DrawLatex (0.4 ,0.85, \"Significance in 2.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "#latex . DrawLatex (0.4 ,0.80, \"Significance in 3#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "#latex . DrawLatex (0.4 ,0.75, \"Significance in 3.5#sigma region around peak = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "latex . DrawLatex (0.2 ,0.75, \" #Gamma = %.4f #pm %.5f GeV\"%(std,estd ))\n",
    "latex . DrawLatex (0.2 ,0.70, \" m_{0} = %.4f #pm %.5f GeV\"%(par2 [2],f2.GetParError(2) ))\n",
    "#latex . DrawLatex (0.2 ,0.65,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "\n",
    "\n",
    "legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "legend . SetLineWidth (0)\n",
    "legend.Draw()\n",
    "\n",
    "canvas . cd ()\n",
    "pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "pad2 . Draw ()\n",
    "pad2 . cd ()\n",
    "pad2.Clear()\n",
    "\n",
    "\n",
    "h3.SetLineColor(TColor.GetColor(5))\n",
    "h3.SetYTitle(\"d-f/#Deltad\")\n",
    "h3.Draw()\n",
    "line = ROOT . TLine (mm,0 ,1.125 ,0)\n",
    "line . SetLineColor ( ROOT . kRed )\n",
    "line . SetLineWidth (2)\n",
    "line . Draw (\" same \")\n",
    "\n",
    "\n",
    "pad1 . SetBottomMargin (0)\n",
    "pad2 . SetTopMargin (0)\n",
    "pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "h1 . GetXaxis (). SetLabelSize (0)\n",
    "h1 . GetXaxis (). SetTitleSize (0)\n",
    "h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "h3 . SetTitle (\"\")\n",
    "h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "h1 . GetXaxis (). SetRangeUser (1. ,1.126)\n",
    "h3 . GetXaxis (). SetRangeUser (1.11 ,1.122)\n",
    "#ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "#207,512 divisions\n",
    "h3 . GetYaxis (). SetNdivisions (207)\n",
    "h1 . GetYaxis (). SetRangeUser (0.5 ,1000)\n",
    "h1 .GetYaxis().SetNdivisions(107)\n",
    "h3 . GetXaxis (). SetNdivisions (207)\n",
    "\n",
    "canvas.Update()\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.png\")\n",
    "\n",
    "\n",
    "\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "#canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sgnal[sgnal['issignal']==1]\n",
    "lowest_rapidity = df[df['rapidity']<0.5]\n",
    "low_rapidity = df[(df['rapidity']>0.5)   & (df['rapidity']<1)]\n",
    "mid_rapidity = df[(df['rapidity']>1.5)   & (df['rapidity']<2)]\n",
    "high_rapidity = df[(df['rapidity']>2)    & (df['rapidity']<2.5)]\n",
    "higher_rapidity = df[(df['rapidity']>2.5)]\n",
    "\n",
    "    \n",
    "\n",
    "low_pT_lowest_rapidity = lowest_rapidity[lowest_rapidity['pT']<1]\n",
    "mid_pT_lowest_rapidity = lowest_rapidity[(lowest_rapidity['pT']>1) & (lowest_rapidity['pT']<2)]\n",
    "high_pT_lowest_rapidity =lowest_rapidity[(lowest_rapidity['pT']>2)]\n",
    "\n",
    "\n",
    "low_pT_low_rapidity = low_rapidity[low_rapidity['pT']<1]\n",
    "mid_pT_low_rapidity = low_rapidity[(low_rapidity['pT']>1) & (low_rapidity['pT']<2)]\n",
    "high_pT_low_rapidity= low_rapidity[(low_rapidity['pT']>2)]\n",
    "    \n",
    "\n",
    "low_pT_mid_rapidity = mid_rapidity[mid_rapidity['pT']<1]\n",
    "mid_pT_mid_rapidity = mid_rapidity[(mid_rapidity['pT']>1) & (mid_rapidity['pT']<2)]\n",
    "high_pT_mid_rapidity=mid_rapidity[(mid_rapidity['pT']>2)]\n",
    "    \n",
    "\n",
    "low_pT_high_rapidity = high_rapidity[high_rapidity['pT']<1]\n",
    "mid_pT_high_rapidity = high_rapidity[(high_rapidity['pT']>1) & (high_rapidity['pT']<2)]\n",
    "high_pT_high_rapidity=high_rapidity[(high_rapidity['pT']>2)]\n",
    "\n",
    "low_pT_higher_rapidity = higher_rapidity[higher_rapidity['pT']<1]\n",
    "mid_pT_higher_rapidity = higher_rapidity[(higher_rapidity['pT']>1) & (higher_rapidity['pT']<2)]\n",
    "high_pT_higher_rapidity=higher_rapidity[(higher_rapidity['pT']>2)]\n",
    "\n",
    "\n",
    "del  lowest_rapidity, mid_rapidity, high_rapidity, higher_rapidity, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [low_pT_lowest_rapidity, mid_pT_lowest_rapidity, high_pT_lowest_rapidity, low_pT_low_rapidity,\n",
    "         mid_pT_low_rapidity, high_pT_low_rapidity, low_pT_mid_rapidity, mid_pT_mid_rapidity,\n",
    "        high_pT_mid_rapidity, low_pT_high_rapidity, mid_pT_high_rapidity, high_pT_high_rapidity, \n",
    "         low_pT_higher_rapidity, mid_pT_higher_rapidity, high_pT_higher_rapidity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal[signal['issignal']==1]['rapidity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['mass'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('pt_y_yield_bdt_cut_0.95.root','recreate')\n",
    "t = TTree('t1','tree with df')\n",
    "\n",
    "\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'issignal/F')\n",
    "\n",
    "for i in range(len(df4_urqmd['mass'])):\n",
    "    rapidity[0] = df4_urqmd['rapidity'].iloc[i]\n",
    "    mass[0] = df4_urqmd['mass'].iloc[i]\n",
    "    pT[0] = df4_urqmd['pT'].iloc[i]\n",
    "    issignal[0] = df4_urqmd['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('pt_y_yield_bdt_cut_0.95.root','recreate')\n",
    "t = TTree('t1','tree with df')\n",
    "\n",
    "\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'pT/F')\n",
    "\n",
    "for i in range(len(df3_base['mass'])):\n",
    "    mass[0] = df3_base['mass'].iloc[i]\n",
    "    pT[0] = df4_urqmd['pT'].iloc[i]\n",
    "    issignal[0] = df4_urqmd['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[new['issignal']>2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[new['issignal']==1].shape\n",
    "new[new['issignal']==2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Root trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best way\n",
    "import uproot\n",
    "import awkward as ak\n",
    "file = uproot.recreate(\"signal_urqmd_uproot.root\")\n",
    "file[\"t1\"] = dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, x_train, y_train, df_scaled, dtest, dtrain, dtest1, dtest2, x_whole, x_test, y_test, x_whole_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "cut = 0.6\n",
    "df3 = df_clean[df_clean['xgb_preds1']>=cut]\n",
    "df3 = df3[df3['issignal']>0]\n",
    "df3 = df3[['pT', 'rapidity', 'mass','issignal','xgb_preds1']]\n",
    "df3.columns.values[[0,1,2,3,4]] = ['MCpT', 'MCrapidity','MCmass', 'MCissignal','MCxgb_preds']\n",
    "df3[\"MCissignal\"]=df3[\"MCissignal\"].astype(\"float\")\n",
    "df3[\"MCxgb_preds\"]=df3[\"MCxgb_preds\"].astype(\"float\")\n",
    "del df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3_base = df_clean_urqmd[df_clean_urqmd['xgb_preds1']>=cut]\n",
    "df3_base3 = df3_base[['pT', 'rapidity', 'mass', 'issignal','xgb_preds1']]\n",
    "df3_base3[\"issignal\"]=df3_base3[\"issignal\"].astype(\"double\")\n",
    "df3_base3[\"xgb_preds\"]=df3_base3[\"xgb_preds\"].astype(\"double\")\n",
    "del df_clean_urqmd, df3_base\n",
    "import uproot\n",
    "import awkward as ak\n",
    "file = uproot.recreate(\"new_c3_pt_y_y_yield_bdt_cut_0.8.root\")\n",
    "file[\"t1\"] = df3_base3\n",
    "file[\"t2\"] = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_base3['xgb_preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(7)\n",
    "df3 = uproot.open(\"new_c3_pt_y_y_yield_bdt_cut_0.8.root:t2\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "df3 = df3[df3['MCxgb_preds']>0.9]\n",
    "urqmd = uproot.open(\"new_c3_pt_y_y_yield_bdt_cut_0.8.root:t1\").arrays(library='pd',decompression_executor=executor,\n",
    "                                  interpretation_executor=executor)\n",
    "urqmd = urqmd[(urqmd['xgb_preds']>0.9)&(urqmd['issignal']>0)]\n",
    "\n",
    "h2d_dcm = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3)\n",
    "for i in range(0,len( df3['MCrapidity'])):\n",
    "    h2d_dcm.Fill( df3['MCrapidity'].iloc[i],df3['MCpT'].iloc[i])\n",
    "h2d_dcm1=h2d_dcm.Clone()\n",
    "type(h2d_dcm)\n",
    "\n",
    "h2d_urqmd = ROOT.TH2F(\"urqmd\", \"urqmd\", 15,0,3,15,0,3)\n",
    "for i in range(0,len(urqmd['rapidity'])):\n",
    "    h2d_urqmd.Fill(urqmd['rapidity'].iloc[i],urqmd['pT'].iloc[i])\n",
    "type(h2d_urqmd)\n",
    "h2d_urqmd1=h2d_urqmd.Clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot on\n",
    "file = TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/dcm/m_320_340_dcm_125.root\")\n",
    "MC_DCM=file.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "file1 = ROOT.TFile(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/urqmd/m_320_340_urqmd_125.root\")\n",
    "MC_URQMD=file1.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\")\n",
    "c1=ROOT.TCanvas(\"\")\n",
    "c1.Draw()\n",
    "ratio = h2d_dcm.Divide(MC_DCM)\n",
    "h2d_urqmd.Divide(h2d_dcm)\n",
    "h2d_urqmd.Divide(MC_URQMD)\n",
    "ROOT.gStyle.SetPaintTextFormat(\"4.2f\");\n",
    "h2d_urqmd.Draw(\"colz\")\n",
    "h2d_urqmd.Draw(\"TEXT SAME\");\n",
    "h2d_urqmd.SetStats(0)\n",
    "c1.Print(\"hists1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(0,25,1)\n",
    "y = np.arange(0,25,1)\n",
    "value = np.arange(0,25,1)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2d = ROOT.TH2F(\"\", \"M\", 5,0,3,5,0,3)\n",
    "for i in np.arange(0,25,1):\n",
    "    h2d.Fill(x,y,value)\n",
    "        \n",
    "\n",
    "c1=ROOT.TCanvas(\"\")\n",
    "c1.Draw()\n",
    "h2d.Draw('colz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('dcm_prim_100k_cleaned.root','recreate')\n",
    "t = TTree('PlainTree','tree with df')\n",
    "\n",
    "chi2geo = array('f',[0])\n",
    "chi2primneg = array('f',[0])\n",
    "chi2primpos = array('f',[0])\n",
    "distance = array('f',[0])\n",
    "ldl = array('f',[0])\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('chi2geo', chi2geo,'chi2geo/F')\n",
    "t.Branch('chi2primneg', chi2primneg,'chi2primneg/F')\n",
    "t.Branch('chi2primpos', chi2primpos,'chi2primpos/F')\n",
    "t.Branch('distance', distance,'distance/F')\n",
    "t.Branch('ldl', ldl,'y/F')\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'issignal/F')\n",
    "\n",
    "for i in range(len(new['mass'])):\n",
    "    chi2geo[0] = new['chi2geo'].iloc[i]\n",
    "    chi2primneg[0] = new['chi2primneg'].iloc[i]\n",
    "    chi2primpos[0] = new['chi2primpos'].iloc[i]\n",
    "    distance[0] = new['distance'].iloc[i]\n",
    "    ldl[0] = new['ldl'].iloc[i]    \n",
    "    rapidity[0] = new['rapidity'].iloc[i]\n",
    "    mass[0] = new['mass'].iloc[i]\n",
    "    pT[0] = new['pT'].iloc[i]\n",
    "    issignal[0] = new['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r urqmd_100k_cleaned.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_filling(new, t):\n",
    "    for i in range(len(new['mass'])):\n",
    "        chi2geo[0] = new['chi2geo'].iloc[i]\n",
    "        chi2primneg[0] = new['chi2primneg'].iloc[i]\n",
    "        chi2primpos[0] = new['chi2primpos'].iloc[i]\n",
    "        distance[0] = new['distance'].iloc[i]\n",
    "        ldl[0] = new['ldl'].iloc[i]    \n",
    "        rapidity[0] = new['rapidity'].iloc[i]\n",
    "        mass[0] = new['mass'].iloc[i]\n",
    "        pT[0] = new['pT'].iloc[i]\n",
    "        issignal[0] = new['issignal'].iloc[i]\n",
    "        t.Fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "f = TFile('urqmd_100k_cleaned.root','recreate')\n",
    "t = TTree('PlainTree','tree with df')\n",
    "\n",
    "chi2geo = array('f',[0])\n",
    "chi2primneg = array('f',[0])\n",
    "chi2primpos = array('f',[0])\n",
    "distance = array('f',[0])\n",
    "ldl = array('f',[0])\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('chi2geo', chi2geo,'chi2geo/F')\n",
    "t.Branch('chi2primneg', chi2primneg,'chi2primneg/F')\n",
    "t.Branch('chi2primpos', chi2primpos,'chi2primpos/F')\n",
    "t.Branch('distance', distance,'distance/F')\n",
    "t.Branch('ldl', ldl,'y/F')\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'pT/F')\n",
    "\n",
    "for i in range(len(new['mass'])):\n",
    "    chi2geo[0] = new['chi2geo'].iloc[i]\n",
    "    chi2primneg[0] = new['chi2primneg'].iloc[i]\n",
    "    chi2primpos[0] = new['chi2primpos'].iloc[i]\n",
    "    distance[0] = new['distance'].iloc[i]\n",
    "    ldl[0] = new['ldl'].iloc[i]    \n",
    "    rapidity[0] = new['rapidity'].iloc[i]\n",
    "    mass[0] = new['mass'].iloc[i]\n",
    "    pT[0] = new['pT'].iloc[i]\n",
    "    issignal[0] = new['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_maker(new,'urqmd_100k_cleaned.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_importer import tree_importer\n",
    "dcm = tree_importer('dcm_1m_prim_signal.root','PlainTree',7)\n",
    "del dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[(df4['rapidity']>1.4)& (df4['pT']>1.4)]['pT'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot off\n",
    "#lorentzian + second order pol\n",
    "a = []\n",
    "pt_y_bin_for_yield_min=[]\n",
    "pt_y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_min=[]\n",
    "true_mc_in_recons =[]\n",
    "\n",
    "\n",
    "\n",
    "df = df4_urqmd\n",
    "\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,1,1):\n",
    "        canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "        canvas.Draw()\n",
    "\n",
    "        binning = [100]\n",
    "        for b in binning:\n",
    "\n",
    "            y_bin_low=1\n",
    "            y_bin_up =1.2\n",
    "            for i in range(0,1,1):\n",
    "                \n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0.\n",
    "                for i in range(0,1,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    #print(pt_bin_low)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']==1].shape[0]\n",
    "                    #step 0\n",
    "                    if df_pt.shape[0]>400:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"pol2\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fb =TF1(\"fb\",\"[0]+[1]*x+[2]*x*x+[3]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #fb.SetParameters(0,0,0);\n",
    "                        #fb.SetParameters(0,0,0,0);\n",
    "                        h0.Fit(fb,\"RIEM\");\n",
    "                        par = fb.GetParameters()\n",
    "                        #Step 1\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        h1.Fit(f1,\"RNI\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "                        canvas .Clear ()\n",
    "                        pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "                        pad1 . Draw ()\n",
    "                        pad1 . cd ()\n",
    "                        pad1. Clear()\n",
    "\n",
    "                #step 2\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3], par1[4]);\n",
    "                        f2.SetNpx(100000);\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        f2.SetLineColor(ROOT.kRed)\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fs.SetNpx(100000);\n",
    "                        fs.SetLineColor(ROOT.kGreen)\n",
    "                        fb.SetLineStyle(4)\n",
    "                        fb.SetLineColor(ROOT.kBlue)\n",
    "                        fb.SetNpx(100000);\n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5], par2[6]);\n",
    "\n",
    "\n",
    "                        h1.SetTitleOffset(0)\n",
    "                        h1.SetFillStyle(3003);\n",
    "                        h1.SetLineWidth(2)\n",
    "                        h1.SetStats (0)\n",
    "                        h1.SetYTitle(\"Entries\")\n",
    "                        h1.SetLineColor(ROOT.kBlack)\n",
    "                        h1.GetYaxis().SetTitle(\"Counts\")\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h4 =  ROOT.TH1F(\"h2\", \"\", b, mm, 1.23)\n",
    "                        h5 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3.SetLineWidth(2)\n",
    "                        h3.SetStats (0)\n",
    "                        h3.GetXaxis().SetTitle(\"Mass (GeV/#it{c}^{2}]\")\n",
    "\n",
    "                        h1.Draw(\"pe\")\n",
    "                        h_mc.SetLineColor(ROOT.kMagenta)\n",
    "                        h_mc.SetLineWidth(2)\n",
    "                        #h_mc.Draw(\"SAMEpe\")\n",
    "                        fs.Draw(\"SAME\")\n",
    "                        fb.Draw(\"SAME\")\n",
    "                        f2.Draw(\"SAME\")\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            fs_values = fs.Eval(h3.GetBinCenter(i))\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            t_value_mc = h_mc.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            h4.SetBinContent(i,fs_values)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "                            if (h_mc.GetBinError(i) > 0):\n",
    "                                h5.SetBinContent(i,(t_value_mc-fs_values)/h_mc.GetBinError(i))\n",
    "\n",
    "\n",
    "                        h2.Sumw2()\n",
    "                        #h4.Sumw2()\n",
    "                        h5.SetLineColor(ROOT.kBlue)\n",
    "                        h5.SetLineWidth(2)\n",
    "\n",
    "                        integral_min = par2[2] - (TMath.Abs(3*par2[1]));\n",
    "                        integral_max = par2[2] + (TMath.Abs(3*par2[1]));\n",
    "\n",
    "                        binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "                        tot = f2.Integral(integral_min,integral_max)/binwidth;\n",
    "                        sigma_integral = f2.IntegralError(integral_min,integral_max);\n",
    "                        #signal_under_peak = par2[0] * np.sqrt(2*3.1415*par2[1]*par2[1])/binwidth\n",
    "                        signal_under_peak = fs.Integral(integral_min,integral_max)/binwidth\n",
    "                        if signal_under_peak<0:\n",
    "                            signal_under_peak = 0\n",
    "                            print('Negative signal')                \n",
    "                        sigma_signal_under_peak = fs.IntegralError(integral_min,integral_max);\n",
    "                        man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "                        if sigma_signal_under_peak!=0:\n",
    "                            print(\"Integral errors \",sigma_signal_under_peak)\n",
    "\n",
    "                        tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak\n",
    "                    #Background\n",
    "                        backgnd_under_peak = (fb.Integral(integral_min,integral_max)/binwidth)\n",
    "                        if backgnd_under_peak<0:\n",
    "                            print('Negative background')\n",
    "                        sigma_backgnd_under_peak = fb.IntegralError(integral_min,integral_max);\n",
    "                        tot_bac_3_sigma = tot_bac_3_sigma+backgnd_under_peak\n",
    "                    #Significance = signal/(signal+background)^0.5\n",
    "                        Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "                        #print(\"total - background = \",tot-backgnd_under_peak)\n",
    "                        #3.5 sigma\n",
    "                        signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "                        tot_sig_3_point_5_sigma = tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma\n",
    "                        tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "                        sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "                        man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "                        signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "                        tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "                        tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "                        sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "                        man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "\n",
    "                        std = par2 [1]\n",
    "                        estd = f2.GetParError(1)\n",
    "                        \n",
    "                        latex = ROOT . TLatex ()\n",
    "                        latex . SetNDC ()\n",
    "                        latex . SetTextSize (0.02)\n",
    "                        latex . DrawLatex (0.4 ,0.85, \"Significance in m_{0} #pm 2.5#Gamma  = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm 3#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "                        latex . DrawLatex (0.4 ,0.75, \"Significance in m_{0} #pm 3.5#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "                        latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(par2 [1],f2.GetParError(1) ))\n",
    "                        latex . DrawLatex (0.4 ,0.65, \" m_{0} = %.4f #pm %.5f GeV\"%(par2 [2],f2.GetParError(2) ))\n",
    "                        latex . DrawLatex (0.4 ,0.6,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "                        latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "                        \n",
    "                        latex1 = ROOT . TLatex ()\n",
    "                        latex1 . SetNDC ()\n",
    "                        latex1 . SetTextSize (0.035)\n",
    "                        latex1. DrawLatex (0.4 ,0.25, \"CBM performance\")\n",
    "                        latex1. DrawLatex (0.4 ,0.15, \"URQMD, Au+Au @ 12#it{A} GeV/#it{c}\")\n",
    "                        latex1.Draw()\n",
    "\n",
    "                        \n",
    "                        legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "                        legend.AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "                        legend.AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "                        legend.AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "                        legend.AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "                        legend . SetLineWidth (0)\n",
    "                        legend.Draw()\n",
    "\n",
    "                        canvas . cd ()\n",
    "                        pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "                        pad2 . Draw ()\n",
    "                        pad2 . cd ()\n",
    "                        pad2.Clear()\n",
    "\n",
    "\n",
    "                        h3.SetLineColor(TColor.GetColor(5))\n",
    "                        h3.SetYTitle(\"d-f/#Deltad\")\n",
    "                        #h5.Draw()\n",
    "                        h3.Draw(\"SAME\")\n",
    "                        line = ROOT . TLine (mm,0 ,1.2 ,0)\n",
    "                        line . SetLineColor ( ROOT . kRed )\n",
    "                        line . SetLineWidth (2)\n",
    "                        line . Draw (\" same \")\n",
    "\n",
    "\n",
    "                        pad1 . SetBottomMargin (0)\n",
    "                        pad2 . SetTopMargin (0)\n",
    "                        pad2 . SetBottomMargin (0.25)\n",
    "\n",
    "                        h1 . GetXaxis (). SetLabelSize (0)\n",
    "                        #h1.SetTitle(\"\")\n",
    "                        h1 . GetXaxis (). SetTitleSize (0)\n",
    "                        h1 . GetYaxis (). SetTitleSize (0.05)\n",
    "                        h1 . GetYaxis (). SetLabelSize (0.03)\n",
    "                        h1 . GetYaxis (). SetTitleOffset (0.6)\n",
    "\n",
    "                        h3 . SetTitle (\"\")\n",
    "                        h3 . GetXaxis (). SetLabelSize (0.12)\n",
    "                        h3 . GetXaxis (). SetTitleSize (0.12)\n",
    "                        h3 . GetYaxis (). SetLabelSize (0.1)\n",
    "                        h3 . GetYaxis (). SetTitleSize (0.15)\n",
    "                    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "                        h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "                    #207,512 divisions\n",
    "                        h3 . GetYaxis (). SetNdivisions (207)\n",
    "                        #h_mc.GetYaxis (). SetRangeUser (0.5 ,700)\n",
    "                        h1 . GetYaxis (). SetRangeUser (0.5 ,400)\n",
    "                        h3 . GetXaxis (). SetRangeUser (1.08 ,1.2)\n",
    "                        h1 . GetXaxis (). SetRangeUser (1.08 ,1.2)\n",
    "                        h1 .GetYaxis().SetNdivisions(107)\n",
    "                        h3 . GetXaxis (). SetNdivisions (207)\n",
    "\n",
    "                        canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/c.png\")\n",
    "\n",
    "            #a.append(tot_sig_2_point_5_sigma)\n",
    "                        a.append(signal_under_peak)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low+0.2))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up+0.2))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "                    else:\n",
    "                        a.append(0)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low+0.2))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up+0.2))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "            #a.append(tot_sig_3_point_5_sigma)\n",
    "#canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency \n",
    "Efficieny correction on just one configuration i.e lorenztian + 2nd order pol, 100 mass binings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h1_set(h1):\n",
    "    h1 . SetTitleOffset(-1)\n",
    "    h1 . SetFillStyle(3003);\n",
    "    h1 . SetLineWidth(3)\n",
    "    h1 . SetStats (0)\n",
    "    h1 . SetYTitle(\"Counts\")\n",
    "    h1 . SetMarkerStyle(8)\n",
    "    #h1 . SetMarkerSize(2)\n",
    "    h1 . SetLineColor (ROOT . kBlack)\n",
    "    h1 . GetXaxis () . SetLabelSize (0)\n",
    "    h1 . GetXaxis () . SetTitleSize (0)\n",
    "    h1 . GetYaxis () . SetTitleSize (0.05)\n",
    "    h1 . GetYaxis() . CenterTitle();\n",
    "    h1 . GetYaxis () . SetLabelSize (0.05)\n",
    "    h1 . GetYaxis () . SetTitleOffset (1)\n",
    "    h1 . GetYaxis () . SetNdivisions(107)\n",
    "    #h1 . GetYaxis () . SetRangeUser(0.8,100)\n",
    "    #h1 . GetXaxis () . SetRangeUser(1.105,1.13)\n",
    "    #h1 . SetTitle (\"\")\n",
    "    return h1\n",
    "\n",
    "\n",
    "def h3_set(h3):   \n",
    "    h3 . SetLineWidth(3)\n",
    "    h3 . SetStats (0)\n",
    "    h3 . GetXaxis() . SetTitle(\"Mass [GeV/c^{2}]\")\n",
    "    h3 . SetTitle (\"\")\n",
    "    h3 . GetXaxis () . SetLabelSize (0.15)\n",
    "    h3 . GetXaxis() . CenterTitle();\n",
    "    h3 . GetXaxis () . SetTitleSize (0.15)\n",
    "    h3 . GetYaxis () . SetLabelSize (0.15)\n",
    "    h3 . GetYaxis () . SetTitleSize (0.15)\n",
    "    h3 . GetXaxis () . SetTitleOffset (1.1)\n",
    "    h3 . GetYaxis () . SetTitleOffset (0.4)\n",
    "    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "    #207,512 divisions\n",
    "    h3 . GetYaxis (). SetNdivisions (207)\n",
    "    h3.SetLineColor(TColor.GetColor(5))\n",
    "    h3.SetYTitle(\"d-f/#Deltad\")\n",
    "    h3 . GetXaxis () . SetRangeUser(1.105,1.13)\n",
    "    return h3\n",
    "\n",
    "\n",
    "def f_set(ftot, fs, fb):\n",
    "    ftot.SetNpx(100000);\n",
    "    ftot.SetLineColor(ROOT.kRed)\n",
    "    ftot.SetLineWidth(4)\n",
    "    \n",
    "    fs.SetLineStyle(4)\n",
    "    fs.SetNpx(100);\n",
    "    fs.SetLineColor(ROOT.kGreen)\n",
    "    fs.SetLineWidth(4)\n",
    "    fs.SetMarkerSize(2)\n",
    "    \n",
    "    fb.SetLineStyle(3)\n",
    "    fb.SetLineColor(ROOT.kBlue)\n",
    "    fb.SetNpx(100);\n",
    "    fb.SetLineWidth(4)\n",
    "    return ftot, fs, fb\n",
    "\n",
    "\n",
    "def draw_line():\n",
    "    line = ROOT . TLine (1.104,0 ,1.131 ,0)\n",
    "    line . SetLineColor ( ROOT . kRed )\n",
    "    line . SetLineWidth (2)\n",
    "    return line\n",
    "\n",
    "\n",
    "def draw_latex():\n",
    "    latex = ROOT . TLatex ()\n",
    "    latex . SetNDC ()\n",
    "    latex . SetTextSize (0.02)\n",
    "    #latex . DrawLatex (0.4 ,0.85, \"Significance in m_{0} #pm 2.5#Gamma  = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "    #latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm 3#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "    #latex . DrawLatex (0.4 ,0.75, \"Significance in m_{0} #pm 3.5#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "    #latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(par2 [1],f2.GetParError(1) ))\n",
    "    #latex . DrawLatex (0.4 ,0.85, \"Significance in m_{0} #pm 2.5#sigma  = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "    #latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm 3#sigma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,Significance ))\n",
    "    #latex . DrawLatex (0.4 ,0.75, \"Significance in m_{0} #pm 3.5#sigma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "    latex . DrawLatex (0.17 ,0.80, \"#scale[1.5]{#sigma = %.4f #pm %.5f GeV}\"%(par2 [1],f2.GetParError(1) ))\n",
    "    latex . DrawLatex (0.17 ,0.75, \"#scale[1.5]{#mu = %.4f #pm %.5f GeV}\"%(par2 [2],f2.GetParError(2) ))\n",
    "    #latex . DrawLatex (0.4 ,0.6,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "    #latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "    latex . DrawLatex (0.55 ,0.85, \"#scale[1.7]{CBM performance}\")\n",
    "    latex . DrawLatex (0.65 ,0.8, \"#scale[1.7]{URQMD, Au+Au}\")\n",
    "    latex . DrawLatex (0.65 ,0.75, \"#scale[1.7]{@ 12#it{A} GeV/#it{c}}\")\n",
    "    \n",
    "    latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm %.1f#sigma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(sigma,signal_under_peak,man_sigma_signal_under_peak, signal_under_peak,backgnd_under_peak,signal_under_peak/TMath.Sqrt(backgnd_under_peak+signal_under_peak) ))\n",
    "\n",
    "\n",
    "    return latex\n",
    "    \n",
    "    \n",
    "def draw_legend():\n",
    "    legend = ROOT.TLegend(0.85,0.22,0.58,0.7);\n",
    "    legend . AddEntry(h1,\"  #Lambda hyperons\",\"ep,X0\");\n",
    "    #legend . AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "    #legend . AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "    legend.AddEntry(f2,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}+B+Cx+Dx^{2}\",\"l\");\n",
    "    legend.AddEntry(fs,\"Ae^{#frac{-1}{2} #frac{(x-#mu)^{2}}{#sigma^{2}}}\",\"l\");\n",
    "    legend . AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "    legend . SetLineWidth (0)\n",
    "    legend . SetTextSize(0.038)\n",
    "    return legend\n",
    "\n",
    "def createCanvasPads():\n",
    "    c = ROOT . TCanvas (\" canvas \",\"\", 500,600)\n",
    "    \n",
    "    pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "    pad1 . SetBottomMargin (0)\n",
    "    pad1 . SetLeftMargin (0.15)\n",
    "    #pad1 . SetLogy()\n",
    "    pad1 . Draw ()\n",
    "    \n",
    "    pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "    pad2 . SetGrid()\n",
    "    pad2 . SetTopMargin (0)\n",
    "    pad2 . SetBottomMargin (0.5)\n",
    "    pad2 . SetLeftMargin (0.15)\n",
    "    pad2 . Draw ()\n",
    "    return c, pad1, pad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hist(h1, f2, fs, fb, h3):     \n",
    "\n",
    "    ROOT . gStyle . SetFillColor(-1);\n",
    "    ROOT . gStyle . SetFillStyle(4000);\n",
    "    ROOT . gStyle . SetLegendBorderSize(0);\n",
    "    ROOT . gStyle . SetFrameBorderSize(0);\n",
    "    ROOT . gStyle . SetFrameFillColor(-1);\n",
    "    c, pad1, pad2 = createCanvasPads ()\n",
    "    c . Draw ()\n",
    "    pad1 . cd ()\n",
    "    \n",
    "    h1 = h1_set (h1)\n",
    "    f2, fs, fb = f_set (f2, fs, fb)\n",
    "    \n",
    "    h1 . Draw(\"pe,X0\")\n",
    "    fb . Draw(\"SAME\")\n",
    "    f2 . Draw(\"SAME\")\n",
    "    fs . Draw(\"SAME\")\n",
    "    draw_latex()\n",
    "    legend = draw_legend ()\n",
    "    legend . Draw()\n",
    "    \n",
    "    c . cd ()\n",
    "    pad2 . cd ()\n",
    "\n",
    "    h3_set(h3) . Draw()\n",
    "    \n",
    "    \n",
    "    line = draw_line ()\n",
    "    line . Draw (\" same \")\n",
    "    c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "    \n",
    "    #c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_cal(h, f_tot, fs, fb, sigma):\n",
    "    tot_sig_sigma, tot_bac_sigma = 0, 0   \n",
    "    \n",
    "    binwidth = h.GetXaxis().GetBinWidth(1);\n",
    "    tot = f_tot.Integral(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])))/binwidth;\n",
    "    sigma_integral = f_tot.IntegralError(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])));\n",
    "    #params.integral = fit->GetParameter(0) * sqrt(2*3.1415) * fit->GetParameter(2) / h->GetBinWidth(1);\n",
    "    #signal_under_peak = par2[1] * np.sqrt(2*3.1415) *3 *par2[2]/ binwidth\n",
    "    signal_under_peak = fs.Integral(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])))/binwidth\n",
    "               \n",
    "    sigma_signal_under_peak = fs.IntegralError(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])));\n",
    "    man_sigma_signal_under_peak = TMath.Sqrt(signal_under_peak)\n",
    "\n",
    "\n",
    "    tot_sig_sigma= tot_sig_sigma+signal_under_peak\n",
    "#Background\n",
    "    backgnd_under_peak = (fb.Integral(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])))/binwidth)\n",
    "\n",
    "    sigma_backgnd_under_peak = fb.IntegralError(par2[2] - (TMath.Abs(sigma*par2[1])),par2[2] + (TMath.Abs(sigma*par2[1])));\n",
    "    tot_bac_sigma = tot_bac_sigma+backgnd_under_peak\n",
    "#Significance = signal/(signal+background)^0.5\n",
    "    Significance = signal_under_peak/TMath.Sqrt(tot);\n",
    "  \n",
    "    return tot, signal_under_peak, man_sigma_signal_under_peak,tot_sig_sigma, backgnd_under_peak, Significance, sigma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as ss\n",
    "ss.erf(3/np.sqrt(2))\n",
    "#(2/np.pi)* np.arctan(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "pt_y_bin_for_yield_min=[]\n",
    "pt_y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_min=[]\n",
    "true_mc_in_recons =[]\n",
    "red_chi = []\n",
    "\n",
    "df = df_clean[df_clean['xgb_preds']>0.8]\n",
    "\n",
    "mass_range_min = [1.08]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               1.23,\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "\n",
    "\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,1,1):\n",
    "\n",
    "        binning = [150]\n",
    "        for b in binning:\n",
    "\n",
    "            y_bin_low=-0.2\n",
    "            y_bin_up =0\n",
    "            for i in range(0,15,1):\n",
    "                tot_sig_3_point_5_sigma, tot_sig_3_sigma, tot_sig_2_point_5_sigma, tot_sig_2_sigma = 0, 0, 0, 0\n",
    "                tot_bac_3_sigma, tot_bac_3_point_5_sigma, tot_bac_2_point_5_sigma = 0, 0, 0\n",
    "                \n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0\n",
    "                \n",
    "                for i in range(0,15,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    #print(pt_bin_low)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "                    #print(y_bin_low, y_bin_up, \" pT \", pt_bin_low,pt_bin_up)\n",
    "                    if df_pt.shape[0]>200:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"pol2\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        h0.Fit(fb,\"RMN\");\n",
    "                        par = fb.GetParameters()\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        #f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #f1=TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x+[4]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        #f1.SetParameters(1,par[0], par[1], par[2],par[3]);\n",
    "                        f1 = TF1(\"step1\",\"[0]*exp(-0.5*((x-1.115683)/0.0014)^2) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        h1.Fit(f1,\"RN\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x+[6]*x*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3], par1[4]);\n",
    "                        #f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);\n",
    "                        f2 = TF1(\"full\",\"[0]*exp(-0.5*((x-[2])/[1])^2) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1],par1[2], par1[3])\n",
    "                        \n",
    "\n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        #fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        #fs = TF1(\"fs\",\"[0]*exp(-0.5*((x-[2])/[1])^2)\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        \n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5], par2[6]);\n",
    "\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h3\", \"\", b, mm, 1.23);\n",
    "\n",
    "\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "                        h2.Sumw2()\n",
    "\n",
    "                        tot, signal_under_peak, man_sigma_signal_under_peak,tot_sig_sigma, backgnd_under_peak, Significance, sigma= signal_cal(h1, f2, fs, fb,3)\n",
    "\n",
    "                        \n",
    "                        draw_hist(h1, f2, fs, fb, h3)\n",
    "                        \n",
    "                        \n",
    "            #a.append(tot_sig_2_point_5_sigma)\n",
    "                        if f2.GetNDF()!= 0:\n",
    "                            red_chi.append(f2.GetChisquare() / f2.GetNDF() )\n",
    "                        else:\n",
    "                            red_chi.append(0)\n",
    "                        #a.append(signal_under_peak_2_sigma)\n",
    "                        #a.append(signal_under_peak_2_point_5_sigma)\n",
    "                        a.append(signal_under_peak)\n",
    "                        #a.append(signal_under_peak_3_point_5_sigma)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "                    else:\n",
    "                        a.append(0)\n",
    "                        red_chi.append(0)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_clean_mc = true_mc_in_recons\n",
    "#len(dcm_clean_mc)\n",
    "#sum(true_mc_in_recons)\n",
    "len(dcm_clean_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)\n",
    "#mc_counts\n",
    "#sum(a)\n",
    "#sum(true_mc_in_recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt[df_pt['issignal']==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "file =uproot.open(\"lambda_qa_dcm.root\")\n",
    "array1 = file[\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\"].to_numpy()\n",
    "#for i in range(0,14,1):\n",
    "array1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URQMD\n",
    "size = 15*15\n",
    "pt_y_yields = pd.DataFrame(data=np.arange(0,size,1),columns = ['numbering'])\n",
    "pt_y_yields['rapidity_min_MC'] = np.zeros(size)\n",
    "pt_y_yields['pT_min_MC'] = np.zeros(size)\n",
    "\n",
    "pt_y_yields['ratio_recons_sim']=np.zeros(size)\n",
    "pt_y_yields['ratio_recons_mc']=np.zeros(size)\n",
    "pt_y_yields['pT_min'] = np.zeros(size)\n",
    "pt_y_yields ['pt_y_yields_MC']=np.zeros(size)\n",
    "pt_y_yields['pt_y_yields_recons']=a\n",
    "pt_y_yields['true_mc_in_recons'] = true_mc_in_recons\n",
    "#pt_y_yields['total_mc_in_recons'] = dcm_clean_mc\n",
    "\n",
    "for i in range(0,15):\n",
    "    for j in range(0,15):\n",
    "        pt_y_yields['rapidity_min_MC'].iloc[i+j*15]=0+j*0.2\n",
    "    \n",
    "\n",
    "for i in range(0,15):    \n",
    "    pt_y_yields['pT_min_MC'].iloc[i]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+1*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+2*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+3*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+4*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+5*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+6*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+7*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+8*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+9*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+10*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+11*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+12*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+13*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+14*15]=i/5\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(0,15,1):\n",
    "    pt_y_yields ['pt_y_yields_MC'].iloc[i]=array1[0][0][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+1*15]=array1[0][1][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+2*15]=array1[0][2][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+3*15]=array1[0][3][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+4*15]=array1[0][4][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+5*15]=array1[0][5][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+6*15]=array1[0][6][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+7*15]=array1[0][7][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+8*15]=array1[0][8][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+9*15]=array1[0][9][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+10*15]=array1[0][10][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+11*15]=array1[0][11][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+12*15]=array1[0][12][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+13*15]=array1[0][13][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+14*15]=array1[0][14][i]\n",
    "\n",
    "for i in range(0,15*15,1):\n",
    "    pt_y_yields['ratio_recons_mc'].iloc[i]=a[i]/pt_y_yields['true_mc_in_recons'].iloc[i]\n",
    "    pt_y_yields['ratio_recons_sim'].iloc[i]=a[i]/pt_y_yields['pt_y_yields_MC'].iloc[i]\n",
    "    pt_y_yields['pT_min'].iloc[i] = pt_y_bin_for_yield_min[i]\n",
    "    #print(\"%.2f\"%pt_y_yields['rapidity_min_MC'].iloc[i],\"       \",pt_y_yields['pT_min_MC'].iloc[i],\"    \", pt_y_yields['ratio'].iloc[i] )\n",
    "#plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Reconstructed/Sim')\n",
    "plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_mc'], label='Rencostructed/MC')\n",
    "plt.legend()\n",
    "plt.ylim([0.9,1.1])\n",
    "plt.savefig(\"hists\")\n",
    "#pt_y_yields[(pt_y_yields['rapidity_min_MC']>1) & (pt_y_yields['rapidity_min_MC']<1.4) &(pt_y_yields['pT_min_MC']<1)&(pt_y_yields['pT_min_MC']>0)]\n",
    "pt_y_yields[(pt_y_yields['numbering']>20) & (pt_y_yields['numbering']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcm\n",
    "size = 15*15\n",
    "pt_y_yields = pd.DataFrame(data=np.arange(0,size,1),columns = ['numbering'])\n",
    "pt_y_yields['rapidity_min_MC'] = np.zeros(size)\n",
    "pt_y_yields['pT_min_MC'] = np.zeros(size)\n",
    "\n",
    "pt_y_yields['ratio_recons_sim']=np.zeros(size)\n",
    "pt_y_yields['ratio_recons_mc']=np.zeros(size)\n",
    "pt_y_yields['pT_min'] = np.zeros(size)\n",
    "pt_y_yields ['pt_y_yields_MC']=np.zeros(size)\n",
    "pt_y_yields['pt_y_yields_recons']=dcm_clean_mc\n",
    "pt_y_yields['true_mc_in_recons'] = true_mc_in_recons\n",
    "#pt_y_yields['total_mc_in_recons'] = dcm_clean_mc\n",
    "\n",
    "for i in range(0,15):\n",
    "    for j in range(0,15):\n",
    "        pt_y_yields['rapidity_min_MC'].iloc[i+j*15]=0+j*0.2\n",
    "    \n",
    "\n",
    "for i in range(0,15):    \n",
    "    pt_y_yields['pT_min_MC'].iloc[i]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+1*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+2*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+3*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+4*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+5*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+6*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+7*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+8*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+9*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+10*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+11*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+12*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+13*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+14*15]=i/5\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(0,15,1):\n",
    "    pt_y_yields ['pt_y_yields_MC'].iloc[i]=array1[0][0][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+1*15]=array1[0][1][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+2*15]=array1[0][2][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+3*15]=array1[0][3][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+4*15]=array1[0][4][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+5*15]=array1[0][5][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+6*15]=array1[0][6][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+7*15]=array1[0][7][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+8*15]=array1[0][8][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+9*15]=array1[0][9][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+10*15]=array1[0][10][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+11*15]=array1[0][11][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+12*15]=array1[0][12][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+13*15]=array1[0][13][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+14*15]=array1[0][14][i]\n",
    "\n",
    "for i in range(0,15*15,1):\n",
    "#    pt_y_yields['ratio_recons_mc'].iloc[i]=dcm_clean_mc[i]/pt_y_yields['true_mc_in_recons'].iloc[i]\n",
    "    pt_y_yields['ratio_recons_sim'].iloc[i]=dcm_clean_mc[i]/pt_y_yields['pt_y_yields_MC'].iloc[i]\n",
    "#    pt_y_yields['pT_min'].iloc[i] = pt_y_bin_for_yield_min[i]\n",
    "    #print(\"%.2f\"%pt_y_yields['rapidity_min_MC'].iloc[i],\"       \",pt_y_yields['pT_min_MC'].iloc[i],\"    \", pt_y_yields['ratio'].iloc[i] )\n",
    "#plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Reconstructed/Sim')\n",
    "plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_mc'], label='Rencostructed/MC')\n",
    "plt.legend()\n",
    "plt.ylim([0.9,1.1])\n",
    "plt.savefig(\"hists\")\n",
    "#pt_y_yields[(pt_y_yields['rapidity_min_MC']>1) & (pt_y_yields['rapidity_min_MC']<1.4) &(pt_y_yields['pT_min_MC']<1)&(pt_y_yields['pT_min_MC']>0)]\n",
    "pt_y_yields[(pt_y_yields['numbering']>20) & (pt_y_yields['numbering']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3);\n",
    "h6 = ROOT.TH2F(\"Mc in reconstructed\", \"Mc in reconstructed\", 15,0,3,15,0,3);\n",
    "h7 = ROOT.TH2F(\"DCM Efficiency\", \"DCM Efficiency\", 15,0,3,15,0,3);\n",
    "#h8 = ROOT.TH2F(\"total mc in reconstructed\", \"total mc in reconstructed\", 15,0,3,15,0,3);\n",
    "\n",
    "\n",
    "h4.SetStats(0)\n",
    "h5.SetStats(0)\n",
    "h6.SetStats(0)\n",
    "\n",
    "c = ROOT . TCanvas (\" canvas \",\"\", 950,800)\n",
    "c.Draw()\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h6.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h7.SetBinContent(y_bin, pT_bin, a[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, pt_y_yields['total_mc_in_recons'].iloc[i]);\n",
    "\n",
    "c.SetGrid()\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h8)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h4.Divide(h5)\n",
    "h4.Draw('colz')\n",
    "\n",
    "h4.GetZaxis().SetRangeUser (0 ,0.4)\n",
    "h4.SetTitleOffset(-1)\n",
    "latex = ROOT . TLatex ()\n",
    "latex . SetNDC ()\n",
    "latex . SetTextSize (0.039)\n",
    "#latex . DrawLatex (0.4 ,0.7, \"#Lambda_{Reconstructed} / #Lambda_{MC} =  %.f / %.f = %.3f\"%(sum(a),df4[df4['issignal']>0].shape[0], sum(a) / (df4[df4['issignal']>0].shape[0])))\n",
    "latex . DrawLatex (0.12 ,0.68, \"ML algorithm Efficiency = #Lambda_{Reconstructed} / #Lambda_{Reconstructable} \" )\n",
    "\n",
    "latex.Draw()\n",
    "\n",
    "latex1 = ROOT . TLatex ()\n",
    "latex1 . SetNDC ()\n",
    "latex1 . SetTextSize (0.035)\n",
    "latex1. DrawLatex (0.12 ,0.84, \"CBM performance\")\n",
    "latex1. DrawLatex (0.12 ,0.76, \"DCM-QGSM-SMM, Au+Au @ 12#it{A} GeV/#it{c}\")\n",
    "#latex1 . DrawLatex (0.45 ,.61, \"= %.f / %.f = %.3f\"%(sum(a),sum(pt_y_yields ['total_mc_in_recons']), sum(a) / (sum(pt_y_yields ['total_mc_in_recons']))))\n",
    "latex1.Draw()\n",
    "\n",
    "\n",
    "h4 . SetTitle (\"\")\n",
    "h4 .GetXaxis().SetTitle(\"#it{y}_{Lab}\")\n",
    "h4. GetXaxis().SetTitleSize(0.06)\n",
    "h4 .GetXaxis().SetTitleOffset(0.7)\n",
    "h4 .GetXaxis().SetLabelSize(0.05)\n",
    "h4 .GetYaxis().SetTitle(\"p_{T} (GeV/#it{c})\")\n",
    "h4. GetYaxis().SetTitleSize(0.06)\n",
    "h4 .GetYaxis().SetTitleOffset(0.7)\n",
    "h4 .GetYaxis().SetLabelSize(0.05)\n",
    "h4 .GetZaxis().SetLabelSize(0.05)\n",
    "\n",
    "c.SetRightMargin(0.13);\n",
    "c. Update()\n",
    "c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "f = TFile('xgb_vs_kfpf.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h1 = ROOT.TH1D(\"xgb_recons_urqmd\",\"xgb_recons_urqmd\",200,1.08,1.2)\n",
    "h2 = ROOT.TH1D(\"KFPF_urqmd\", \"kfpf_urqmd\", 200,1.08,1.2);\n",
    "h3 = ROOT.TH1D(\"ratio\", \"ratio\", 200,1.08,1.2);\n",
    "h1.SetStats(0)\n",
    "h2.SetStats(0)\n",
    "h3.SetStats(0)\n",
    "\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "\n",
    "for i in range(0,df3_base.shape[0]):\n",
    "    h1.Fill(df3_base['mass'].iloc[i])\n",
    "    h3.Fill(df3_base['mass'].iloc[i])\n",
    "for i in range(0,new_check_set.shape[0]):\n",
    "    h2.Fill(new_check_set['mass'].iloc[i])\n",
    "\n",
    "canvas.SetGrid()\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h1.Divide(h2)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "h3.Divide(h2)\n",
    "h1.Draw()\n",
    "h2.Draw(\"same\")\n",
    "\n",
    "\n",
    "\n",
    "h11.SetTitleOffset(-1)\n",
    "\n",
    "\n",
    "h1 . SetTitle (\"\")\n",
    "h1 .GetXaxis().SetTitle(\"mass\")\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h8 = ROOT.TH2F(\"recons_urqmd\", \"recons_urqmd\", 15,0,3,15,0,3);\n",
    "h9 = ROOT.TH2F(\"Mc_urqmd\", \"Mc_urqmd\", 15,0,3,15,0,3);\n",
    "h10 = ROOT.TH2F(\"Mc in reconstructed_urqmd\", \"Mc in reconstructed_urqmd\", 15,0,3,15,0,3);\n",
    "h11 = ROOT.TH2F(\"urqmd_Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h11.SetStats(0)\n",
    "h9.SetStats(0)\n",
    "h10.SetStats(0)\n",
    "\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "bin1 = h8.FindBin(0);\n",
    "bin2 = h8.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h8.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h9.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h10.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h11.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "\n",
    "canvas.SetGrid()\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "ratio_recons_to_recons_mc=h11.Divide(h9)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "#ratio_recons_to_mc=h4.Divide(h5)\n",
    "h11.Draw('colz')\n",
    "\n",
    "h11.GetZaxis().SetLabelSize (0.02)\n",
    "\n",
    "h11.SetTitleOffset(-1)\n",
    "latex = ROOT . TLatex ()\n",
    "latex . SetNDC ()\n",
    "latex . SetTextSize (0.03)\n",
    "#latex . DrawLatex (0.4 ,0.7, \"#Lambda_{Reconstructed} / #Lambda_{MC} =  %.f / %.f = %.3f\"%(sum(a),df4[df4['issignal']>0].shape[0], sum(a) / (df4[df4['issignal']>0].shape[0])))\n",
    "latex . DrawLatex (0.3 ,0.7, \"#Lambda_{Reconstructed} / #Lambda_{Simulated} =  %.f / %.f = %.3f\"%(sum(a),sum(pt_y_yields ['pt_y_yields_MC']), sum(a) / (sum(pt_y_yields ['pt_y_yields_MC']))))\n",
    "latex . DrawLatex (0.3 ,0.6, \"URQMD\")\n",
    "\n",
    "latex.Draw()\n",
    "\n",
    "\n",
    "h11 . SetTitle (\"\")\n",
    "h11 .GetXaxis().SetTitle(\"y_{Lab}\")\n",
    "h11 .GetXaxis().SetTitleOffset(0)\n",
    "h11 .GetYaxis().SetTitle(\"p_{T} GeV/c\")\n",
    "h11 .GetXaxis().SetTitleOffset(0)\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('new_urqmd_efficiency_pt_y_yield_bdt_cut_0.9.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h8 = ROOT.TH2F(\"recons_urqmd\", \"recons_urqmd\", 15,0,3,15,0,3);\n",
    "h9 = ROOT.TH2F(\"Mc_urqmd\", \"Mc_urqmd\", 15,0,3,15,0,3);\n",
    "h10 = ROOT.TH2F(\"Mc in reconstructed_urqmd\", \"Mc in reconstructed_urqmd\", 15,0,3,15,0,3);\n",
    "h11 = ROOT.TH2F(\"urqmd_Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8.SetStats(0)\n",
    "h9.SetStats(0)\n",
    "h10.SetStats(0)\n",
    "\n",
    "\n",
    "bin1 = h8.FindBin(0);\n",
    "bin2 = h8.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h8.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h9.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h10.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h11.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h8.Divide(h9)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h11.Divide(h9)\n",
    "#h8.Draw('colz')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h8 . SetTitle (\"\")\n",
    "h8 .GetXaxis().SetTitle(\"y_{Lab}\")\n",
    "h8 .GetXaxis().SetTitleOffset(0)\n",
    "h8 .GetYaxis().SetTitle(\"p_{T} GeV/c\")\n",
    "h8 .GetXaxis().SetTitleOffset(0)\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('new_dcm_100_efficiency_pt_y_yield_bdt_cut_0.8.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3);\n",
    "h6 = ROOT.TH2F(\"Mc in reconstructed\", \"Mc in reconstructed\", 15,0,3,15,0,3);\n",
    "h7 = ROOT.TH2F(\"Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8 = ROOT.TH2F(\"reconstructable_mc\", \"reconstructable_mc\", 15,0,3,15,0,3);\n",
    "\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h6.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h7.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, dcm_clean_mc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h5)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h7.Divide(h5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(df_clean['rapidity'],df_clean['pT'] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((14.03+18.58)  /2)- (np.sqrt(0.59*0.59+3.22*3.22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(3267.93*3267.93+1622.55*1622.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2994-3267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inFile = ROOT . TFile . Open ( \"lambda_qa_urqmd.root\" ,\" READ \")\n",
    "inFile.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_2d = inFile.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inFile.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h4.SetStats(0)\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "canvas.Draw()\n",
    "\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (df4['rapidity'].iloc[i])\n",
    "    pT=(df4['pT'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, df4['issignal'].iloc[i]);\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h4.Draw('colz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "fig, axs =plt.subplots(figsize=(12,10))\n",
    "h =axs.hist2d(df4[df4['issignal']==1]['rapidity'], df4[df4['issignal']==1]['pT'], bins=(bins1, bins1),norm=mpl.colors.LogNorm())\n",
    "cbar=fig.colorbar(h[3], ax=axs)\n",
    "plt.show()\n",
    "fig.savefig('hists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins1 = np.linspace(0,3,16)\n",
    "bins1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic as b_s\n",
    "bin_means, bin_edges, binnumber = b_s(df[variable_xaxis],df[variable_yaxis], statistic='mean', bins=non_uniform_binning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "x_whole1 = x_whole_1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mkl\n",
    "mkl.set_num_threads(6)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "snn_classifier = MLPClassifier(hidden_layer_sizes = [300]*3)\n",
    "snn_classifier.fit(x_train, y_train)\n",
    "#snn_predictions = snn_classifier.predict(training_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_train = pd.DataFrame(snn_classifier.predict_proba(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(bst_train[0], bins=100)\n",
    "plt.hist(bst_train[1], bins=100)\n",
    "plt.show()\n",
    "\n",
    "#bst_train.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_test['mlp_preds'] = snn_classifier.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aba = snn_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(aba,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = [100]*5)\n",
    "\n",
    "grid = GridSearchCV(mlp, {}, n_jobs=6, cv=ShuffleSplit(n_splits=1),\n",
    "                    verbose=2)\n",
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
