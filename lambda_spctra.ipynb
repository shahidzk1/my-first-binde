{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import weakref \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "#from root_pandas import read_root\n",
    "\n",
    "\n",
    "from data_cleaning import clean_df\n",
    "from KFPF_lambda_cuts import KFPF_lambda_cuts\n",
    "from plot_tools import AMS, preds_prob, plot_confusion_matrix, plt_sig_back\n",
    "from tree_importer import tree_importer_with_cuts\n",
    "import uproot\n",
    "\n",
    "\n",
    "#To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") \n",
    "        \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(8)\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tree_importer_with_cuts(\"/home/shahid/Mount/gsi/u/flat_trees/PFSimplePlainTree_urqmd_5k.root\",\"PlainTree\",8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal = uproot.open('dcm_100k_signal.root:t1').arrays(library='pd')\n",
    "gc.collect()\n",
    "signal = df_clean_signal[(df_clean_signal['issignal']==1) & (df_clean_signal['mass']>1.108)\n",
    "               & (df_clean_signal['mass']<1.1227)]\n",
    "\n",
    "df_clean_urqmd = uproot.open('/home/shahid/cbmsoft/Data/urqmd_100k.root:t1').arrays(library='pd')\n",
    "df_clean =  uproot.open('/home/shahid/cbmsoft/Data/dcm_100k.root:t1').arrays(library='pd')\n",
    "signal_selected= signal\n",
    "background_selected = df_clean_urqmd[(df_clean_urqmd['issignal'] == 0)\n",
    "                & ((df_clean_urqmd['mass'] > 1.07)\n",
    "                & (df_clean_urqmd['mass'] < 1.108) | (df_clean_urqmd['mass']>1.1227) \n",
    "                   & (df_clean_urqmd['mass'] < 1.3))].sample(n=3*(signal_selected.shape[0]))\n",
    "gc.collect()\n",
    "\n",
    "dfs = [signal_selected, background_selected]\n",
    "df_scaled = pd.concat(dfs)\n",
    "\n",
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "del dfs, signal_selected, background_selected\n",
    "# Let's take a look at the top 10 entries of the df\n",
    "df_scaled.iloc[0:10,:]\n",
    "print(df_scaled.shape)\n",
    "df_scaled[df_scaled['issignal']==1].shape\n",
    "plt_sig_back(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2primneg', 'chi2primpos', 'ldl', 'distance', 'chi2geo']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int')\n",
    "\n",
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "x_whole = df_clean[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=324)\n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)\n",
    "gc.collect()\n",
    "\n",
    "x_whole_1 = df_clean_urqmd[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole_1 = pd.DataFrame(df_clean_urqmd['issignal'], dtype='int')\n",
    "dtest2 = xgb.DMatrix(x_whole_1, label = y_whole_1)\n",
    "\n",
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.3,\n",
    "              'eval_metric': 'auc', 'objective':'binary:logistic', 'nthread' : 6}\n",
    "    cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })\n",
    "\n",
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=15, init_points=8, acq='ei')\n",
    "#0.9951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0))\n",
    "        , 'objective': 'binary:logistic'}\n",
    "\n",
    "#Fit/train on training data\n",
    "bst = xgb.train(param, dtrain)\n",
    "\n",
    "#predicitions on training set\n",
    "bst_train= pd.DataFrame(data=bst.predict(dtrain, output_margin=False),  columns=[\"xgb_preds\"])\n",
    "y_train=y_train.set_index(np.arange(0,bst_train.shape[0]))\n",
    "bst_train['issignal']=y_train['issignal']\n",
    "\n",
    "#predictions on test set\n",
    "bst_test = pd.DataFrame(data=bst.predict(dtest1, output_margin=False),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']\n",
    "\n",
    "#ROC cures for the predictions on train and test sets\n",
    "train_best, test_best = AMS(y_train, bst_train['xgb_preds'],y_test, bst_test['xgb_preds'])\n",
    "\n",
    "#The first argument should be a data frame, the second a column in it, in the form 'preds'\n",
    "preds_prob(bst_test,'xgb_preds', 'issignal','test')\n",
    "\n",
    "#Applying XGB on the 10k events data-set\n",
    "df_clean['xgb_preds'] = bst.predict(dtest, output_margin=False)\n",
    "#preds_prob(df_clean,'xgb_preds', 'issignal','test')\n",
    "\n",
    "df_clean_urqmd['xgb_preds'] = bst.predict(dtest2, output_margin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.6\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3_base=df_clean[mask1]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "#xgb\n",
    "\n",
    "#issignal has 0,1,2 . So we convert all signals above zero to 1\n",
    "\n",
    "\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True, '\\n True positives = \\n (MC =1)\\n signal in \\n the distribution')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = bins1, range=range1,facecolor='magenta',alpha = 0.3,grid=True,sharey=True )\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"XGB selected $\\Lambda$ candidates with a cut of %.3f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "plt.xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.text(1.123, 4000, 'CBM Performance', fontsize=18)\n",
    "axs.text(1.123, 3500, 'URQMD, Au+Au @ 12A GeV/$c$', fontsize=18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds1'] = ((df_clean['xgb_preds']>cut3)*1)\n",
    "cnf_matrix = confusion_matrix(y_whole, df_clean['xgb_preds1'], labels=[1,0])\n",
    "#cnf_matrix = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut3))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')\n",
    "\n",
    "(cnf_matrix[[0]][0][1])/ (cnf_matrix[[0]][0][0]+cnf_matrix[[0]][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best\n",
    "df4 = df3_base\n",
    "df4 = df4[(df4['mass']>1.07)&(df4['mass']<1.3)]\n",
    "df4 = df4[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best\n",
    "df4_urqmd = df3_base\n",
    "df4_urqmd = df4_urqmd[(df4_urqmd['mass']>1.07)&(df4_urqmd['mass']<1.3)]\n",
    "df4_urqmd = df4_urqmd[['rapidity', 'mass', 'pT', 'issignal']]\n",
    "del df3_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('pt_y_yield_bdt_cut_0.95.root','recreate')\n",
    "t = TTree('t1','tree with df')\n",
    "\n",
    "\n",
    "rapidity = array('f',[0])\n",
    "mass = array('f',[0])\n",
    "pT = array('f',[0])\n",
    "issignal = array('f',[0])\n",
    "\n",
    "t.Branch('rapidity', rapidity,'y/F')\n",
    "t.Branch('mass', mass,'mass/F')\n",
    "t.Branch('pT', pT,'pT/F')\n",
    "t.Branch('issignal', issignal,'pT/F')\n",
    "\n",
    "for i in range(len(df4['mass'])):\n",
    "    rapidity[0] = df4['rapidity'].iloc[i]\n",
    "    mass[0] = df4['mass'].iloc[i]\n",
    "    pT[0] = df4['pT'].iloc[i]\n",
    "    issignal[0] = df4['issignal'].iloc[i]\n",
    "    t.Fill()\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, ROOT\n",
    "from ROOT import TF1, TCanvas,TMath, TColor\n",
    "\n",
    "class Linear:\n",
    "    def __call__( self, x, par ):\n",
    "        return par[0] + x[0]*par[1]\n",
    "\n",
    "class lorenztian:\n",
    "    def _call_(self, x, p):\n",
    "        return 0.5*p[0]*p[1] /( ((x[0]-p[2])**2) + ((0.5 * (p[1])**2))) \n",
    "\n",
    "class gaus:\n",
    "    def _call_(self, x ,p):\n",
    "        return p[0]*np.exp(-0.5*((x[0]-p[2])/p[1])**2)\n",
    "    \n",
    "import math\n",
    "def truncate(number, decimals=2):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "\n",
    "def background_selector(df):\n",
    "    df1 = df[(df['mass']<1.108)]\n",
    "    df2 = df[df['mass']>1.13]\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    return df3['mass'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h1_set(h1):\n",
    "    h1 . SetTitleOffset(-1)\n",
    "    h1 . SetFillStyle(3003);\n",
    "    h1 . SetLineWidth(2)\n",
    "    h1 . SetStats (0)\n",
    "    h1 . SetYTitle(\"Entries\")\n",
    "    h1 . SetLineColor (ROOT . kBlack)\n",
    "    h1 . GetXaxis () . SetLabelSize (0)\n",
    "    h1 . GetXaxis () . SetTitleSize (0)\n",
    "    h1 . GetYaxis () . SetTitleSize (0.05)\n",
    "    h1 . GetYaxis () . SetLabelSize (0.03)\n",
    "    h1 . GetYaxis () . SetTitleOffset (0.6)\n",
    "    h1 . GetYaxis () . SetNdivisions(107)\n",
    "    return h1\n",
    "\n",
    "\n",
    "def h3_set(h3):   \n",
    "    h3 . SetLineWidth(2)\n",
    "    h3 . SetStats (0)\n",
    "    h3 . GetXaxis() . SetTitle(\"Mass [GeV/c^{2}]\")\n",
    "    h3 . SetTitle (\"\")\n",
    "    h3 . GetXaxis () . SetLabelSize (0.12)\n",
    "    h3 . GetXaxis () . SetTitleSize (0.12)\n",
    "    h3 . GetYaxis () . SetLabelSize (0.1)\n",
    "    h3 . GetYaxis () . SetTitleSize (0.15)\n",
    "    #ratio . GetYaxis (). SetTitle (\" Data /MC\")\n",
    "    h3 . GetYaxis (). SetTitleOffset (0.17)\n",
    "    #207,512 divisions\n",
    "    h3 . GetYaxis (). SetNdivisions (207)\n",
    "    h3 . GetXaxis (). SetNdivisions (207)\n",
    "    h3.SetLineColor(TColor.GetColor(5))\n",
    "    h3.SetYTitle(\"d-f/#Deltad\")\n",
    "    return h3\n",
    "\n",
    "\n",
    "def f_set(ftot, fs, fb):\n",
    "    ftot.SetNpx(100000);\n",
    "    ftot.SetLineColor(ROOT.kRed)\n",
    "    \n",
    "    fs.SetNpx(100000);\n",
    "    fs.SetLineColor(ROOT.kGreen)\n",
    "    \n",
    "    fb.SetLineStyle(4)\n",
    "    fb.SetLineColor(ROOT.kBlue)\n",
    "    fb.SetNpx(100000);\n",
    "    return ftot, fs, fb\n",
    "\n",
    "\n",
    "def draw_line():\n",
    "    line = ROOT . TLine (mm,0 ,1.23 ,0)\n",
    "    line . SetLineColor ( ROOT . kRed )\n",
    "    line . SetLineWidth (2)\n",
    "    return line\n",
    "\n",
    "\n",
    "def draw_latex():\n",
    "    latex = ROOT . TLatex ()\n",
    "    latex . SetNDC ()\n",
    "    latex . SetTextSize (0.02)\n",
    "    latex . DrawLatex (0.4 ,0.85, \"Significance in m_{0} #pm 2.5#Gamma  = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma,bac_under_peak_2_point_5_sigma,signal_under_peak_2_point_5_sigma/TMath.Sqrt(bac_under_peak_2_point_5_sigma+signal_under_peak_2_point_5_sigma) ))\n",
    "    latex . DrawLatex (0.4 ,0.80, \"Significance in m_{0} #pm 3#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_sigma,man_sigma_signal_under_peak_3_sigma, signal_under_peak_3_sigma,backgnd_under_peak_3_sigma,signal_under_peak_3_sigma/TMath.Sqrt(backgnd_under_peak_3_sigma+signal_under_peak_3_sigma) ))\n",
    "    latex . DrawLatex (0.4 ,0.75, \"Significance in m_{0} #pm 3.5#Gamma = #frac{%.1f #pm %.1f}{#sqrt{%.1f+%.1f}} = %.1f\"%(signal_under_peak_3_point_5_sigma,man_sigma_signal_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma,bac_under_peak_3_point_5_sigma,signal_under_peak_3_point_5_sigma/TMath.Sqrt(signal_under_peak_3_point_5_sigma+bac_under_peak_3_point_5_sigma) ))\n",
    "    latex . DrawLatex (0.4 ,0.70, \" #Gamma = %.4f #pm %.5f GeV\"%(par2 [1],f2.GetParError(1) ))\n",
    "    latex . DrawLatex (0.4 ,0.65, \" m_{0} = %.4f #pm %.5f GeV\"%(par2 [2],f2.GetParError(2) ))\n",
    "    latex . DrawLatex (0.4 ,0.6,\" #frac{#chi^{2}}{ndf} = %.1f/%d = %.4f\"%(f2.GetChisquare() , f2.GetNDF() , f2.GetChisquare() / f2.GetNDF() ))\n",
    "    latex . DrawLatex (0.4 ,0.55,\" True signal (MC=1) = %.f\"%(mc_counts))\n",
    "    return latex\n",
    "    \n",
    "    \n",
    "def draw_legend():\n",
    "    legend = ROOT.TLegend(0.87,0.3,0.6,0.6);\n",
    "    legend . AddEntry(h1,\"Invariant mass of lambda\",\"l\");\n",
    "    legend . AddEntry(f2,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}+B+Cx+Dx^{2}\",\"l\");\n",
    "    legend . AddEntry(fs,\"A #frac{0.5 #Gamma}{(m-m_{0})^{2} + 0.25#Gamma^{2}}\",\"l\");\n",
    "    legend . AddEntry(fb,\"B+Cx+Dx^{2}\",\"l\");\n",
    "    legend . SetLineWidth (0)\n",
    "    return legend\n",
    "\n",
    "\n",
    "def createCanvasPads():\n",
    "    c = ROOT . TCanvas (\" canvas \",\"\", 1200,1000)\n",
    "    \n",
    "    pad1 = ROOT . TPad (\" pad1 \",\" pad1 \" ,0 ,0.3 ,1 ,1)\n",
    "    pad1 . SetBottomMargin (0)\n",
    "    pad1 . Draw ()\n",
    "    \n",
    "    pad2 = ROOT . TPad (\" pad2 \",\" pad2 \" ,0 ,0.05 ,1 ,0.3)\n",
    "    pad2 . SetGrid()\n",
    "    pad2 . SetTopMargin (0)\n",
    "    pad2 . SetBottomMargin (0.25)\n",
    "    pad2 . Draw ()\n",
    "    return c, pad1, pad2\n",
    "\n",
    "\n",
    "def draw_hist(h1, f2, fs, fb, h3):     \n",
    "    c, pad1, pad2 = createCanvasPads ()\n",
    "    c . Draw ()\n",
    "    pad1 . cd ()\n",
    "    \n",
    "    h1 = h1_set (h1)\n",
    "    f2, fs, fb = f_set (f2, fs, fb)\n",
    "    h1 . Draw(\"pe\")\n",
    "    fs . Draw(\"SAME\")\n",
    "    fb . Draw(\"SAME\")\n",
    "    f2 . Draw(\"SAME\")\n",
    "    draw_latex()\n",
    "    legend = draw_legend ()\n",
    "    legend . Draw()\n",
    "    \n",
    "    c . cd ()\n",
    "    pad2 . cd ()\n",
    "    h3_set(h3) . Draw()\n",
    "    line = draw_line ()\n",
    "    line . Draw (\" same \")\n",
    "    \n",
    "    c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf [\")\n",
    "    \n",
    "    #c . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.pdf ]\")\n",
    "    \n",
    "    \n",
    "def signal_cal(h1, f2, fs, fb):\n",
    "    tot_sig_3_point_5_sigma, tot_sig_3_sigma, tot_sig_2_point_5_sigma, tot_sig_2_sigma = 0, 0, 0, 0\n",
    "    tot_bac_3_sigma, tot_bac_3_point_5_sigma, tot_bac_2_point_5_sigma = 0, 0, 0    \n",
    "    \n",
    "    binwidth = h1.GetXaxis().GetBinWidth(1);\n",
    "    tot = f2.Integral(par2[2] - (TMath.Abs(3*par2[1])),par2[2] + (TMath.Abs(3*par2[1])))/binwidth;\n",
    "    sigma_integral = f2.IntegralError(par2[2] - (TMath.Abs(3*par2[1])),par2[2] + (TMath.Abs(3*par2[1])));\n",
    "    #params.integral = fit->GetParameter(0) * sqrt(2*3.1415) * fit->GetParameter(2) / h->GetBinWidth(1);\n",
    "    #signal_under_peak = par2[1] * np.sqrt(2*3.1415) *3 *par2[2]/ binwidth\n",
    "    signal_under_peak_3_sigma = fs.Integral(par2[2] - (TMath.Abs(3*par2[1])),par2[2] + (TMath.Abs(3*par2[1])))/binwidth\n",
    "               \n",
    "    sigma_signal_under_peak_3_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3*par2[1])),par2[2] + (TMath.Abs(3*par2[1])));\n",
    "    man_sigma_signal_under_peak_3_sigma = TMath.Sqrt(signal_under_peak_3_sigma)\n",
    "\n",
    "\n",
    "    tot_sig_3_sigma= tot_sig_3_sigma+signal_under_peak_3_sigma\n",
    "#Background\n",
    "    backgnd_under_peak_3_sigma = (fb.Integral(par2[2] - (TMath.Abs(3*par2[1])),par2[2] + (TMath.Abs(3*par2[1])))/binwidth)\n",
    "\n",
    "    sigma_backgnd_under_peak_3_sigma = fb.IntegralError(par2[2] - (TMath.Abs(3*par2[1])),par2[2] + (TMath.Abs(3*par2[1])));\n",
    "    tot_bac_3_sigma = tot_bac_3_sigma+backgnd_under_peak_3_sigma\n",
    "\n",
    "    signal_under_peak_3_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "    bac_under_peak_3_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])))/binwidth);\n",
    "    tot_sig_3_point_5_sigma = tot_sig_3_point_5_sigma+signal_under_peak_3_point_5_sigma\n",
    "    tot_bac_3_point_5_sigma = tot_bac_3_point_5_sigma + bac_under_peak_3_point_5_sigma\n",
    "\n",
    "    sigma_signal_under_peak_3_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(3.5*par2[1])),par2[2] + (TMath.Abs(3.5*par2[1])));\n",
    "    man_sigma_signal_under_peak_3_point_5_sigma = TMath.Sqrt(signal_under_peak_3_point_5_sigma)\n",
    "\n",
    "    signal_under_peak_2_point_5_sigma = (fs.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "    bac_under_peak_2_point_5_sigma = (fb.Integral(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])))/binwidth);\n",
    "    tot_sig_2_point_5_sigma = tot_sig_2_point_5_sigma+signal_under_peak_2_point_5_sigma\n",
    "    tot_bac_2_point_5_sigma = tot_bac_2_point_5_sigma + bac_under_peak_2_point_5_sigma\n",
    "\n",
    "    sigma_signal_under_peak_2_point_5_sigma = fs.IntegralError(par2[2] - (TMath.Abs(2.5*par2[1])),par2[2] + (TMath.Abs(2.5*par2[1])));\n",
    "    man_sigma_signal_under_peak_2_point_5_sigma = TMath.Sqrt(signal_under_peak_2_point_5_sigma)\n",
    "\n",
    "    signal_under_peak_2_sigma = (fs.Integral(par2[2] - (TMath.Abs(2*par2[1])),par2[2] + (TMath.Abs(2*par2[1])))/binwidth);\n",
    "    \n",
    "    return  signal_under_peak_3_sigma, man_sigma_signal_under_peak_3_sigma, backgnd_under_peak_3_sigma, signal_under_peak_3_point_5_sigma, bac_under_peak_3_point_5_sigma, tot_sig_3_point_5_sigma, tot_bac_3_point_5_sigma, man_sigma_signal_under_peak_3_point_5_sigma, signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, bac_under_peak_3_point_5_sigma, bac_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma, signal_under_peak_2_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "535/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "pt_y_bin_for_yield_min=[]\n",
    "pt_y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_min=[]\n",
    "true_mc_in_recons =[]\n",
    "\n",
    "\n",
    "df = df4\n",
    "mass_range_min = [1.08]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               1.23,\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "\n",
    "\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,1,1):\n",
    "\n",
    "        binning = [100]\n",
    "        for b in binning:\n",
    "\n",
    "            y_bin_low=-0.2\n",
    "            y_bin_up =0\n",
    "            for i in range(0,15,1):\n",
    "                tot_sig_3_point_5_sigma, tot_sig_3_sigma, tot_sig_2_point_5_sigma, tot_sig_2_sigma = 0, 0, 0, 0\n",
    "                tot_bac_3_sigma, tot_bac_3_point_5_sigma, tot_bac_2_point_5_sigma = 0, 0, 0\n",
    "                \n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0\n",
    "                \n",
    "                for i in range(0,15,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    #print(pt_bin_low)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "                    #print(y_bin_low, y_bin_up, \" pT \", pt_bin_low,pt_bin_up)\n",
    "                    if df_pt.shape[0]>200:\n",
    "                        data0 = background_selector(df_pt)\n",
    "                        h0 = ROOT.TH1F(\"Background\",\"Background without peak\",b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data0.shape[0]):\n",
    "                            h0.Fill(data0.iloc[i])\n",
    "                        fb = TF1(\"fb\",\"pol2\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        h0.Fit(fb,\"RIEM\");\n",
    "                        par = fb.GetParameters()\n",
    "                        data = df_pt['mass']\n",
    "                        \n",
    "                #the minimum x (lower edge of the first bin)=mm        \n",
    "                        h1 = ROOT.TH1F(\"B_&_S\",\"rapidity=[%.2f,%.2f] & p_{T}=[%.2f,%.2f] & Min Mass= %.3f & bins=%.0f\"%(df_pt['rapidity'].min(),df_pt['rapidity'].max(),df_pt['pT'].min(),df_pt['pT'].max(), mm, b),b,mm,fit_limit_low[5])\n",
    "                        for i in range(0,data.shape[0]):\n",
    "                            h1.Fill(data.iloc[i])\n",
    "                        f1 = TF1(\"step1\",\"((0.5)*[0]*0.0014) /((x-1.115683)*(x-1.115683)+ .25*0.0014*0.0014) +[1]+[2]*x+[3]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);\n",
    "                        f1.SetParameters(1,par[0], par[1], par[2]);\n",
    "                        h1.Fit(f1,\"RNI\");\n",
    "                        par1 = f1.GetParameters()\n",
    "\n",
    "\n",
    "                        f2 = TF1(\"full\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1]) +[3]+[4]*x+[5]*x*x\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3])\n",
    "                        f2.SetParameters(par1[0],0.001,1.115,par1[1], par1[2], par1[3]);                  \n",
    "                        r= ROOT.TFitResultPtr(h1.Fit(f2,\"MNIR\"))\n",
    "                        par2 = f2.GetParameters()\n",
    "\n",
    "                        fs = TF1(\"fs\",\"((0.5)*[0]*[1]) /((x-[2])*(x-[2])+ .25*[1]*[1])\",fit_limit_low[mmm]+mm,fit_limit_low[mmm+3]);                     \n",
    "                        fs.SetParameters(par2[0],par2[1],par2[2]);\n",
    "                        fb.SetParameters(par2[3],par2[4],par2[5], par2[6]);\n",
    "\n",
    "                        h2 = ROOT.TH1F(\"h2\", \"\", b, mm, 1.23);\n",
    "                        h3 = ROOT.TH1F(\"h3\", \"\", b, mm, 1.23);\n",
    "\n",
    "                        bin1 = h1.FindBin(fit_limit_low[mmm]+mm);\n",
    "                        bin2 = h1.FindBin(fit_limit_low[mmm+3]);\n",
    "                        for i in range(bin1,bin2):\n",
    "                            f_value= f2.Eval(h1.GetBinCenter(i));\n",
    "                            t_value = h1.GetBinContent(i)\n",
    "                            h2.SetBinContent(i,f_value)\n",
    "                            if (h1.GetBinError(i) > 0):\n",
    "                                h3.SetBinContent(i,(t_value-f_value)/h1.GetBinError(i))\n",
    "\n",
    "                        h2.Sumw2()\n",
    "\n",
    "                        signal_under_peak_3_sigma, man_sigma_signal_under_peak_3_sigma, backgnd_under_peak_3_sigma, signal_under_peak_3_point_5_sigma, bac_under_peak_3_point_5_sigma, tot_sig_3_point_5_sigma, tot_bac_3_point_5_sigma, man_sigma_signal_under_peak_3_point_5_sigma, signal_under_peak_2_point_5_sigma, man_sigma_signal_under_peak_2_point_5_sigma, bac_under_peak_3_point_5_sigma, bac_under_peak_2_point_5_sigma, signal_under_peak_2_point_5_sigma, signal_under_peak_2_sigma  = signal_cal(h1, f2, fs, fb)\n",
    "\n",
    "                        draw_hist(h1, f2, fs, fb, h3)\n",
    "                        \n",
    "                        \n",
    "            #a.append(tot_sig_2_point_5_sigma)\n",
    "                        a.append(signal_under_peak_2_sigma)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "                    else:\n",
    "                        a.append(0)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "file =uproot.open(\"lambda_qa_dcm.root\")\n",
    "array1 = file[\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda\"].to_numpy()\n",
    "#for i in range(0,14,1):\n",
    "array1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URQMD\n",
    "size = 15*15\n",
    "pt_y_yields = pd.DataFrame(data=np.arange(0,size,1),columns = ['numbering'])\n",
    "pt_y_yields['rapidity_min_MC'] = np.zeros(size)\n",
    "pt_y_yields['pT_min_MC'] = np.zeros(size)\n",
    "\n",
    "pt_y_yields['ratio_recons_sim']=np.zeros(size)\n",
    "pt_y_yields['ratio_recons_mc']=np.zeros(size)\n",
    "pt_y_yields['pT_min'] = np.zeros(size)\n",
    "pt_y_yields ['pt_y_yields_MC']=np.zeros(size)\n",
    "pt_y_yields['pt_y_yields_recons']=true_mc_in_recons\n",
    "pt_y_yields['true_mc_in_recons'] = true_mc_in_recons\n",
    "#pt_y_yields['total_mc_in_recons'] = dcm_clean_mc\n",
    "\n",
    "for i in range(0,15):\n",
    "    for j in range(0,15):\n",
    "        pt_y_yields['rapidity_min_MC'].iloc[i+j*15]=0+j*0.2\n",
    "    \n",
    "\n",
    "for i in range(0,15):    \n",
    "    pt_y_yields['pT_min_MC'].iloc[i]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+1*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+2*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+3*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+4*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+5*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+6*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+7*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+8*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+9*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+10*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+11*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+12*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+13*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+14*15]=i/5\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(0,15,1):\n",
    "    pt_y_yields ['pt_y_yields_MC'].iloc[i]=array1[0][0][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+1*15]=array1[0][1][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+2*15]=array1[0][2][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+3*15]=array1[0][3][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+4*15]=array1[0][4][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+5*15]=array1[0][5][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+6*15]=array1[0][6][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+7*15]=array1[0][7][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+8*15]=array1[0][8][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+9*15]=array1[0][9][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+10*15]=array1[0][10][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+11*15]=array1[0][11][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+12*15]=array1[0][12][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+13*15]=array1[0][13][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+14*15]=array1[0][14][i]\n",
    "\n",
    "for i in range(0,15*15,1):\n",
    "    pt_y_yields['ratio_recons_mc'].iloc[i]=true_mc_in_recons[i]/pt_y_yields['true_mc_in_recons'].iloc[i]\n",
    "    pt_y_yields['ratio_recons_sim'].iloc[i]=true_mc_in_recons[i]/pt_y_yields['pt_y_yields_MC'].iloc[i]\n",
    "    pt_y_yields['pT_min'].iloc[i] = pt_y_bin_for_yield_min[i]\n",
    "    #print(\"%.2f\"%pt_y_yields['rapidity_min_MC'].iloc[i],\"       \",pt_y_yields['pT_min_MC'].iloc[i],\"    \", pt_y_yields['ratio'].iloc[i] )\n",
    "#plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Reconstructed/Sim')\n",
    "plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_mc'], label='Rencostructed/MC')\n",
    "plt.legend()\n",
    "plt.ylim([0.9,1.1])\n",
    "plt.savefig(\"hists\")\n",
    "#pt_y_yields[(pt_y_yields['rapidity_min_MC']>1) & (pt_y_yields['rapidity_min_MC']<1.4) &(pt_y_yields['pT_min_MC']<1)&(pt_y_yields['pT_min_MC']>0)]\n",
    "pt_y_yields[(pt_y_yields['numbering']>20) & (pt_y_yields['numbering']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcm\n",
    "size = 15*15\n",
    "pt_y_yields = pd.DataFrame(data=np.arange(0,size,1),columns = ['numbering'])\n",
    "pt_y_yields['rapidity_min_MC'] = np.zeros(size)\n",
    "pt_y_yields['pT_min_MC'] = np.zeros(size)\n",
    "\n",
    "pt_y_yields['ratio_recons_sim']=np.zeros(size)\n",
    "pt_y_yields['ratio_recons_mc']=np.zeros(size)\n",
    "pt_y_yields['pT_min'] = np.zeros(size)\n",
    "pt_y_yields ['pt_y_yields_MC']=np.zeros(size)\n",
    "pt_y_yields['pt_y_yields_recons']=true_mc_in_recons\n",
    "pt_y_yields['true_mc_in_recons'] = true_mc_in_recons\n",
    "#pt_y_yields['total_mc_in_recons'] = dcm_clean_mc\n",
    "\n",
    "for i in range(0,15):\n",
    "    for j in range(0,15):\n",
    "        pt_y_yields['rapidity_min_MC'].iloc[i+j*15]=0+j*0.2\n",
    "    \n",
    "\n",
    "for i in range(0,15):    \n",
    "    pt_y_yields['pT_min_MC'].iloc[i]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+1*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+2*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+3*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+4*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+5*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+6*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+7*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+8*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+9*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+10*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+11*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+12*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+13*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+14*15]=i/5\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(0,15,1):\n",
    "    pt_y_yields ['pt_y_yields_MC'].iloc[i]=array1[0][0][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+1*15]=array1[0][1][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+2*15]=array1[0][2][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+3*15]=array1[0][3][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+4*15]=array1[0][4][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+5*15]=array1[0][5][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+6*15]=array1[0][6][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+7*15]=array1[0][7][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+8*15]=array1[0][8][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+9*15]=array1[0][9][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+10*15]=array1[0][10][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+11*15]=array1[0][11][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+12*15]=array1[0][12][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+13*15]=array1[0][13][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+14*15]=array1[0][14][i]\n",
    "\n",
    "for i in range(0,15*15,1):\n",
    "#    pt_y_yields['ratio_recons_mc'].iloc[i]=dcm_clean_mc[i]/pt_y_yields['true_mc_in_recons'].iloc[i]\n",
    "    pt_y_yields['ratio_recons_sim'].iloc[i]=true_mc_in_recons[i]/pt_y_yields['pt_y_yields_MC'].iloc[i]\n",
    "#    pt_y_yields['pT_min'].iloc[i] = pt_y_bin_for_yield_min[i]\n",
    "    #print(\"%.2f\"%pt_y_yields['rapidity_min_MC'].iloc[i],\"       \",pt_y_yields['pT_min_MC'].iloc[i],\"    \", pt_y_yields['ratio'].iloc[i] )\n",
    "#plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Reconstructed/Sim')\n",
    "plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_mc'], label='Rencostructed/MC')\n",
    "plt.legend()\n",
    "plt.ylim([0.9,1.1])\n",
    "plt.savefig(\"hists\")\n",
    "#pt_y_yields[(pt_y_yields['rapidity_min_MC']>1) & (pt_y_yields['rapidity_min_MC']<1.4) &(pt_y_yields['pT_min_MC']<1)&(pt_y_yields['pT_min_MC']>0)]\n",
    "pt_y_yields[(pt_y_yields['numbering']>20) & (pt_y_yields['numbering']<50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('new_urqmd_efficiency_pt_y_yield_bdt_cut_0.9.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h8 = ROOT.TH2F(\"recons_urqmd\", \"recons_urqmd\", 15,0,3,15,0,3);\n",
    "h9 = ROOT.TH2F(\"Mc_urqmd\", \"Mc_urqmd\", 15,0,3,15,0,3);\n",
    "h10 = ROOT.TH2F(\"Mc in reconstructed_urqmd\", \"Mc in reconstructed_urqmd\", 15,0,3,15,0,3);\n",
    "h11 = ROOT.TH2F(\"urqmd_Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8.SetStats(0)\n",
    "h9.SetStats(0)\n",
    "h10.SetStats(0)\n",
    "\n",
    "\n",
    "bin1 = h8.FindBin(0);\n",
    "bin2 = h8.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h8.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h9.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h10.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h11.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h8.Divide(h9)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h11.Divide(h9)\n",
    "#h8.Draw('colz')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h8 . SetTitle (\"\")\n",
    "h8 .GetXaxis().SetTitle(\"y_{Lab}\")\n",
    "h8 .GetXaxis().SetTitleOffset(0)\n",
    "h8 .GetYaxis().SetTitle(\"p_{T} GeV/c\")\n",
    "h8 .GetXaxis().SetTitleOffset(0)\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('new_dcm_100_efficiency_pt_y_yield_bdt_cut_0.9.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "\n",
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3);\n",
    "h6 = ROOT.TH2F(\"Mc in reconstructed\", \"Mc in reconstructed\", 15,0,3,15,0,3);\n",
    "h7 = ROOT.TH2F(\"Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8 = ROOT.TH2F(\"reconstructable_mc\", \"reconstructable_mc\", 15,0,3,15,0,3);\n",
    "\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h6.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h7.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, dcm_clean_mc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h5)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h7.Divide(h5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
