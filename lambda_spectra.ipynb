{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Cuts Optimization using Extra Gradient Boosting\n",
    "<br></p><br>\n",
    "\n",
    "Over the last years, **Machine Learning** tools have been successfully applied to problems in high-energy physics. For example, for the classification of physics objects. Supervised machine learning algorithms allow for significant improvements in classification problems by taking into account observable correlations and by learning the optimal selection from examples, e.g. from Monte Carlo simulations.\n",
    "\n",
    "\n",
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook.\n",
    "\n",
    "**Matplotlib** comes handy in plotting data while the machine learning is performed by **XGBOOST**. We will import data splitter from **Scikit-learn** as *train_test_split*. **Evaluation metrics** such as *confusion matrix*, *Receiver operating characteristic (ROC)*, and *Area Under the Receiver Operating Characteristic Curve (ROC AUC)*  will be used to asses our models.\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "import weakref \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "#from root_pandas import read_root\n",
    "\n",
    "\n",
    "from data_cleaning import clean_df\n",
    "from KFPF_lambda_cuts import KFPF_lambda_cuts\n",
    "from plot_tools import AMS, preds_prob, plot_confusion_matrix, plt_sig_back\n",
    "import tree_importer \n",
    "import uproot\n",
    "\n",
    "\n",
    "#To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") \n",
    "        \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(8)\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file, for example a URQMD 12 AGeV, is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **tree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "In this example, we download three Analysis Trees. The first one contains mostly background candidates for lambda i.e. protons and pions which do not come from a lambda. The second file contains mostly signal candidates of lamba i.e. it contains protons and pions which come from a lambda decay. The third one contains 10k events generated using URQMD generator with 12 AGeV energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3/4)/(1-(3/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['index', 'LambdaCandidates_chi2geo', 'LambdaCandidates_chi2primneg',\n",
    "       'LambdaCandidates_chi2primpos', 'LambdaCandidates_chi2topo',\n",
    "       'LambdaCandidates_cosineneg', 'LambdaCandidates_cosinepos',\n",
    "       'LambdaCandidates_cosinetopo', 'LambdaCandidates_distance',\n",
    "       'LambdaCandidates_eta', 'LambdaCandidates_l', 'LambdaCandidates_ldl',\n",
    "       'LambdaCandidates_mass', 'LambdaCandidates_p', 'LambdaCandidates_pT',\n",
    "       'LambdaCandidates_phi', 'LambdaCandidates_rapidity','LambdaCandidates_issignal']\n",
    "new_labels = []\n",
    "\n",
    "for i in a:\n",
    "    if 'LambdaCandidates_' in i:\n",
    "        new_labels.append(i.replace('LambdaCandidates_',''))\n",
    "    else:\n",
    "        new_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal = uproot.open('dcm_5m_signal.root:plain_tree').arrays(library='pd')\n",
    "df_clean_signal.columns = new_labels\n",
    "gc.collect()\n",
    "\n",
    "signal = df_clean_signal[ (df_clean_signal['mass']>df_clean_signal['mass'].mean()-3*df_clean_signal['mass'].std())\n",
    "               & (df_clean_signal['mass']<df_clean_signal['mass'].mean()+3*df_clean_signal['mass'].std()) ]\n",
    "#signal['issignal']=((signal['issignal']<2)*0 )\n",
    "signal['issignal']=((signal['issignal']>0)*1)\n",
    "signal[\"issignal\"]=signal[\"issignal\"].astype(\"int8\")\n",
    "#signal[\"issignal\"].replace({1: 0, 2: 1}, inplace=True)\n",
    "#del df_clean_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_urqmd = uproot.open('urqmd_100k_uproot.root:t1').arrays(library='pd')\n",
    "df_clean_urqmd.columns = new_labels\n",
    "df_clean_urqmd['issignal']=((df_clean_urqmd['issignal']>0)*1)\n",
    "df_clean_urqmd[\"issignal\"]=df_clean_urqmd[\"issignal\"].astype(\"int8\")\n",
    "#df_clean_urqmd = df_clean_urqmd[df_clean_urqmd['issignal']>0]\n",
    "#df_clean_urqmd[\"issignal\"].replace({1: 0, 2: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean =  uproot.open('dcm_100k_uproot.root:t1').arrays(library='pd')\n",
    "df_clean.columns = new_labels\n",
    "df_clean['issignal']=((df_clean['issignal']>0)*1)\n",
    "df_clean[\"issignal\"]=df_clean[\"issignal\"].astype(\"int8\")\n",
    "#df_clean = df_clean[df_clean['issignal']>0]\n",
    "#df_clean[\"issignal\"].replace({1: 0, 2: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_bac_dist(df1):\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    with PdfPages('multipage_pdf.pdf') as pdf:\n",
    "        df = df1.copy()\n",
    "        for i in df1.columns:\n",
    "            list =['chi2geo', 'chi2primneg', 'chi2primpos','chi2topo', 'distance', 'ldl', 'rapidity','mass', 'pT']\n",
    "            if i in list:\n",
    "                df[i]=np.log(df[i])\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            bin1 = 300 \n",
    "            plt.hist(df[df['issignal']==0][i],bins = bin1, color = 'blue',alpha = 0.3,label='Background')\n",
    "            plt.hist(df[df['issignal']==1][i],bins = bin1, color = 'red',label='Primaries', alpha =0.3)\n",
    "            plt.hist(df[df['issignal']>1][i],bins = bin1, color = 'green',label='Secondaries', alpha =0.3)\n",
    "            plt.yscale('log')\n",
    "            #plt.grid()\n",
    "            plt.ylabel('counts (log scale)', fontsize = 18)\n",
    "            #plt.xlabel('$\\chi^{2}_{geometrical}$', fontsize = 18)\n",
    "            plt.legend(loc='upper right',fontsize=18)\n",
    "            plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "            #ax.text(0, 1500, r'CBM Performance', fontsize=15)\n",
    "            #ax.text(0, 500, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "            plt.title('URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=18)\n",
    "            if i in list:\n",
    "                plt.xlabel(\"log \"+i, fontsize = 18)\n",
    "            else:\n",
    "                plt.xlabel(i, fontsize = 18)\n",
    "            #plt.xlim([1.07,1.2])\n",
    "            #plt.show()\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        del df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data frame object has some columns/features and for them at the very last column the true Monte Carlos information is available. This MC information tells us whether this reconstructed particle was originally produced as a decaying particle or not. So a value of 1 means that it is a true candidate and 0 means that it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which do not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. Ofcourse, we lose some data points but these outliers sometimes cause problems when we perform analysis. \n",
    "\n",
    "Since our experiment is a fixed target experiment so there are certain constraints which have to be applied on the data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Background and Signal\n",
    "Our sample contains a lot of background (2178718) and somewhat signal candidates (36203). For analysis we will use a signal set of 4000 candidates and a background set of 12000 candidates. The background and signal candidates will be selected by using MC information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_selected= signal[(signal['mass']>1.1) & (signal['mass']<1.135)]\n",
    "background_selected = df_clean_urqmd[(df_clean_urqmd['issignal'] == 0)\n",
    "                & ((df_clean_urqmd['mass'] > 1.07)\n",
    "                & (df_clean_urqmd['mass'] < 1.1) | (df_clean_urqmd['mass']>1.135) \n",
    "                   & (df_clean_urqmd['mass'] < 1.3))].sample(n=3*(signal_selected.shape[0]))\n",
    "gc.collect()\n",
    "\n",
    "#Let's combine signal and background\n",
    "dfs = [signal_selected, background_selected]\n",
    "df_scaled = pd.concat(dfs)\n",
    "\n",
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "#del dfs, signal_selected, background_selected, signal\n",
    "# Let's take a look at the top 10 entries of the df\n",
    "df_scaled.iloc[0:10,:]\n",
    "#del signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaled.shape)\n",
    "print(df_scaled[df_scaled['issignal']==1].shape)\n",
    "print(df_scaled[df_scaled['issignal']==2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 0.5 * (np.log(E+P/E-P))\n",
    "\n",
    "\n",
    "https://cbm-wiki.gsi.de/foswiki/bin/view/PWG/CbmCollisionEnergies\n",
    "\n",
    "\n",
    "y = 0.5 * (np.log((12+10)/(12-10)))\n",
    "\n",
    "\n",
    "using this the rapidity is 3.1992 for Ebeam =12.04 and pbeam =12 and mid rapidity is y/2=  1.5996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt_sig_back(df_scaled)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(8)\n",
    "axs.text(1.13, 6000, r'DCM-QGSM-SMM', color = 'magenta',  fontsize=15)\n",
    "axs.text(1.13, 4000, r'Au+Au @ 12 $A$GeV/$c$', color = 'magenta',  fontsize=15)\n",
    "axs.text(1.13, 2000, r'URQMD, Au+Au @ 12 $A$GeV/$c$', fontsize=15)\n",
    "fig.savefig(\"hists.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2geo', 'chi2primneg', 'chi2primpos', 'distance', 'ldl']\n",
    "#cuts = [ 'chi2geo', 'chi2primneg', 'chi2primpos', 'chi2topo',\n",
    "#       'cosineneg', 'cosinepos', 'cosinetopo', 'distance', 'eta', 'l', 'ldl']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()\n",
    "\n",
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "x_whole = df_clean[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>XGB Boost \n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient. Stratify so that both train and test get the same ratio of signal to background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=324, stratify=y)\n",
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)\n",
    "x_whole_1 = df_clean_urqmd[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole_1 = pd.DataFrame(df_clean_urqmd['issignal'], dtype='int8')\n",
    "dtest2 = xgb.DMatrix(x_whole_1, label = y_whole_1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.3,\n",
    "              'eval_metric': 'auc','tree_method':'hist','objective':'binary:logistic', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params=params, dtrain=dtrain, num_boost_round=10, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0.01,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })\n",
    "\n",
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=5, init_points=5, acq='ei')\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")\n",
    "#0.9983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,∞]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'],\n",
    "        'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), \n",
    "         'objective':'binary:logistic','tree_method':'hist','nthread' : 7}\n",
    "\n",
    "#Fit/train on training data\n",
    "bst = xgb.train(param, dtrain, num_boost_round=20)\n",
    "\n",
    "#predicitions on training set\n",
    "bst_train= pd.DataFrame(data=bst.predict(dtrain),  columns=[\"xgb_preds\"])\n",
    "y_train=y_train.set_index(np.arange(0,bst_train.shape[0]))\n",
    "bst_train['issignal']=y_train['issignal']\n",
    "\n",
    "#predictions on test set\n",
    "bst_test = pd.DataFrame(data=bst.predict(dtest1),  columns=[\"xgb_preds\"])\n",
    "y_test=y_test.set_index(np.arange(0,bst_test.shape[0]))\n",
    "bst_test['issignal']=y_test['issignal']\n",
    "\n",
    "#ROC cures for the predictions on train and test sets\n",
    "train_best, test_best = AMS(y_train, bst_train['xgb_preds'],y_test, bst_test['xgb_preds'])\n",
    "\n",
    "#The first argument should be a data frame, the second a column in it, in the form 'preds'\n",
    "preds_prob(bst_train,'xgb_preds', 'issignal',bst_test,'xgb_preds', 'issignal')\n",
    "\n",
    "#Applying XGB on the 100k events data-set\n",
    "df_clean['xgb_preds'] = bst.predict(dtest)\n",
    "#preds_prob(df_clean,'xgb_preds', 'issignal','test')\n",
    "\n",
    "df_clean_urqmd['xgb_preds'] = bst.predict(dtest2)\n",
    "#del x_test, y_test, x_train, y_train, dtrain, dtest, x_whole, y_whole, x_whole_1, y_whole_1, dtest1, df_scaled\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following graph will show us that which features are important for the model\n",
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "#ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = 0.1\n",
    "df3_base=df_clean_urqmd[(df_clean_urqmd['xgb_preds']>cut3) ]\n",
    "#df3_base=df3_base[df3_base['xgb_preds1']>0.45]\n",
    "#df3_base=df3_base[(df3_base['xgb_preds2']<0.45)&(df3_base['xgb_preds2']>0.15)]\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "range1= (1.105, 1.14)\n",
    "bins1 = 150\n",
    "\n",
    "#xgb\n",
    "\n",
    "#issignal has 0,1,2 . So we convert all signals above zero to 1\n",
    "\n",
    "\n",
    "\n",
    "df3_base['mass'].plot.hist(bins = bins1, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True, label='XGB selected $\\Lambda$s')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True, '\\n True positives = \\n (MC =1)\\n signal in \\n the distribution')\n",
    "#df3_base[df3_base['issignal']==1]['mass'].plot.hist(bins = bins1, range=range1,facecolor='magenta',alpha = 0.3,grid=True,sharey=True )\n",
    "df3_base[df3_base['issignal']==0]['mass'].plot.hist(bins = bins1, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True, label ='\\n False positives = \\n (MC =0)\\n background in \\n the distribution')\n",
    "\n",
    "plt.legend( fontsize = 18, loc='upper right')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"XGB selected $\\Lambda$ candidates with a cut of %.3f \"%cut3 +\"on the XGB back probability distribution\", fontsize = 18)\n",
    "axs.set_xlabel(\"Mass (GeV/${c^2}$)\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "#axs.text(1.123, 4000, 'CBM Performance', fontsize=18)\n",
    "#axs.text(1.123, 3500, 'URQMD, Au+Au @ 12A GeV/$c$', fontsize=18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3_base[df3_base['issignal']==0].shape[0])\n",
    "print(df3_base[df3_base['issignal']==1].shape[0])\n",
    "print(df3_base[df3_base['issignal']>1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficiency_plot_mass(df,signal_column, predictions_column, cut_value, range_min, range_max, bin1):\n",
    "    from matplotlib import gridspec\n",
    "    x_min, x_max = range_min , range_max\n",
    "    range1= (x_min, x_max)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1,figsize=(10,10), sharex=True, constrained_layout=True,  gridspec_kw={'width_ratios': [10],\n",
    "                               'height_ratios': [8,4]})\n",
    "    \n",
    "    ns, bins, patches=axs[0].hist((df[(df[predictions_column]>cut_value) & (df[signal_column]==1)]['mass']),bins = bin1,histtype='step', range=range1,Fill=False, color='red', facecolor='red', linewidth=2)\n",
    "    ns1, bins1, patches1=axs[0].hist((df[df[signal_column]==1]['mass']),bins = bin1,histtype='step', Fill=False, range=range1,facecolor='blue',linewidth=2)\n",
    "\n",
    "    #plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "    axs[0].set_ylabel(\"log(counts)\", fontsize = 18)\n",
    "    axs[0].legend(('XGBoost TP','MC TP'), fontsize = 18, loc='upper right')\n",
    "    axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "    axs[0].set_yscale('log')\n",
    "\n",
    "    err = np.std(ns)\n",
    "    err1 = np.std(ns1)\n",
    "    corr_ns_ns1 = np.corrcoef(ns,ns1)[[0],[1]][0]\n",
    "    err_dif = (ns / ns1) * (np.sqrt( ((err/ns)**2) + ((err1/ns1)**2)\n",
    "                                      -2* ((corr_ns_ns1*err*err1)/(ns*ns1))))\n",
    "\n",
    "\n",
    "    axs[1].hlines(y=1, xmin=x_min, xmax=x_max, colors='black', linestyles='dashed', label='')\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    axs[1].errorbar(center,  ns / ns1, yerr=err_dif,  fmt='o',\n",
    "                     c='Blue', label='Background in predictions')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "    axs[1].set_ylabel(\"XGB / MC\", fontsize = 18)\n",
    "    axs[1].grid()\n",
    "    axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "    fig.show()\n",
    "    fig.tight_layout()\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_plot_mass(df_clean_urqmd,'issignal', 'xgb_preds', 0.96, 1.112, 1.12, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "Thus in binary classification, the count of true positives is $C_{0,0}$, false positives is $C_{1,0}$, true negatives is $C_{1,1}$ and false negatives is $C_{0,1}$.\n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "cut3 = test_best\n",
    "df_clean['xgb_preds'] = ((df_clean['xgb_preds']>cut3)*1)\n",
    "cnf_matrix = confusion_matrix(df_clean['issignal'], df_clean['xgb_preds'], labels=[2,1,0])\n",
    "#cnf_matrix = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['secondaries','primaries','background'], title='Confusion Matrix for XGB for cut > '+str(cut3))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')\n",
    "cpproot_time = time.time() - starttime\n",
    "print(f\"total time: {cpproot_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.08, 1.15)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(10,10), sharex=True, constrained_layout=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base['mass']),bins = 50,histtype='step', range=range1,Fill=False, color='red', facecolor='red', linewidth=2)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 50,histtype='step', Fill=False, range=range1,facecolor='blue',linewidth=2)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 18)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost','KFPF '), fontsize = 18, loc='upper right')\n",
    "\n",
    "\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "axs[1].hlines(y=1, xmin=1.112, xmax=1.12, colors='black', linestyles='dashed', label='')\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "err = np.std(ns)\n",
    "err1 = np.std(ns1)\n",
    "corr_ns_ns1 = np.corrcoef(ns,ns1)[[0],[1]][0]\n",
    "err_dif = (ns / ns1) * (np.sqrt( ((err/ns)**2) + ((err1/ns1)**2)\n",
    "                                      -2* ((corr_ns_ns1*err*err1)/(ns*ns1))))\n",
    "axs[1].errorbar(center,  ns / ns1,   fmt='o',\n",
    "                 c='Blue', label='Background in predictions')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 18)\n",
    "#axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "\n",
    "fig.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyRoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys, ROOT\n",
    "from ROOT import TF1, TCanvas,TMath, TColor, TFile\n",
    "\n",
    "import math\n",
    "def truncate(number, decimals=2):\n",
    "    \"\"\"\n",
    "    Returns a value truncated to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer.\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more.\")\n",
    "    elif decimals == 0:\n",
    "        return math.trunc(number)\n",
    "\n",
    "    factor = 10.0 ** decimals\n",
    "    return math.trunc(number * factor) / factor\n",
    "\n",
    "\n",
    "def background_selector(df):\n",
    "    df1 = df[(df['mass']<1.108)]\n",
    "    df2 = df[df['mass']>1.13]\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    return df3['mass'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Root trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "cut = 0.7\n",
    "df3 = df_clean[df_clean['xgb_preds']>=cut]\n",
    "df3 = df3[df3['issignal']>0]\n",
    "df3 = df3[['pT', 'rapidity', 'mass','issignal','xgb_preds']]\n",
    "df3.columns.values[[0,1,2,3,4]] = ['MCpT', 'MCrapidity','MCmass', 'MCissignal','MCxgb_preds']\n",
    "df3[\"MCissignal\"]=df3[\"MCissignal\"].astype(\"float\")\n",
    "df3[\"MCxgb_preds\"]=df3[\"MCxgb_preds\"].astype(\"float\")\n",
    "df3_base = df_clean_urqmd[df_clean_urqmd['xgb_preds']>=cut]\n",
    "df3_base3 = df3_base[['pT', 'rapidity', 'mass', 'issignal','xgb_preds']]\n",
    "df3_base3[\"issignal\"]=df3_base3[\"issignal\"].astype(\"double\")\n",
    "df3_base3[\"xgb_preds\"]=df3_base3[\"xgb_preds\"].astype(\"double\")\n",
    "\n",
    "file = uproot.recreate(\"pt_y_yield_bdt_cut_0.8.root\")\n",
    "file[\"t1\"] = df3_base3\n",
    "file[\"t2\"] = df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency \n",
    "Efficieny correction on just one configuration i.e lorenztian + 2nd order pol, 100 mass binings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as ss\n",
    "ss.erf(3/np.sqrt(2))\n",
    "#(2/np.pi)* np.arctan(7)\n",
    "true_mc_in_recons =[]\n",
    "pt_y_bin_for_yield_min=[]\n",
    "pt_y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_max=[]\n",
    "y_bin_for_yield_min=[]\n",
    "df = df_clean[df_clean['xgb_preds']>=cut]\n",
    "\n",
    "mass_range_min = [1.08]\n",
    "fit_limit_low=[0,0.1* (df['mass'].describe()[2]),   0.2* (df['mass'].describe()[2]),\n",
    "               1.23,\n",
    "               df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.1* (df['mass'].describe()[2]),\n",
    "                df['mass'].describe()[1]+1.2*(df['mass'].describe()[2])+0.2* (df['mass'].describe()[2])]\n",
    "\n",
    "\n",
    "for mm in mass_range_min:\n",
    "    for mmm in range(0,1,1):\n",
    "\n",
    "        binning = [150]\n",
    "        for b in binning:\n",
    "\n",
    "            y_bin_low=-0.2\n",
    "            y_bin_up =0\n",
    "            for i in range(0,15,1):\n",
    "                tot_sig_3_point_5_sigma, tot_sig_3_sigma, tot_sig_2_point_5_sigma, tot_sig_2_sigma = 0, 0, 0, 0\n",
    "                tot_bac_3_sigma, tot_bac_3_point_5_sigma, tot_bac_2_point_5_sigma = 0, 0, 0\n",
    "                \n",
    "                y_bin_low = truncate(y_bin_low+0.2)\n",
    "                y_bin_up = truncate(y_bin_up+0.2)\n",
    "                df_y = df[(df['rapidity']>y_bin_low) & (df['rapidity']<y_bin_up)]\n",
    "                pt_bin_low =-0.2\n",
    "                pt_bin_up =0\n",
    "                \n",
    "                for i in range(0,15,1):\n",
    "                    pt_bin_low = truncate(pt_bin_low+0.2)\n",
    "                    #print(pt_bin_low)\n",
    "                    pt_bin_up = truncate(pt_bin_up+0.2)\n",
    "                    df_pt = df_y[(df_y['pT']>pt_bin_low) & (df_y['pT']<pt_bin_up)]\n",
    "                    mc_counts = df_pt[df_pt['issignal']>0].shape[0]\n",
    "                    #print(y_bin_low, y_bin_up, \" pT \", pt_bin_low,pt_bin_up)\n",
    "                    if df_pt.shape[0]>0:\n",
    "                        true_mc_in_recons.append(mc_counts)\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                    else:\n",
    "                        y_bin_for_yield_min.append(truncate(y_bin_low))\n",
    "                        y_bin_for_yield_max.append(truncate(y_bin_up))\n",
    "                        pt_y_bin_for_yield_min.append(pt_bin_low)\n",
    "                        pt_y_bin_for_yield_max.append(pt_bin_up)\n",
    "                        true_mc_in_recons.append(mc_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "file =uproot.open(\"lambda_qa_dcm.root\")\n",
    "array1 = file[\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\"].to_numpy()\n",
    "array2 = pd.DataFrame(data=array1[0])\n",
    "array2.columns = np.arange(0,3,0.2)\n",
    "array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcm\n",
    "size = 15*15\n",
    "pt_y_yields = pd.DataFrame(data=np.arange(0,size,1),columns = ['numbering'])\n",
    "pt_y_yields['rapidity_min_MC'] = np.zeros(size)\n",
    "pt_y_yields['pT_min_MC'] = np.zeros(size)\n",
    "\n",
    "pt_y_yields['ratio_recons_sim']=np.zeros(size)\n",
    "pt_y_yields['ratio_recons_mc']=np.zeros(size)\n",
    "pt_y_yields['pT_min'] = pt_y_bin_for_yield_min\n",
    "pt_y_yields ['pt_y_yields_MC']=np.zeros(size)\n",
    "pt_y_yields['pt_y_yields_recons']=true_mc_in_recons\n",
    "pt_y_yields['true_mc_in_recons'] = true_mc_in_recons\n",
    "#pt_y_yields['total_mc_in_recons'] = dcm_clean_mc\n",
    "\n",
    "for i in range(0,15):\n",
    "    for j in range(0,15):\n",
    "        pt_y_yields['rapidity_min_MC'].iloc[i+j*15]=0+j*0.2\n",
    "    \n",
    "\n",
    "for i in range(0,15):    \n",
    "    pt_y_yields['pT_min_MC'].iloc[i]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+1*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+2*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+3*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+4*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+5*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+6*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+7*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+8*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+9*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+10*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+11*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+12*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+13*15]=i/5\n",
    "    pt_y_yields['pT_min_MC'].iloc[i+14*15]=i/5\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(0,15,1):\n",
    "    pt_y_yields ['pt_y_yields_MC'].iloc[i]=array1[0][0][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+1*15]=array1[0][1][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+2*15]=array1[0][2][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+3*15]=array1[0][3][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+4*15]=array1[0][4][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+5*15]=array1[0][5][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+6*15]=array1[0][6][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+7*15]=array1[0][7][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+8*15]=array1[0][8][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+9*15]=array1[0][9][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+10*15]=array1[0][10][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+11*15]=array1[0][11][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+12*15]=array1[0][12][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+13*15]=array1[0][13][i]\n",
    "    pt_y_yields['pt_y_yields_MC'].iloc[i+14*15]=array1[0][14][i]\n",
    "\n",
    "for i in range(0,15*15,1):\n",
    "#    pt_y_yields['ratio_recons_mc'].iloc[i]=dcm_clean_mc[i]/pt_y_yields['true_mc_in_recons'].iloc[i]\n",
    "    pt_y_yields['ratio_recons_sim'].iloc[i]=true_mc_in_recons[i]/pt_y_yields['pt_y_yields_MC'].iloc[i]\n",
    "#    pt_y_yields['pT_min'].iloc[i] = pt_y_bin_for_yield_min[i]\n",
    "    #print(\"%.2f\"%pt_y_yields['rapidity_min_MC'].iloc[i],\"       \",pt_y_yields['pT_min_MC'].iloc[i],\"    \", pt_y_yields['ratio'].iloc[i] )\n",
    "#plt.plot(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Reconstructed/Sim')\n",
    "plt.scatter(pt_y_yields['numbering'], pt_y_yields['ratio_recons_sim'], label='Rencostructed/MC')\n",
    "plt.legend()\n",
    "plt.ylim([0.8,1.2])\n",
    "plt.savefig(\"hists\")\n",
    "#pt_y_yields[(pt_y_yields['rapidity_min_MC']>1) & (pt_y_yields['rapidity_min_MC']<1.4) &(pt_y_yields['pT_min_MC']<1)&(pt_y_yields['pT_min_MC']>0)]\n",
    "pt_y_yields[(pt_y_yields['numbering']>=100) & (pt_y_yields['numbering']<150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =TFile(\"lambda_qa_dcm.root\")\n",
    "#array1 = file[\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\"]\n",
    "mc_spectra = file.Get(\"SimParticles_McLambda/SimParticles_rapidity_SimParticles_pT_McLambda;1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot off\n",
    "import ROOT\n",
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "f = TFile('new_dcm_100_efficiency_pt_y_yield_bdt_cut_0.8.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3);\n",
    "h6 = ROOT.TH2F(\"Mc in reconstructed\", \"Mc in reconstructed\", 15,0,3,15,0,3);\n",
    "h7 = ROOT.TH2F(\"Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8 = ROOT.TH2F(\"reconstructable_mc\", \"reconstructable_mc\", 15,0,3,15,0,3);\n",
    "\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_recons'].iloc[i]);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h6.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h7.SetBinContent(y_bin, pT_bin, true_mc_in_recons[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, dcm_clean_mc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h5)\n",
    "\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h7.Divide(h5)\n",
    "\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 950,800)\n",
    "canvas.Draw()\n",
    "canvas.SetGrid()\n",
    "h7.Draw('colz')\n",
    "#h7.GetZaxis().SetRangeUser(0,0.4)\n",
    "\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot on\n",
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "\n",
    "h4 = ROOT.TH2F(\"recons\", \"#frac{#Lambda_{recons}}{#epsilon_{DCM}}x#frac{1}{#Lambda_{sim}}\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc\", \"Mc\", 15,0,3,15,0,3);\n",
    "h6 = ROOT.TH2F(\"Mc in reconstructed\", \"Mc in reconstructed\", 15,0,3,15,0,3);\n",
    "h7 = ROOT.TH2F(\"Efficiency\", \"Efficiency\", 15,0,3,15,0,3);\n",
    "h8 = ROOT.TH2F(\"reconstructable_mc\", \"reconstructable_mc\", 15,0,3,15,0,3);\n",
    "h9 = ROOT.TH2F(\"recons_new\", \"recons_new\", 15,0,3,15,0,3);\n",
    "\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, true_mc_in_recons[i]);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h6.SetBinContent(y_bin, pT_bin, pt_y_yields['true_mc_in_recons'].iloc[i]);\n",
    "    h7.SetBinContent(y_bin, pT_bin, true_mc_in_recons[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, dcm_clean_mc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h5)\n",
    "\n",
    "#h4.Rebin(3)\n",
    "#h7.Rebin(3)\n",
    "#mc_spectra.Rebin(3)\n",
    "#h6.Draw('colz')\n",
    "ratio_recons_to_mc=h7.Divide(mc_spectra)\n",
    "ratio1 = h4.Divide(h7)\n",
    "ratio2 = h4.Divide(mc_spectra)\n",
    "\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 950,800)\n",
    "canvas.Draw()\n",
    "canvas.SetGrid()\n",
    "h4.Draw('colz')\n",
    "h4.Draw(\"TEXT SAME\");\n",
    "h4.SetStats(0)\n",
    "h4.GetZaxis().SetRangeUser(0.9,1.1)\n",
    "canvas . Print (\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/Project/pT_rapidity_distribution_XGB_extracted_signal.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_half_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n*multiplier + 0.1) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (y_bin_for_yield_min[i])\n",
    "    pT=(pt_y_bin_for_yield_min[i])\n",
    "    y_bin = round(y*1.6)+1;\n",
    "    pT_bin = round(pT*1.6);\n",
    "    print(y_bin_for_yield_min[i],y_bin,pT_bin, true_mc_in_recons[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[(df_clean['pT']>2.6) & (df_clean['rapidity']>1.6) & (df_clean['rapidity']<1.8) & (df_clean['issignal']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot on\n",
    "from ROOT import TFile, TTree\n",
    "from array import array\n",
    "from ROOT import std\n",
    "\n",
    "\n",
    "h4 = ROOT.TH2F(\"recons\", \"recons\", 15,0,3,15,0,3);\n",
    "\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h4.SetBinContent(y_bin, pT_bin, true_mc_in_recons[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, dcm_clean_mc[i]);\n",
    "    \n",
    "ratio1 = h4.Divide(h7)\n",
    "ratio1 = h4.Divide(mc_spectra)\n",
    "h4.GetZaxis().SetRangeUser(0.9,1.1)\n",
    "canvas = ROOT . TCanvas (\" canvas \",\"\", 950,800)\n",
    "canvas.Draw()\n",
    "canvas.SetGrid()\n",
    "h4.Draw('colz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TFile('new_urqmd_100_efficiency_pt_y_yield_bdt_cut_0.8.root','recreate')\n",
    "t = TTree('t1','tree')\n",
    "\n",
    "h4 = ROOT.TH2F(\"recons_urqmd\", \"recons_urqmd\", 15,0,3,15,0,3);\n",
    "h5 = ROOT.TH2F(\"Mc_urqmd\", \"Mc_urqmd\", 15,0,3,15,0,3);\n",
    "\n",
    "bin1 = h4.FindBin(0);\n",
    "bin2 = h4.FindBin(3);\n",
    "for i in range(1,225):\n",
    "    #recons.SetBinContent( (pt_y_yields1['rapidity_min'].iloc[i]), (pt_y_yields1['pT_min'].iloc[i]) ,pt_y_yields1['pt_y_yields'].iloc[i])\n",
    "    y= (pt_y_yields['rapidity_min_MC'].iloc[i])\n",
    "    pT=(pt_y_yields['pT_min_MC'].iloc[i])\n",
    "    y_bin = int((y+0.1)/0.2 + 1);\n",
    "    pT_bin = int((pT+0.1)/0.2 + 1);\n",
    "    h5.SetBinContent(y_bin, pT_bin, pt_y_yields['pt_y_yields_MC'].iloc[i]);\n",
    "    h4.SetBinContent(y_bin, pT_bin, true_mc_in_recons[i]);\n",
    "    #h8.SetBinContent(y_bin, pT_bin, dcm_clean_mc[i]);\n",
    "    \n",
    "\n",
    "\n",
    "#h4.Draw('colz')\n",
    "\n",
    "#h5.Draw('colz')\n",
    "#hist_2d.Draw('colz')\n",
    "#ratio_recons_to_recons_mc=h4.Divide(h5)\n",
    "\n",
    "\n",
    "#h7.GetZaxis().SetRangeUser(0,0.4)\n",
    "\n",
    "\n",
    "f.Write()\n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(df:pd.DataFrame, pid):\n",
    "    mean = df[df['pid']==pid]['mass2'].mean()\n",
    "    std = df[df['pid']==pid]['mass2'].std()\n",
    "    in_sigma = ([df['pid']==pid])&(df['mass2'] > (mean-std)) & (df['mass2'] < (mean+std))\n",
    "    return df[in_sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
